 
 
 
 
 
 

Dissertação de Mestrado 

 

UMA INTERFACE PARA O CONTROLE DE 

ROBÔS MÓVEIS POR INTERMÉDIO DE GESTOS 

 

 
 
 

Oto Emerson de Albuquerque 

Natal, agosto de 2015 

 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 

 

UNIVERSIDADE FEDERAL DO RIO GRANDE DO NORTE

PROGRAMA DE P ´OS-GRADUA ¸C ˜AO EM ENGENHARIA MEC ˆANICA

CENTRO DE TECNOLOGIA

Oto Emerson de Albuquerque

UMA INTERFACE PARA O CONTROLE DE ROB ˆOS

M ´OVEIS POR INTERM´EDIO DE GESTOS

Natal-RN

2015

Oto Emerson de Albuquerque

UMA INTERFACE PARA O CONTROLE DE ROB ˆOS

M ´OVEIS POR INTERM´EDIO DE GESTOS

Disserta¸c˜ao de Mestrado apresentada ao Pro-
grama de P´os-Gradua¸c˜ao em Engenharia
Mecˆanica da UFRN como parte dos requi-
sitos para a obten¸c˜ao do t´ıtulo de Mestre em
Engenharia Mecˆanica. ´Area de concentra¸c˜ao:
Engenharia Computacional.

Orientador: Prof. Dr. Wallace Moreira Bessa

Natal-RN

2015

Dados Internacionais de Cataloga¸c˜ao na Publica¸c˜ao (CIP)

Albuquerque, Oto Emerson de.

76..f: il.

Uma interface para o controle de robˆos m´oveis por interm´edio
de gestos / Oto Emerson de Albuquerque. – Natal-RN : UFRN,
2015.

15 + 89 p. ; (INPE-00000-TDI/0000)

Disserta¸c˜ao () – Universidade Federal do Rio Grande do Norte,

Natal-RN, 2015.

Orientador : Wallace Moreira Bessa.

1. Vis˜ao Computacional. 2. OpenCV 3. Rob´otica M´ovel. 4. Ro-

botino. 5. Sistema de Cores I. T´ıtulo.

CDU 000.000

Oto Emerson de Albuquerque

UMA INTERFACE PARA O CONTROLE DE ROB ˆOS

M ´OVEIS POR INTERM´EDIO DE GESTOS

Disserta¸c˜ao de Mestrado apresentada ao Pro-
grama de P´os-Gradua¸c˜ao em Engenharia
Mecˆanica da UFRN como parte dos requi-
sitos para a obten¸c˜ao do t´ıtulo de Mestre em
Engenharia Mecˆanica. ´Area de concentra¸c˜ao:
Engenharia Computacional.

Aprovada em:

Prof. Dr. Wallace Moreira Bessa

Universidade Federal do Rio Grande do Norte - UFRN

Orientador

Prof. Dr. Angelo Roncalli Oliveira Guerra

Universidade Federal do Rio Grande do Norte - UFRN

Membro

Prof. Dr. Carlos Magno de Lima

Universidade Federal do Rio Grande do Norte - UFRN

Membro

A minha Esposa Jaciana e fam´ılia.

AGRADECIMENTOS

Agrade¸co primeiramente a Deus. Agrade¸co a minha esposa Jaciana e a toda a minha

fam´ılia. Ao meu Orientador Wallace Moreira Bessa.

Agrade¸co aos meus amigos: Kayo, Caio, Jorge, Sandro e George, que ajudaram a

tornar poss´ıvel a conclus˜ao desta disserta¸c˜ao.

Agrade¸co ao programa PRH 14 e a ANP pelo apoio ﬁnanceiro concedido atrav´es
da bolsa de mestrado, ao Laborat´orio de Engenharia de Computa¸c˜ao e Automa¸c˜ao, ao
Laborat´orio de Manufatura, ao Laborat´orio de Tribologia do Curso de Engenharia Me-
cˆanica e a Petrobras, pelos recursos disponibilizados e pela oportunidade de desenvolver
este trabalho.

“Bendito seja Deus, que n˜ao rejeitou a minha ora¸c˜ao, nem desviou de

mim a sua miseric´ordia”.

B´ıblia Sagrada
em “Salmos 66.20”

RESUMO

Esse trabalho tem como objetivo o estudo de t´ecnicas de vis˜ao computacional para o
reconhecimento de gestos humanos, bem como sua aplica¸c˜ao ao controle de robˆos m´oveis.
S˜ao analisados os desaﬁos e propostas algumas solu¸c˜oes para o desenvolvimento de uma
interface simples, a qual permita o reconhecimento e a interpreta¸c˜ao dos gestos gerados
pela m˜ao humana, no intuito de controlar os movimentos de um robˆo. Neste contexto, foi
desenvolvido um programa em C++ para captar as imagens geradas por uma cˆamera de
alta deﬁni¸c˜ao e, a partir da informa¸c˜ao extra´ıda das imagens, deﬁnir qual movimento seria
realizado pelo robˆo. Para auxiliar a tarefa de reconhecimento e processamento da imagem
captada foram utilizadas fun¸c˜oes da biblioteca OpenCV. Os resultados obtidos com a in-
terface desenvolvida conﬁrmam e demonstram a viabilidade de aplica¸c˜ao desta tecnologia.

Palavras-chave: Vis˜ao Computacional, OpenCV, Rob´otica M´ovel, Robotino, C++

A GESTURE BASED INTERFACE FOR THE CONTROL OF MOBILE

ROBOTS

ABSTRACT

This work describes some computer vision techniques that could be used to
recognize human gestures, as well as the application of these techniques to the control of
a mobile robot. The related challenges are investigated and some solutions are proposed
for the development of a simple interface, that could be able to recognize and perceive
gestures generated by a human hand, and control a mobile robot as well. In this context,
a C++ program was developed to capture images from a high-deﬁnition camera and, with
the inferred information, deﬁned the motion to be performed by the robot. The OpenCV
computer library was used in order to assist within the image recognition and processing.
The results obtained with the developed interface endorse the feasibility of this technology.

Keywords: Computer Vision, OpenCV, Mobile Robotics, Robotino, C++

LISTA DE FIGURAS

P´ag.

1

2

3

4

5

6

7

8

9

Exemplo de vis˜ao computacional (reconhecimento de pessoas)

. . . . . . . . . 16

Robˆo autˆonomo - Asimo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

Robˆo autˆonomo - Robotino . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18

Rastreamento feito com o aux´ılio da biblioteca OpenCV . . . . . . . . . . . . 19

Imagem original (a) e sua representa¸c˜ao no espa¸co de cor RGB (b) e CMYK (c) 24

Espa¸co de cores YCbCr

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25

Espectro de cores da pele humana . . . . . . . . . . . . . . . . . . . . . . . . . 26

Espa¸co de cores RGB e CMYK . . . . . . . . . . . . . . . . . . . . . . . . . . 28

Espa¸co de cores HSV . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29

10 Figuras representativas (a) e (b), e varredura de imagem explicativa (c) . . . . 31

11

Lado esquerdo uma imagem com ru´ıdo, e na direita a mesma imagem ap´os
ﬁltragem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33

12 Aplica¸c˜ao do ﬁltro mediana (direita) sobre uma imagem original (esquerda) . . 35

13 Forma 1-D da distribui¸c˜ao Gaussiana com m´edia zero e desvio padr˜ao um,

σ = 1 na equa¸c˜ao da fun¸c˜ao Gaussiana . . . . . . . . . . . . . . . . . . . . . . 36

14 Representa¸c˜ao da fun¸c˜ao Gaussiana em 3D com m´edia em (0,0) desvio padr˜ao

σ = 1

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

15 Dilata¸c˜ao morfol´ogica. (a) Imagem Original, (b) Imagem Dilatada, (c) Imagem

Dilatada 2x . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38

16

Imagem morfol´ogica. (a) Imagem Original, (b) Imagem Erodizada, (c) Imagem
Erodizada 2x . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38

17 Conjunto . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

18 Reﬂex˜ao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

8

19 Complemento . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40

20 Diferen¸ca . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40

21 Dilata¸c˜ao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41

22 Eros˜ao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42

23 Representa¸c˜ao visual do convex hull do conjunto fechado. . . . . . . . . . . . . 44

24 Convex hull de um conjunto ﬁnito: analogia com uma tira el´astica (atilho).

. . 45

25 Defeitos de convexidade: a linha escura do contorno em torno da m˜ao ´e um
casco convexo (convex hull); as regi˜oes quadriculadas (A-H) s˜ao defeitos de
convexidade no contorno da m˜ao em rela¸c˜ao ao casco convexo. . . . . . . . . . 46

26 T´ecnicas usadas: convex hull (ponta dos dedos) e convex hull defect (vales) . . 48

27 Robotino (vista frontal)

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49

28 Roda Mecanum usada pelo Robotino . . . . . . . . . . . . . . . . . . . . . . . 49

29 Acess´orios do Robotino (a) scanner a laser, (b) girosc´opio, (c) esta¸c˜ao para

carregamento . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49

30 Robotino (vista frontal explicativa) . . . . . . . . . . . . . . . . . . . . . . . . 50

31 Cart˜ao de mem´oria . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50

32 Ponto de acesso Wi-Fi

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51

33 Unidade omnidirecional

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52

34 Baterias usadas no Robotino . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55

35 Encoder incremental, sensores anti-colis˜ao e sensores de medi¸c˜ao de distˆancia . 56

36

Interface I/O (imagem esquerda). Interface I/O e suas liga¸c˜oes poss´ıveis (ima-
gem direita) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58

37 Algoritmo do programa de controle . . . . . . . . . . . . . . . . . . . . . . . . 59

38 Exemplo de utiliza¸c˜ao da linha de comando . . . . . . . . . . . . . . . . . . . 64

39 Exemplo de utiliza¸c˜ao da linha de comando . . . . . . . . . . . . . . . . . . . 65

40

Janela ativa contendo a legenda de reconhecimento de gestos . . . . . . . . . . 65

41 Visualiza¸c˜ao dos pontos interpretados: 0, 1 e 2 . . . . . . . . . . . . . . . . . . 66

42 Visualiza¸c˜ao dos pontos interpretados: 3, 4 e 5 . . . . . . . . . . . . . . . . . . 66

43 Contorno da imagem captada pela cˆamera (imagem da esquerda) e imagem

real captada (imagem da direita).

. . . . . . . . . . . . . . . . . . . . . . . . . 68

44 Ponto vermelho central: COG com sua respectiva coordenada (x, y). . . . . . . 68

45

(a) Interpreta¸c˜ao correta da imagem. (b) Presen¸ca de falsos positivos na ima-
gem (pontos 1, 2 e 3) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70

46 Parte do ombro ´e desprezada . . . . . . . . . . . . . . . . . . . . . . . . . . . 71

47 Notebook utilizado, ASUS G750JX . . . . . . . . . . . . . . . . . . . . . . . . 74

48 Programa 1: Contorno da imagem captada pela cˆamera (imagem da esquerda)

e imagem real captada (imagem da direita).

. . . . . . . . . . . . . . . . . . . 75

49 Programa 2: Contorno da imagem captada pela cˆamera (imagem da esquerda)

e imagem real captada (imagem da direita).

. . . . . . . . . . . . . . . . . . . 75

50 Programas 3 e 4: Contorno da imagem captada pela cˆamera (imagem da es-
querda), imagem negativa ´util da imagem (centro) e imagem real captada
(imagem da direita).

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76

51 Programa 5: Contorno da imagem captada pela cˆamera (imagem da esquerda)

e imagem real captada (imagem da direita).

. . . . . . . . . . . . . . . . . . . 76

52 Kinect utilizado no Xbox One . . . . . . . . . . . . . . . . . . . . . . . . . . . 79

53 Acess´orios do Robotino . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80

LISTA DE TABELAS

P´ag.

1

2

3

4

5

6

7

8

Filtro Mediana . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

Tabela Representativa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52

Dados de performance do motor . . . . . . . . . . . . . . . . . . . . . . . . . . 53

Redutor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53

Roletes para todo tipo de terreno . . . . . . . . . . . . . . . . . . . . . . . . . 53

Cˆamera VGA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54

Dados t´ecnicos do sensor de proximidade . . . . . . . . . . . . . . . . . . . . . 56

Informa¸c˜oes sobre percentuais de reconhecimento de imagem . . . . . . . . . . 77

LISTA DE ABREVIATURAS E SIGLAS

– Sistema de imagens usa trˆes separadores do tipo CCD
– Sistema operacional m´ovel desenvolvido pela Google
– Aplicativo de interface de programa¸c˜ao
– Bitmap
– Licen¸ca de c´odigo aberto do tipo Berkeley Software Distribution
– Linguagem de Programa¸c˜ao
– Linguagem de programa¸c˜ao orientada a objetos criada pela Microsoft
– Evolu¸c˜ao da Linguagem de programa¸c˜ao baseada em C
– Charged Coupled Device
– Complementary Metal Oxide Semiconductor
– Sistema de cores formado por Ciano, Magenta, Amarelo e Preto
– Center of gravity
– Tubo de raios cat´odicos
– Sistema de cores que adiciona intensidade
– Sistema de cores que adiciona luminosidade
– Sistema de cores formadas pelas componentes hue, saturation e value
–
–
– Entrada e Sa´ıda
– Sistema operacional m´ovel desenvolvido pela Apple
–
– Joint Photographic Experts Group
– Liquid Crystal Display
– Sistema operacional Open Source desenvolvido por Linus Torvalds
– Computador pessoal desenvolvido pela Apple
– C´odigo aberto
– Biblioteca de Vis˜ao Computacional (Open Source)
– Personal computer
– Proportional Integrator-Diﬀerentiator
– TV que gera imagens de uma g´as (plasma) preenchido com Xenˆonio e Neon
– Quarter VGA
– Red, Green and Blue (Vermelho, verde e azul)

3CCD
Android
API
BMP
BSD
C
C#
C++
CCD
CMOS
CMYK
COG
CRT
HSI
HSL
HSV
IEEE
IHC
I/O
iOS
IP
JPEG
LCD
Linux
Mac
Open Source
OpenCV
PC
PID
Plasma
QVGA
RGB
Robotino SIM – Simulador virtual do robotino
RPM

– Revolu¸c˜oes por minuto

Institute of Electrical and Electronics Engineers
Itera¸c˜ao Homem-Computador

Internet Protocol

– Sistema operacional
– Super VGA
– Universidade Federal do Rio Grande do Norte
– Universal Serial Bus
– Tens˜ao M´edia
– Video Graphics Array
– Wired Equivalent Privacy
– Wireless Fidelity

SO
SVGA
UFRN
USB
VDC
VGA
WEP
WIFI
WPA-PSK – WiFi Protected Access, Pre-Shared Key
YCbCr

– Espa¸co de cores utilizado para codiﬁca¸c˜ao do espa¸co de cores VGA

SUM ´ARIO

P´ag.

1 INTRODU ¸C ˜AO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
1.1 Aplica¸c˜ao em Rob´otica M´ovel
. . . . . . . . . . . . . . . . . . . . . . . . . . . 18
1.2 Motiva¸c˜ao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
1.3 Objetivo Geral
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
1.4 Objetivos Espec´ıﬁcos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
1.5 Estrutura da Disserta¸c˜ao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

2 FUNDAMENTA ¸C ˜AO TE ´ORICA . . . . . . . . . . . . . . . . . . . . .
23
2.1 Espa¸co de cores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.1.1 Espa¸co Y CbCr
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
2.1.2 Espa¸co RGB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
2.1.3 Espa¸co HSV . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
2.2 Reconhecimento dos contornos de uma imagem . . . . . . . . . . . . . . . . . 29
´Area do contorno . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
2.3
2.3.1 F´ormula de Green . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
2.4 Filtros . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
2.4.1 Filtro Mediana (Median blur) . . . . . . . . . . . . . . . . . . . . . . . . . . 34
2.4.2 Filtro Gaussiano (Gaussian blur) . . . . . . . . . . . . . . . . . . . . . . . . 36
2.4.3 Filtros de Dilata¸c˜ao e Eros˜ao . . . . . . . . . . . . . . . . . . . . . . . . . . 37
2.4.3.1 Deﬁni¸c˜oes b´asicas
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
2.4.3.2 Dilata¸c˜ao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
2.4.3.3 Eros˜ao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
2.5 Centro Geom´etrico (COG) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
2.6 Convex Hull . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
2.7 Convexity Defect (defeitos de convexidade) . . . . . . . . . . . . . . . . . . . . 46

3 MATERIAIS E M´ETODOS . . . . . . . . . . . . . . . . . . . . . . . . .
47
3.1 Biblioteca OpenCV . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
3.2 Robotino
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
3.3 Dados t´ecnicos
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
3.4 Algoritmos de Mapeamento . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
3.5 Testes experimentais . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60

4 SISTEMA DE CONTROLE POR GESTOS . . . . . . . . . . . . . . .
4.1

61
Inicializa¸c˜ao do robˆo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61

4.2 Fun¸c˜oes, Headers e Classe Utilizadas para Inicializa¸c˜ao do Robˆo . . . . . . . . 61
4.3 Elabora¸c˜ao do Programa de Controle . . . . . . . . . . . . . . . . . . . . . . . 64

70
´Area M´ınima da Imagem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70

5 AN ´ALISE E DISCUSS ˜AO DOS RESULTADOS . . . . . . . . . . . . .
5.1
5.2 Problemas Enfrentados Durante o Desenvolvimento do Programa de Controle

e Suas Respectivas Solu¸c˜oes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
5.3 Estat´ısticas do processo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74

6 CONSIDERA ¸C ˜OES FINAIS . . . . . . . . . . . . . . . . . . . . . . . .
78
6.1 Conclus˜oes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
6.2 Sugest˜oes para Trabalhos Futuros . . . . . . . . . . . . . . . . . . . . . . . . . 79

REFERˆENCIAS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

81

APˆENDICE .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

. 87

1

INTRODU ¸C ˜AO

Vis˜ao computacional ´e a ciˆencia e tecnologia das m´aquinas que enxergam. Ela com-
preende o desenvolvimento da teoria e tecnologia para a constru¸c˜ao de sistemas artiﬁciais
que obt´em informa¸c˜ao de imagens ou quaisquer dados multi dimensionais. Na ﬁgura (1)
pode-se ter uma ideia da interpreta¸c˜ao de informa¸c˜oes desejadas (pessoas) na imagem.
Exemplos de aplica¸c˜oes incluem o controle de processos (como robˆos industriais ou ve´ıcu-
los autˆonomos), detec¸c˜ao de eventos, organiza¸c˜ao de informa¸c˜ao, modelagem de objetos ou
ambientes e intera¸c˜ao (atrelado a uni˜ao homem-computador) (FORSYTH; PONCE, 2002).

Figura 1 - Exemplo de vis˜ao computacional (reconhecimento de pessoas)

Fonte: (LINUX MAGAZINE ONLINE, 2015)

Tem-se lan¸cado m˜ao de v´arias ´areas do conhecimento com o intuito de cumprir tais
objetivos, como por exemplo: inteligˆencia artiﬁcial, rob´otica, engenharia el´etrica e mecˆa-
nica, inform´atica; at´e ciˆencias sociais e ciˆencias que estudam o comportamento humano.
Por´em, mesmo com todo o avan¸co que j´a se alcan¸cou em tais ciˆencias, o homem ainda
n˜ao obteve ˆexito em criar uma m´aquina que tenha todas as habilidades f´ısicas e cognitivas
encontradas em um ser humano.

Embora aparentemente esteja distante o dia em que tal feito seja concretizado,
avan¸cos importantes foram obtidos nas ´ultimas d´ecadas. Temos m´aquinas, ﬁgura (2), com
habilidades (mesmo que limitadas) de ouvir, tomar decis˜oes, falar, ver e interagir com
objetos e seres humanos.

Figura 2 - Robˆo autˆonomo - Asimo

17

Fonte: (HONDA, 2015)

Entre as habilidades mencionadas, a habilidade relacionada `a vis˜ao de robˆos (mais
especiﬁcamente a vis˜ao computacional), tem sido alvo de v´arias pesquisas e estudos na ´area
de computa¸c˜ao (CROWLEY et al., 1994). Partindo de um conhecimento de como funciona
a vis˜ao humana, pesquisadores tem buscado desenvolver mecanismos que possibilitem que
esta habilidade seja poss´ıvel a robˆos, de modo que a percep¸c˜ao tridimensional de objetos
em um ambiente por parte de um robˆo e a associa¸c˜ao destes objetos a um conhecimento
pr´evio sobre tais, seja realizada com sucesso (ORLANDINI, 2012). Tal tarefa ´e bastante
complexa, uma vez que, quando se analisa a vis˜ao humana, percebe-se que esta n˜ao se
constitui somente da captura de imagens pelo globo ocular, mas tamb´em, pela associa¸c˜ao
a todo um conhecimento adquirido pelo ser humano ao longo de sua vida.

H´a na literatura especializada alguns trabalhos que abordam conceitos semelhantes
aos apresentados nesta disserta¸c˜ao. Dentre eles se destaca um artigo em que o controle
´e feito utilizando-se apenas um tipo de ﬁltro (Gaussiano) e um conjunto de modelos pr´e
programados (matrizes contendo informa¸c˜oes pr´evias dos gestos) com a ajuda de redes
neurais, para interpreta¸c˜ao dos gestos (bra¸co) executados pelo usu´ario, por´em com um
desempenho de apenas dez quadros por segundo e a obrigatoriedade de se repetir os
padr˜oes de gestos que foram previamente armazenados na matrizes (WALDHERR, 2000).
Ainda nesta linha, Iba (IBA, 1999) aborda algo parecido, pois para o controle do robˆo,
foram necess´arios o armazenamento em uma matriz de forma oﬄine de 5000 amostras
representativas de gestos da m˜ao, para a partir da´ı analisar os gestos feitos pelo usu´ario,
comparar com a matriz previamente armazenada e s´o ent˜ao comandar o robˆo. Dentre
os artigos dispon´ıveis, o trabalho de Kaura (KAURA, 2013) ´e o que mais se aproxima
da abordagem aqui apresentada. Kaura (KAURA, 2013) utiliza programa¸c˜ao em C++ e
fun¸c˜oes da biblioteca OpenCV para controlar um robˆo m´ovel (Arduino), por´em, utiliza
apenas quatro gestos (a partir de dois at´e cinco dedos) para controle do robˆo, al´em da
intercepta¸c˜ao da palma da m˜ao para estabelecer as dire¸c˜oes direita/esquerda/tr´as.

18

1.1 Aplica¸c˜ao em Rob´otica M´ovel

Nos ´ultimos anos, tem havido grande interesse em pesquisas voltadas para automa-
¸c˜ao de processos por meio de sistemas rob´oticos, com intuito de promover a melhoria da
qualidade dos produtos e a otimiza¸c˜ao do tempo. Esses sistemas, em sua maioria, utilizam
t´ecnicas de inteligˆencia artiﬁcial (IA) empregadas na constru¸c˜ao dos algoritmos propos-
tos para solucionar os problemas. Por isso, sistemas autˆonomos de navega¸c˜ao rob´otica
que permitem a tomada de decis˜ao com base em informa¸c˜oes extra´ıdas do ambiente, que
proporcionam a coopera¸c˜ao de agentes ou possuem vis˜ao computacional, tˆem sido larga-
mente explorados em pesquisas nas ´areas de automa¸c˜ao, rob´otica e IA, o que gera muitas
propostas de aplica¸c˜oes em v´arios segmentos (ARA ´UJO; LIBRANTZ, 2006).

Neste contexto, tem-se o estudo da rob´otica m´ovel. O robˆo m´ovel ´e um dispositivo
autom´atico que ´e capaz de se movimentar e interagir em um ambiente deﬁnido. De acordo
com MUIR, (1988) o mesmo deﬁne um robˆo m´ovel como: um robˆo capaz de se locomover
sobre uma superf´ıcie somente atrav´es da atua¸c˜ao de rodas montadas no robˆo e em contato
com a superf´ıcie (VICTORINO, 1998). As rodas permitem um deslocamento relativo entre o
seu eixo e a superf´ıcie sobre a qual se espera ter um ´unico ponto de contato com rolamento
puro.

Um robˆo m´ovel usualmente ´e equipado com sensores (vis˜ao, infravermelho, sonar,
tato, toque, sistemas de navega¸c˜ao inercial, etc.) que permite a percep¸c˜ao do meio am-
biente, total ou parcialmente desconhecido, e ´e dotado de capacidade de decis˜ao, que lhe
permite cumprir uma tarefa sem interven¸c˜ao humana. Quando tais sensores est˜ao dispo-
n´ıveis no robˆo, pode-se fazer uso da intera¸c˜ao do robˆo com o ambiente, fazendo-o perceber
e construir o modelo do ambiente no qual o movimento se desenvolve e depois decidir as
a¸c˜oes a serem tomadas para a realiza¸c˜ao da tarefa (J ´ACOBO, 2001).

Neste trabalho, o robˆo m´ovel alvo de estudos ´e o Robotino, ﬁgura (3) (FESTO, 2014).

Figura 3 - Robˆo autˆonomo - Robotino

Fonte: (FESTO, 2015d)

19

Para se iniciar as atividades no Robotino foi necess´ario a deﬁni¸c˜ao de qual seria a
melhor estrat´egia para se fazer o controle adequado do robˆo m´ovel. Destacam-se duas fer-
ramentas para controle do robˆo: Robotino View e linguagens de programa¸c˜ao. O Robotino
View (Software nativo da Festo para o Robotino), trabalha com o aux´ılio de blocos pr´e
programados para facilitar o controle e a intera¸c˜ao com o robˆo. A outra ferramenta a dis-
posi¸c˜ao ´e um conjunto de linguagens de programa¸c˜ao que a princ´ıpio seria mais complexo
de se trabalhar, por´em, oferece um controle mais apurado e completo de todo o sistema
do robˆo m´ovel autˆonomo.

O Robotino view tem como pontos positivos a facilidade de uso e uma melhor intera-
¸c˜ao com o usu´ario, por´em, seus blocos pr´e programados n˜ao abrangem toda a necessidade
requerida para implementa¸c˜ao completa desse trabalho.

As linguagens de programa¸c˜ao permitem um controle mais eﬁciente, robusto e com-
pleto do robˆo, pois permite que a linguagem tenha acesso completo e direto a todos os
motores, sensores e parˆametros do robˆo. Ap´os uma minuciosa an´alise dos pr´os e os contras
de cada ferramenta, optou-se pelo uso da linguagem de programa¸c˜ao, uma vez que o autor
dessa disserta¸c˜ao j´a possui conhecimento em v´arias linguagens que oferecem suporte de
controle para robˆo m´ovel.

Neste sentido, o objetivo principal deste trabalho ´e desenvolver aplica¸c˜oes de vis˜ao
computacional utilizando-se algumas linguagens de computa¸c˜ao cient´ıﬁca (C, C++, C#),
al´em da biblioteca OpenCV (OpenCV ´e um conjunto de ferramentas de programa¸c˜ao para
desenvolvimento de aplica¸c˜oes com Vis˜ao, ﬁgura (4)) (OPENCV, 2014).

Figura 4 - Rastreamento feito com o aux´ılio da biblioteca OpenCV

Fonte: (THE GLOWING PYTHON, 2015)

A biblioteca ´e completamente open-source, e ´e distribu´ıda gratuitamente, aberta
a colabora¸c˜oes de qualquer indiv´ıduo ou empresas volunt´arias. O OpenCV implementa
uma variedade de ferramentas de interpreta¸c˜ao de imagens, indo desde opera¸c˜oes simples
como um ﬁltro de ru´ıdo, at´e opera¸c˜oes complexas, tais como a an´alise de movimentos,

20

reconhecimento de padr˜oes e reconstru¸c˜ao em 3D.

Desta forma a linguagem de programa¸c˜ao baseada em C++ e a biblioteca opensource
OpenCV ser˜ao utilizadas para serem integradas a plataforma rob´otica Robotino, provendo
ao robˆo autonomia para vencer obst´aculos; de modo a contribuir com a evolu¸c˜ao da
pesquisa no Brasil relacionada `a vis˜ao computacional em rob´otica m´ovel.

1.2 Motiva¸c˜ao

A pesquisa e desenvolvimento em rob´otica m´ovel requerem conhecimentos de diver-
sas ´areas, da Engenharia Mecˆanica, Engenharia El´etrica, Engenharia da Computa¸c˜ao e
das diferentes ´areas da Computa¸c˜ao. Em particular, a ´area de computa¸c˜ao possui um pa-
pel muito importante como ´area que provˆe o suporte de conhecimentos e t´ecnicas que ir˜ao
permitir dotar estes robˆos m´oveis de sistemas de controle mais robustos, seguros, autˆo-
nomos e inteligentes. Faz-se, portanto, necess´aria a forma¸c˜ao de proﬁssionais capacitados
para atuar nesta ´area em grande expans˜ao, e mais do que isto, ´e de grande importˆancia
apresentar conceitos relacionados `a ´area de vis˜ao computacional, bem como empreg´a-los
no desenvolvimento de um sistema capaz de reconhecer gestos, o qual foi devidamente
implementado e testado em um robˆo m´ovel (Robotino), dispon´ıvel no Laborat´orio de
Manufatura da UFRN.

1.3 Objetivo Geral

Pretende-se com este trabalho desenvolver aplica¸c˜oes de vis˜ao computacional com
foco no reconhecimento de gestos humanos, utilizando-se algumas linguagens de compu-
ta¸c˜ao cient´ıﬁca (C, C++), al´em da biblioteca OpenCV. Desta forma, essas linguagens
e biblioteca Open Source ser˜ao utilizadas com o intuito de se conseguir performance e
robustez na aplica¸c˜ao ﬁnal do software de controle, aumentando-se com isso, a adapta¸c˜ao
e a portabilidade do software desenvolvido para outras plataformas computacionais, al´em
de permitir a integra¸c˜ao `a plataformas rob´oticas, provendo ao robˆo autonomia e versati-
lidade para vencer obst´aculos, de modo a contribuir com a evolu¸c˜ao da pesquisa no pa´ıs,
relacionada `a vis˜ao computacional em rob´otica m´ovel.

A pesquisa tamb´em procura desmistiﬁcar a rob´otica m´ovel e com isto incentivar
a forma¸c˜ao de pessoal capacitado a trabalhar nesta ´area. O objetivo desta disserta¸c˜ao ´e
justamente o de apresentar uma abordagem pouco difundida (controle de robˆos via gestos)
para motivar e introduzir conceitos importantes e atuais relacionados `a rob´otica m´ovel,
contribuindo assim para ampliar a literatura de obras nacionais desta ´area (AGUIRRE L.
A.; DA SILVA, 2007), e assim contribuir para disseminar conhecimentos e t´ecnicas da ´area
de computa¸c˜ao que s˜ao atualmente empregadas no desenvolvimento dos robˆos m´oveis
inteligentes.

O trabalho do desenvolvedor (programador), segundo DOMINGUES, (2004), con-

21

siste em fazer com que os sinais movam-se para dentro do sistema atrav´es de dispositivos
ou interfaces que garantam os contatos do homem com programas. ´E na cria¸c˜ao das inter-
faces, segundo SANTOS, (2010), que os desenvolvedores exploram o limite da conjuga¸c˜ao
do trabalho proposto com os softwares, constr´oi-se o di´alogo entre o biol´ogico (homem) e os
sistemas artiﬁciais (robˆos) em ambientes virtuais e f´ısicos em que dispositivos maqu´ınicos,
cˆameras e sensores, capturam sinais emitidos pelo corpo para process´a-los e devolvˆe-los
transmutados para o controle dos sistemas artiﬁcias em quest˜ao.

1.4 Objetivos Espec´ıﬁcos

A Vis˜ao Computacional usa frequentemente identiﬁca¸c˜ao de cores em conjunto com
outras propriedades para mapear ´areas espec´ıﬁcas da imagem (SZELISKI, 2010). Para
facilitar o processamento computacional ´e necess´ario o uso de um modelo que descreva
matematicamente como uma determinada cor deve ser representada. Existem diversas
op¸c˜oes que podem, dependendo da aplica¸c˜ao, ser mais apropriadas que outras.

Neste contexto s˜ao investigados os principais espa¸cos de cores dispon´ıveis atualmente
para facilitar o reconhecimento dos tons de peles inerentes aos seres humanos, visando-se
assim, uma capta¸c˜ao de imagens mais eﬁciente e redu¸c˜ao de ru´ıdos.

Lembrando-se que para cada aplica¸c˜ao exige-se no m´ınimo a utiliza¸c˜ao de um espa¸co
de cor, podendo-se inclusive existir a convers˜ao de um espa¸co de cor em outro durante a
execu¸c˜ao do processo de gera¸c˜ao do sistema para melhorar a deﬁni¸c˜ao e a compreens˜ao
da imagem ﬁnal.

1.5 Estrutura da Disserta¸c˜ao

Neste trabalho, os testes foram executados no complexo do n´ucleo tecnol´ogico da
UFRN, mais especiﬁcamente nas unidades prediais compreendendo o Laborat´orio de Ma-
nufatura e o Laborat´orio de Tribologia, usando-se para isso ferramentas f´ısicas e compu-
tacionais. A organiza¸c˜ao do trabalho se deu da seguinte forma:

• Introdu¸c˜ao:

– A primeira parte do trabalho compreende uma vis˜ao geral sobre vis˜ao com-
putacional, sistemas artiﬁcias e robˆos m´oveis, al´em das ferramentas que
foram analisadas e posteriormente escolhidas para o processo de desenvol-
vimento do software de controle para o robˆo m´ovel (Robotino); motiva¸c˜oes
para a elabora¸c˜ao do trabalho e sua utilidade futura e os objetivos preten-
didos com esse trabalho.

• Fundamenta¸c˜ao te´orica:

– As informa¸c˜oes analisadas neste cap´ıtulo foram fundamentais, pois foi poss´ı-
vel fazer uma pesquisa mais detalhada sobre as teorias que seriam integradas

22

no programa de controle de gestos, destacando-se a biblioteca OpenCV, os
v´arios espa¸cos de cores, reconhecimento de contornos da imagem, ´area de
contorno, ﬁltros para suaviza¸c˜ao da imagem, centro geom´etrico, algoritmos
de mapeamento, convex hull e ﬁnalmente o convexity defect. Todas essas in-
forma¸c˜oes voltadas principalmente para vis˜ao computacional tiveram como
prop´osito o de se considerar a forma na qual os dados seriam passados e
por consequˆencia, tratados pelo programa desenvolvido.

• Materiais e m´etodos:

– Aborda as caracter´ısticas gerais do projeto (robˆo autˆonomo m´ovel), dando
ˆenfase aos componentes el´etricos/eletrˆonicos e mecˆanicos do Robotino, al´em
dos testes pr´aticos e virtuais que foram executados no Laborat´orio de Manu-
fatura e no Laborat´orio de Tribologia, ambos situados no n´ucleo tecnol´ogico
na UFRN.

• Resultados obtidos:

– Neste cap´ıtulo foi poss´ıvel testar a inicializa¸c˜ao do robˆo, as fun¸c˜oes e classe
utilizadas pelo robˆo (ambos feitos em C++), o desenvolvimento te´orico
(ponto esse que testou na pr´atica os conceitos estudados anteriormente para
a confec¸c˜ao do software de controle) e as estat´ısticas do processo que levou
em considera¸c˜ao todas as vers˜oes do software de controle que foi necess´ario
para a ﬁnaliza¸c˜ao do mesmo.
• An´alise e discuss˜ao dos resultados:

– Neste cap´ıtulo foram abordados os testes pr´aticos para se estabelecer as
v´arias constantes que seriam implementados posteriormente na vers˜ao ﬁ-
nal do software de controle, al´em dos problemas encontrados ao longo do
desenvolvimento do programa, com suas respectivas solu¸c˜oes.

• Conclus˜oes:

– Neste cap´ıtulo foi discutido sobre as contribui¸c˜oes feitas pelo programa de
controle, as experiˆencias ganhas com a biblioteca de c´odigo aberto OpenCV,
sobre a conﬁabilidade da plataforma estudada (Robotino) e a infraestru-
tura encontrada na UFRN para planejamento, cria¸c˜ao e testes em todas as
etapas do programa de controle de gestos. Finalmente, foram propostos tra-
balhos futuros que podem ser feitos para melhorar ainda mais a facilidade,
funcionalidade e robustez do programa de controle.

23

2 FUNDAMENTA ¸C ˜AO TE ´ORICA

Para a aquisi¸c˜ao de dados das imagens foram necess´arias a incorpora¸c˜ao de algumas
t´ecnicas de vis˜ao computacional e espa¸co de cores que levam em considera¸c˜ao a forma na
qual os dados s˜ao fornecidos e por consequˆencia, interpretados e passados para o programa
de computa¸c˜ao e assim termos controle efetivo e reﬁnado das informa¸c˜oes, diminuindo
assim a solicita¸c˜ao computacional exigida na coleta dos dados.

2.1 Espa¸co de cores

Espa¸co de cores (tamb´em sistema de cores ou espectro de cores) ´e um modelo abs-
trato matem´atico para formalizar a descri¸c˜ao de cores atrav´es de tuplas de n´umeros, tipi-
camente formadas por trˆes ou quatro elementos. S˜ao normalmente tridimensionais, mas
tamb´em podem ser quadrimensionais. Os espa¸cos de cores RGB (sistema de cores aditivas
formado pelas iniciais das cores em inglˆes Red, Green e Blue, que signiﬁca em portuguˆes,
respectivamente, Vermelho, Verde e Azul), CMYK (padr˜ao de cores subtrativas, formado
pelos pigmentos prim´arios Cyan (Ciano), Magenta (Magenta) e Yellow (Amarelo) e ainda,
o blacK (Preto)) e YCbCr (sistema de cores onde o Y representa a luminˆancia (criado pela
soma do vermelho, azul e verde), Cb que representa a componente de croma da diferen¸ca
de azul e Cr que representa a componente de croma da diferen¸ca de vermelho) s˜ao os mais
frequentemente utilizados.

Um Espa¸co de Cor pode ser visto como um sistema deﬁnido por uma base represen-
tativa dos componentes, de acordo com a deﬁni¸c˜ao do espa¸co considerado. A representa¸c˜ao
de qualquer cor pode ent˜ao ser feita `a custa da combina¸c˜ao desses componentes.

Al´em do RGB e do CMYK j´a citados, o HSV (se¸c˜ao (2.1.3)) ´e tamb´em muito im-

portante atualmente.

O espa¸co de cor RGB, ﬁgura 5(a), ´e utilizado em todas as telas LCD (Liquid Crystal
Display - mostrador de cristal l´ıquido) ou de CRT (Catodic Ray Tube - tubo de raios
cat´odicos) usados em monitores dos mais diversos tipos. Forma as suas cores a partir da
adi¸c˜ao de matizes em escalas de 0-255, ﬁgura 5(b). Por exemplo, para que seu monitor crie
uma cor vermelha, a placa de v´ıdeo transfere a informa¸c˜ao (255, 0, 0) para os pixels que
devem ser vermelhos. Para o branco, a placa manda um sinal (255, 255, 255). Ao somar
todas as cores prim´arias em seu valor m´aximo, o monitor atinge o branco. No outro
extremo, para sintetizar o preto, o sinal recebido pelos pixels ´e (0, 0, 0), (FAIRCHILD,
2013).

Figura 5 - Imagem original (a) e sua representa¸c˜ao no espa¸co de cor RGB (b) e CMYK (c)

24

Fonte adaptada: (TECMUNDO, 2009)

O modelo de cores CMYK (cor de processo, quatro cores), ﬁgura 5(c), ´e um modelo
de cor subtrativo, usado na impress˜ao a cores, e tamb´em ´e usado para descrever o processo
de impress˜ao em si. O modelo CMYK refere-se aos quatro tipos de tintas usadas em
algumas impressoras `a cores: ciano, magenta, amarelo e a tinta chave (preto). Embora
varie de acordo com o local onde a impress˜ao ser´a feita (gr´aﬁca), operador, fabricante, e
tiragem, a tinta ´e aplicada tipicamente na mesma ordem da abreviatura.

O (K) em CMYK representa a tinta chave porque as quatro cores de impress˜ao,
ciano, magenta, amarelo e a chapa de impress˜ao s˜ao cuidadosamente fechadas, ou alinha-
das, com a chave da placa de cor preta. Algumas fontes sugerem que o (K) em CMYK
vem da ´ultima letra da palavra BLACK (em inglˆes) e foi escolhido porque a letra (B) j´a
signiﬁca azul (BLUE) (HORVAT, 2003; JENNINGS, 2003; GATTER, 2005).

O modelo CMYK funciona parcialmente ou inteiramente mesclando as cores, geral-
mente tendo o fundo de aplica¸c˜ao de cores em branco (papel A4, Letter, A3). A tinta
reduz a luz que de outro modo seria reﬂectida. Esse modelo ´e chamado de subtrativo
porque as tintas subtraem o brilho da folha branca.

Em modelos aditivos de cor, como RGB, o branco ´e a combina¸c˜ao aditiva de todas
as cores (luzes) preliminares, enquanto o preto ´e a ausˆencia de cor (luz). No modelo de
CMYK, ´e o oposto: o branco ´e a cor natural do papel ou outro fundo, enquanto o preto

25

resulta de uma combina¸c˜ao completa de tintas coloridas. Para poupar dinheiro em tinta e
para produzir tons pretos mais profundos, as cores n˜ao saturadas e escuras s˜ao produzidas
usando-se tinta preta em vez da combina¸c˜ao de ciano, magenta e amarelo.

2.1.1 Espa¸co Y CbCr

O espa¸co YCbCr, Y’CbCr ou Y Pb/Cb Pr/Cr, ﬁgura (6), tamb´em escrito como
Y CBCR ou Y (cid:48)CBCR, ´e uma fam´ılia de espa¸cos de cor usado em imagem a cores em
v´ıdeo e sistemas de fotograﬁa digital. Y’ ´e a componente de luma (luminosidade de uma
imagem), Cb ´e a componente de croma (cor) da diferen¸ca de azul e Cr ´e a componente de
croma (cor) da diferen¸ca de vermelho (POYNTON, 2012).

Figura 6 - Espa¸co de cores YCbCr

Fonte adaptada: (UNICAMP, 2001)

YCbCr n˜ao ´e um espa¸co de cor absoluta, mas sim, ´e uma forma de codiﬁca¸c˜ao de
informa¸c˜oes RGB. A cor real exibida depende das cores reais RGB prim´arias usadas para
exibir o sinal. Portanto, um valor expresso como Y (cid:48)CbCr ´e previs´ıvel se as cores prim´arias
padr˜ao RGB forem utilizadas.

O espa¸co de cor YCbCr foi deﬁnido em resposta ao aumento da demanda por al-
goritmos digitais em lidar com informa¸c˜oes de v´ıdeo, e desde ent˜ao se tornou um modelo
amplamente utilizado em v´ıdeo digital. Ele pertence `a fam´ılia dos espa¸cos de cor de te-
levis˜ao de transmiss˜ao. A fam´ılia inclui outros, como YUV e YIQ. YCbCr ´e um sistema
de cor digital, enquanto YUV e YIQ s˜ao espa¸cos anal´ogicos para os respectivos sistemas
PAL e NTSC.

Estes espa¸cos de cor RGB separado (Red-Green-Blue) em luminˆancia e crominˆancia

26

de informa¸c˜ao s˜ao ´uteis nas aplica¸c˜oes de compress˜ao no entanto, a especiﬁca¸c˜ao de cores
´e um pouco intuitiva. O espa¸co de cor YCbCr ´e comumente utilizado no processamento
de imagem, uma vez que separa a luminˆancia, em Y componente, formam a crominˆancia
descrito atrav´es dos componenetes Cb e Componentes Cr, (CHAI; NGAN, 1999).

A diferen¸ca entre YCbCr e RGB ´e que YCbCr representa cores como brilho e duas
cores de diferen¸ca de sinais, enquanto RGB representa cores como vermelho, verde e azul,
(IRAJI; TOSINIA, 2012).

Muitos pesquisadores assumem os componentes de crominˆancia do tom da cor da
pele, ﬁgura (7), s˜ao independentes do componente de luminˆancia, (SOBOTTKA; PITAS,
1998; JONES; REHG, 2002; SABER; TEKALP, 1998), no entanto, na pr´atica, o tom da cor
da pele ´e n˜ao-linear e dependente de luminˆancia.

Figura 7 - Espectro de cores da pele humana

Fonte: (MAKE HUMAN, 2015)

Abaixo pode-se ver matematicamente as equa¸c˜oes usadas para a convers˜ao do padr˜ao

anal´ogico RGB para o padr˜ao anal´ogico Y CBCR, (POYNTON, 2012):

Y = 0.257R + 0.504G + 0.098B + 16
Cb = −0.148R − 0.291G + 0.439B + 128
Cr = 0.439R − 0.368G − 0.071B + 128

E do padr˜ao anal´ogico Y CBCR para o padr˜ao anal´ogico RGB, temos:

R = 1.164(Y − 16) + 1.596(Cr − 128)
G = 1.164(Y − 16) − 0.813(Cr − 128) − 0.392(Cb − 128)
B = 1.164(Y − 16) + 2.017(Cb − 128)

J´a para a transforma¸c˜ao do sinal digital (8 bits por amostra) temos:

27

65738.R

129057.G

25064.B

256

+
Y = 16 +
− 74494.G
Cb = 128 − 37945.R
+
− 94154.G
− 18285.B
112439.R

112439.B

Cr = 128 +

256

256

256

256

+

256

256

256

256

Para a transforma¸c˜ao inversa temos:

R =

G =

B =

298082.Y

256

298082.Y

256

298085.Y

256

408583.Cr

256

+
− 100291.Cb
516412.Cb

256

+

256

− 222921
− 208120.Cr
256
− 276836

+ 135576

A forma padr˜ao do espa¸co Y (cid:48)CbCr ´e usada principalmente para as televis˜ao mais
antigas, uma vez que utiliza um modelo RGB que se adapta `as caracter´ısticas de emiss˜ao
de f´osforo dos equipamentos mais antigos do tipo: tubo de raios cat´odicos (CRT).

2.1.2 Espa¸co RGB

O modelo de cores RGB, ﬁgura (8), ´e um modelo aditivo de cor no qual vermelho,
verde e azul claro s˜ao somados de v´arias maneiras para reproduzir uma ampla gama de
cores. O nome do modelo vem das iniciais (em inglˆes) das trˆes cores prim´arias aditivas:
vermelho, verde e azul.

O espa¸co de cor RGB ´e nativo para muitos dispositivos de captura de v´ıdeo. No
entanto, espa¸co de cor RGB ´e muito sens´ıvel a mudan¸cas no brilho e intensidade. Os
espa¸cos HSI e YUV s˜ao frequentemente utilizados, pois permitem que a intensidade dos
componente de crominˆancia sejam tratados separadamente. A transforma¸c˜ao ´e feita por
convers˜ao de software, que pode ser bastante caro, (KANG et al., 2008).

O principal objetivo do modelo de cores RGB ´e para a detec¸c˜ao, representa¸c˜ao, e
exibi¸c˜ao de imagens em sistemas eletrˆonicos, como televisores e computadores, embora
ele tamb´em tem sido usado em fotograﬁa convencional. Antes da era eletrˆonica, o modelo
de cor RGB j´a tinha uma teoria s´olida por tr´as dele, com base na percep¸c˜ao humana das
cores.

O espa¸co de cor RGB depende do modelo de dispositivo de cor: diferentes dispositivos
podem detectar ou reproduzir um determinado valor RGB de forma diferente, uma vez
que os elementos de cor (como f´osforos ou corantes) emitem respostas individuais para as
cores do tipo R (vermelho), e os n´ıveis G (verde) e B (azul) variam de fabricante para
fabricante, ou mesmo no pr´oprio dispositivo ao longo do tempo. Assim, um valor RGB
n˜ao deﬁne a mesma cor em todos os dispositivos sem algum tipo de gerenciamento de

28

cores.

Dispositivos de entrada t´ıpicos RGB s˜ao cˆameras de TV e v´ıdeo, scanners de imagem
e cˆameras digitais. Dispositivos de sa´ıda RGB t´ıpicos s˜ao televisores de v´arias tecnologias
(CRT, LCD, plasma, etc), computadores e telefones celulares, monitores, projetores de
v´ıdeo, multicolor LED displays e telas grandes, como JumboTron. Conforme mencionado
anteriormente as impressoras coloridas, por outro lado, n˜ao s˜ao dispositivos RGB, mas
trabalham com a subtra¸c˜ao de cores. Estes dispositivos normalmente utilizam o modelo
de cores CMYK, (BOUGHEN, 2003).

Figura 8 - Espa¸co de cores RGB e CMYK

Fonte: (PRINTBIZ, 2015)

2.1.3 Espa¸co HSV

HSV ´e a abreviatura para o sistema de cores formadas pelas componentes hue (ma-
tiz), saturation (satura¸c˜ao) e value (valor). O HSV, ﬁgura (9), tamb´em ´e conhecido como
HSB (hue, saturation e brightness - matiz, satura¸c˜ao e brilho, respectivamente). Esse
sistema de cores deﬁne o espa¸co de cor conforme descrito abaixo, utilizando seus trˆes
parˆametros:

a) Matiz (tonalidade): Veriﬁca o tipo de cor, abrangendo todas as cores do espectro,
desde o vermelho at´e o violeta, mais o magenta. Atinge valores de 0 a 360, mas
para algumas aplica¸c˜oes, esse valor ´e normalizado de 0 a 100%;

b) Satura¸c˜ao: Tamb´em chamado de pureza. Quanto menor esse valor, mais com
tom de cinza aparecer´a a imagem. Quanto maior o valor, mais pura ´e a imagem.
Atinge valores de 0 a 100%;

c) Valor (brilho): Deﬁne o brilho da cor. Atinge valores de 0 a 100%.

Figura 9 - Espa¸co de cores HSV

29

Fonte: (NESTOR PRADO, 2012)

Esse sistema foi inventado no ano de 1974, por Alvy Ray Smith. ´E caracterizada
por ser uma transforma¸c˜ao n˜ao-linear do sistema de cores RGB. Outros sistemas de cores
relacionados incluem o HSL (L de luminosity ou luminosidade) e o HSI (I de intensity ou
intensidade) (SMITH, 2006; JOBLOVE; GREENBERG, 1978; RUNGE, 1810).

Seja uma cor deﬁnida por (R, G, B), onde R, G e B est˜ao entre 0,0 e 1,0, res-
pectivamente, o maior e o menor valor poss´ıvel para cada um. A transforma¸c˜ao para os
parˆametros (H, S, V) dessa cor pode ser determinada pelas f´ormulas abaixo.

Seja MAX e MIN os valores m´aximo e m´ınimo, respectivamente, dos valores (R, G,

B):

H =



60 ×
60 ×
60 ×
60 ×

(cid:18)
(cid:18)
(cid:18)
(cid:18)

G − B
G − B
B − R
R − G

M AX − M IN
M AX − M IN
M AX − M IN
M AX − M IN

(cid:19)
(cid:19)
(cid:19)
(cid:19)

+ 0,

se M AX = R e G ≥ B

+ 360,

se M AX = R e G < B

+ 120,

se M AX = G

+ 240,

se M AX = B

M AX − M IN

M AX

S =

V = M AX

Os resultados d˜ao a tonalidade variando de 0 a 360, indicando o ˆangulo no circulo
aonde a tonalidade (H) est´a deﬁnida, e a satura¸c˜ao e o brilho variando de 0,0 a 1,0,
representando o menor e o maior valor poss´ıvel.

2.2 Reconhecimento dos contornos de uma imagem

Um contorno ´e um agrupamento de pixels que representa uma curva fechada em uma
imagem. Uma vez que uma imagem ´e dividida em componentes conectados, ´e frequen-

30

temente usual computar estat´ısticas para cada ´area (BRADSKI; KAEHLER, 2008). Todas
essas informa¸c˜oes podem ser utilizadas de forma a encontrar uma ´area de interesse em
uma imagem.

Essas estat´ısticas incluem, conforme indicado em (SZELISKI, 2010):
• ´Area do componente conectado - n´umero total de pixels do contorno;
• Per´ımetro - medida de comprimento de um contorno;
• Momentos - caracter´ısticas descritas com mais detalhes na se¸c˜ao (2.5).

Atualmente um dos melhores algoritmos usados para a extra¸c˜ao de contornos, inclu-
sive utilizado pelo pr´oprio OpenCV, foi o algoritmo desenvolvido pelos Japoneses Satoshi
Suzuki e Keiichi Abe, (SUZUKI et al., 1985).

Inicialmente, o algoritmo escaneia uma imagem bin´aria de entrada por interm´edio de
uma varredura e interrompe a varredura quando um pixel (i, j) que satisfa¸ca a exigˆencia
´e encontrada, ﬁguras (10a) e (10b).

A ﬁgura (10c) mostra como o algoritmo processa as informa¸c˜oes passo-a-passo:
• Pegando-se de forma aleat´oria o ponto A(i, j) na ﬁgura (10c), o algoritmo faz
uma varredura na tela e analisa o vizinho anterior A(i, j− 1), o vizinho posterior
A(i, j + 1) e o ponto em quest˜ao A(i, j) e veriﬁca se existe alguma informa¸c˜ao
´util (contorno). Caso haja, a informa¸c˜ao contida em A(i, j) ´e armazenada em
uma matriz. Caso contr´ario o programa segue para o ponto posterior e repete o
processo at´e o ﬁm da varredura. No caso da ﬁgura em particular, os ponto A(i, j),
B(i, j) e C(i, j) n˜ao possuem informa¸c˜oes ´uteis, portanto, nenhuma desses pontos
ser˜ao inseridos na matriz de controle.

• J´a os pontos D(i, j) at´e o ponto M (i, j) possuem informa¸c˜oes ´uteis, pois inter-
ceptam o contorno, sendo assim, suas coordenadas s˜ao inseridas na matriz de
controle e posteriormente utilizadas para construir o contorno da imagem em
quest˜ao.

Figura 10 - Figuras representativas (a) e (b), e varredura de imagem explicativa (c)

31

Fonte: Elaborada pelo autor

2.3

´Area do contorno

A fun¸c˜ao computacional da ´area do contorno utilizada neste trabalho leva em con-
sidera¸c˜ao a f´ormula de Green, equa¸c˜ao (2.1). Assim, a ´area retorna um n´umero de pixels
diferente de zero.

2.3.1 F´ormula de Green

Supondo que desejamos resolver uma equa¸c˜ao linear n˜ao homogˆenea da forma:

Lu(x) = f (x) + condi¸c˜oes de fronteira homogˆenea

(2.1)

Aqui u, f s˜ao fun¸c˜oes cujo dom´ınio ´e Ω. As condi¸c˜oes de contorno que ser˜ao consi-

deradas ser˜ao as de Dirichlet, Neumann, ou alguma mistura dos dois.

Na presente discuss˜ao, L ser´a autoadjunto em rela¸c˜ao ao produto interno habitual
L2, usando as condi¸c˜oes de contorno homogˆeneas impostas. Al´em disso, vamos supor que
a equa¸c˜ao (2.1), admite uma solu¸c˜ao ´unica para qualquer (suﬁcientemente suave) lado
direito de f . Ambas as hip´oteses podem ser relaxadas, a prop´osito, utilizando as fun¸c˜oes
adjuntas ou modiﬁcadas de Green.

O fato de que L ´e autoadjunto em rela¸c˜ao ao L2 signiﬁca que o produto interno para

32

todas as fun¸c˜oes de u, v satisfaz as condi¸c˜oes de fronteira homogˆeneas no problema (2.1),

(Lv)u dx −

(Lu)v dx = 0.

(2.2)

Ω

Ω

E se u, v n˜ao necessariamente satisﬁzerem as condi¸c˜oes de contorno homogˆeneas? Ent˜ao a
equa¸c˜ao (2.2) ainda seria verdadeira, mas os termos que envolvem a fronteira com valores
de u, v iriam aparecer:

(cid:90)

(cid:90)

(cid:90)

(cid:90)

(cid:90)

(cid:90)

(Lv)u dx −

(Lu)v dx = termos de contorno envolvendo u e v.

(2.3)

Ω

Ω

O que esta f´ormula na verdade aparenta, depende do operador linear em quest˜ao e
´e conhecida como a f´ormula de Green para o operador linear G, (GLASNER, KARL, 2014).

δ(x − x1)G(x; x2)dx −

G(x; x1)δ(x − x2)dx = 0

Ω

Ω

Usando a propriedade b´asica da fun¸c˜ao delta, simpliﬁcando temos:

G(x1; x2) − G(x2; x1) = 0

(2.4)

(2.5)

que ´e o que quer´ıamos mostrar.

Um fato relacionado tem a ver com intercˆambio de derivadas parciais de G. Tomemos

o caso unidimensional, e observe que:

∂xG(x, x0) =

(G(x + h, x0) − G(x, x0))

lim
h→0

h

(G(x0, x + h) − G(x0, x))

lim
h→0

h

=

= ∂xoG(x0, x).

(2.6)

Em outras palavras, a reciprocidade implica que se pode trocar as derivadas parciais

em rela¸c˜ao a x e x0 desde que, tamb´em possamos trocar os argumentos x e x0.

A representa¸c˜ao da f´ormula de Green usando o espa¸co livre da fun¸c˜ao de Green ´e:

(cid:90)

(cid:18)

(cid:19)

(cid:18)

(cid:19)

u(x0)(cid:79)x0

∂Ω

−1

4π|x − x0|

.ˆn +

1

4π|x − x0|

u(x) =

G(x, x0)f (x0)dxn

0 +

(cid:90)

Ω

(cid:79)u(x0).ˆndxn−1

0

(2.7)

Observe que o lado direito utiliza dados de Dirichlet (fornece um exemplo de fun¸c˜ao
que ´e descont´ınua em todos os pontos do dom´ınio) e Neumann (em matem´atica, a condi-
¸c˜ao de contorno de Neumann, ou de segundo tipo, ´e um tipo de condi¸c˜ao de contorno).
Enquanto a condi¸c˜ao de contorno de Dirichlet especiﬁca o valor da fun¸c˜ao no contorno,
a condi¸c˜ao de contorno de Neumann especiﬁca a derivada normal `a fun¸c˜ao no dom´ınio,
ou seja, ´e um ﬂuxo (HUGHES, 1995). Normalmente, esses dois n˜ao s˜ao conhecidos, ent˜ao

33

(2.7) n˜ao resolve o problema diretamente. Por outro lado, a equa¸c˜ao (2.7) ´e ´util como uma
f´ormula para ligar os valores das solu¸c˜oes e das suas derivadas normais na fronteira.

2.4 Filtros

Os processos de vis˜ao computacional, muitas vezes, necessitam de uma etapa de
pr´e-processamento envolvendo o processamento de imagens. As imagens de onde queremos
extrair alguma informa¸c˜ao em alguns casos precisam ser convertidas para um determinado
formato ou tamanho, e precisam ainda ser ﬁltradas para remover ru´ıdos provenientes do
processo de aquisi¸c˜ao da imagem.

Filtros s˜ao processos que tem por ﬁnalidade salientar determinados aspectos em
imagens digitais ou reduzir ru´ıdos. Esses ru´ıdos podem ter sido introduzidos na imagem
durante o processo de aquisi¸c˜ao da imagem, devido a limita¸c˜oes de hardware, no processo
de quantiza¸c˜ao e digitaliza¸c˜ao, pelo excesso de compress˜ao da imagem, problemas na
transmiss˜ao entre outros.

Os ﬁltros, ﬁgura (11), s˜ao as ferramentas b´asicas para remover ru´ıdos de imagens,

neste caso, o ru´ıdo ´e aquele que aparece no processo de aquisi¸c˜ao da imagem.

Os ﬁltros podem ser espaciais (ﬁltros que atuam diretamente na imagem) ou de
frequˆencia, onde a imagem ´e inicialmente transformada para o dom´ınio de frequˆencia
usando da transformada de Fourier (geralmente atrav´es da transformada de Fourier dis-
creta) e ent˜ao ´e ﬁltrada neste dom´ınio e em seguida a imagem ﬁltrada ´e transformada de
volta para o dom´ınio de espa¸co.

Figura 11 - Lado esquerdo uma imagem com ru´ıdo, e na direita a mesma imagem ap´os ﬁltragem

Fonte: (VELHO, LUIZ, 2008)

De acordo com o que a frequˆencia dos detalhes eliminados ou mantidos na imagem

os ﬁltros podem ser classiﬁcados nas classes:

• Passa-baixa;
• Passa ou rejeita banda, ou faixa de intervalo;
• Passa-alta.

34

Um ﬁltro do tipo passa-baixa deixa passar as baixas frequˆencias, mas elimina os
valores relacionados `as altas frequˆencias. Portanto, o efeito deste ﬁltro ´e o de suaviza¸c˜ao
da imagem, uma vez que as altas frequˆencias que correspondem `as transi¸c˜oes abruptas
s˜ao atenuadas. A suaviza¸c˜ao tende pelo mesmo motivo, diminuir o ru´ıdo em imagens
(OLIVEIRA, 2004).

Um ﬁltro passa banda, deixar´a presente nas imagens apenas os valores dos sinais
correspondentes `a determinada frequˆencia eliminando os demais valores. Seu efeito visual
depende da faixa predeﬁnida e geralmente ´e projetado para salientar aspectos determina-
dos, eliminar ru´ıdos ou imperfei¸c˜oes presentes em uma frequˆencia conhecida.

Um ﬁltro passa-alta deixa passar as altas frequˆencias, mas elimina os valores relaci-
onados `as baixas frequˆencias. O efeito visual deste tipo de ﬁltro ´e de tornar as transa¸c˜oes
entre diferentes regi˜oes da imagem mais n´ıtidas. O efeito indesejado ´e o de enfatizar o
ru´ıdo que possa existir na imagem.

No desenvolvimento do programa de controle via gestos, por exemplo, foram usados

os ﬁltros: mediana (Median blur) e gaussiano (Gaussian blur).

A seguir ser´a mostrado as aplica¸c˜oes nas quais os ﬁltros podem ser utilizados:

a) Restaura¸c˜ao;

b) Melhoria de imagens;

c) Atenua¸c˜ao de ru´ıdos;

d) Compress˜ao de imagens;

e) Pr´e-processamento para segmenta¸c˜ao e outras ﬁnalidades;

f) Extra¸c˜ao de features (caracter´ısticas).

2.4.1 Filtro Mediana (Median blur)

Filtro mediana ´e uma transforma¸c˜ao bastante comum (passa-banda), e utilizado

principalmente para suavizar ru´ıdos do tipo impulsivo em sinais e imagens digitais.

A ﬁltragem provoca uma suaviza¸c˜ao do sinal, ao descartar pontos muito altos ou
muito baixos com rela¸c˜ao a m´edia do sinal, sem no entanto borrar as bordas das regi˜oes
como no caso do ﬁltro passa-baixa.

A principal utilidade do ﬁltro de mediana ´e diminuir o ru´ıdo na imagem, princi-
palmente quando ele ´e formado por pequenos pontos (tamb´em conhecido como salt and
peper, ou sal e pimenta). Mas ele tamb´em ´e ´util para tornar bordas mais n´ıtidas em al-
gumas situa¸c˜oes. Parece pouco, mas muitas vezes ´e o que faz a diferen¸ca (IMAGESURVEY,
2010).

Tukey (TUKEY, 1977) foi um dos primeiros que sugeriu o uso de ﬁltros de mediana
para a suaviza¸c˜ao do sinal. Mais recentemente, Rabiner, Samgur, Schmidt (RABINER et al.,

35

1975) e Jayant (JAYANT, 1976), aplicaram ﬁltros medianas no processamento da fala, Pratt
(PRATT, 1975) e Frieden (FRIEDEN, 1976) aplicaram-no ao processamento de imagem.

O ﬁltro de mediana ´e um ﬁltro de vizinhan¸ca onde o valor de um pixel ´e substitu´ıdo

(cid:18) N + 1

(cid:19)

2

pela mediana dos pixels da sua vizinhan¸ca, f =

(HUANG et al., 1979).

No caso de dados ordenados de amostras de tamanho N, se N for ´ımpar, a mediana
. Se N for par, a mediana ser´a o resultado da m´edia simples

ser´a o elemento central (N +1)
entre os elementos N
2 + 1.

2 e N

2

Para cada vizinhan¸ca, ordena os pixels em ordem crescente de intensidade e escolhe

como sa´ıda o valor mediano - aquele que est´a no centro da sequˆencia.

Tabela 1 - Filtro Mediana

=⇒ 75 75 77 77 77 77 79 81 253 =⇒

75
77
77

77
253
75

77
81
79

=⇒

75
77
77

77
77
75

77
81
79

O ﬁltro mediana, ﬁgura (12), suaviza imagens ruidosas e em alguns casos elimina

completamente o ruido.

Figura 12 - Aplica¸c˜ao do ﬁltro mediana (direita) sobre uma imagem original (esquerda)

Fonte: (TECGRAF, 2012)

No programa de reconhecimento de gestos foi utilizado um ﬁltro mediana com uma
m´ascara de 17 x 17, para suavizar as bordas da imagem extra´ıda. Esta opera¸c˜ao de
ﬁltragem pode ser escrita matematicamente como:

g(i, j) = median(f (i + k, j + l)h(k, l))

36

onde f ´e a imagem da fonte, g ´e a imagem ﬁltrada, e h ´e a uniformemente ponderada do
n´ucleo mediano de ﬁltragem. Este passo foi necess´ario para reduzir o ru´ıdo da imagem e
o o n´umero de pontos de convexidade indesejados. A fun¸c˜ao OpenCV correspondente ´e
<medianBlur> (DU; TO, 2011).

2.4.2 Filtro Gaussiano (Gaussian blur)

O ﬁltro Gaussiano pode ser usado como um ﬁltro passa-baixa.
Usando a fun¸c˜ao Gaussiana para obter valores de uma m´ascara a ser deﬁnida digi-

talmente.

O Filtro Gaussiano em 1-D tem a forma:

G(x) =

1√
2πσ

−x2
2σ2

e

onde σ ´e o desvio padr˜ao. ´E assumido que a distribui¸c˜ao tem m´edia zero (i.e. est´a

centrada em x = 0 ). A distribui¸c˜ao ´e ilustrada na Figura (13).

Figura 13 - Forma 1-D da distribui¸c˜ao Gaussiana com m´edia zero e desvio padr˜ao um, σ = 1 na

equa¸c˜ao da fun¸c˜ao Gaussiana

Fonte: Elaborada pelo autor

Em 3-D, sua forma isotr´opica (i.e. circularmente sim´etrica) ´e dada pela equa¸c˜ao:

G(x, y) =

−x2+y2

2σ2

1
2πσ2 e

Mostrada na ﬁgura (14).

Figura 14 - Representa¸c˜ao da fun¸c˜ao Gaussiana em 3D com m´edia em (0,0) desvio padr˜ao σ = 1

37

Fonte: (UBI, 2011)

Para ﬁltros Gaussianos com maiores desvios padr˜oes pode-se montar outra m´ascara,

mas o mais usual ´e passar a mesma m´ascara mais de uma vez na imagem.

Os ﬁltros Gaussianos s˜ao muito ´uteis tamb´em em Biologia Computacional e Vi-
s˜ao, pois no processo de vis˜ao algumas c´elulas tˆem forma de resposta aproximadamente
Gaussiana.

O uso do ﬁltro Gaussiano ´e suavizar (smoothing, blur) a imagem, quase da mesma
forma que o ﬁltro de m´edia (mean ﬁlter). O resultado ser´a t˜ao mais suave quanto maior
o valor de sigma, σ, desvio padr˜ao (standard deviation) da Gaussiana usada. Desvios
padr˜oes maiores tamb´em fazem com que a m´ascara (convolution kernel) deva ser maior
para uma representa¸c˜ao adequada.

2.4.3 Filtros de Dilata¸c˜ao e Eros˜ao

As transforma¸c˜oes morfol´ogicas b´asicas s˜ao as de dilata¸c˜ao, ﬁgura (15), e eros˜ao,
ﬁgura (16) e que surgem numa vasta variedade de contextos, tais como a remo¸c˜ao de
ru´ıdo, isolando elementos individuais, e juntando elementos d´ıspares em uma imagem.

Opera¸c˜oes de morfologia mais soﬁsticadas, com base nestas duas opera¸c˜oes b´asicas,
podem tamb´em ser usadas para localizar picos de intensidade (ou buracos) em uma ima-
gem, e deﬁnir uma forma particular de gradiente de imagem (BRADSKI; KAEHLER, 2012).

Figura 15 - Dilata¸c˜ao morfol´ogica. (a) Imagem Original, (b) Imagem Dilatada, (c) Imagem Di-

latada 2x

38

Fonte: (BRADSKI; KAEHLER, 2012)

Figura 16 - Imagem morfol´ogica. (a) Imagem Original, (b) Imagem Erodizada, (c) Imagem Ero-

dizada 2x

Fonte: (BRADSKI; KAEHLER, 2012)

Para um melhor entendimento dos ﬁltros de dilata¸c˜ao e eros˜ao ser˜ao apresentadas
algumas deﬁni¸c˜oes ´uteis da teoria de conjuntos, al´em da deﬁni¸c˜ao matem´atica dos mesmos
(FILHO; NETO, 1999).

2.4.3.1 Deﬁni¸c˜oes b´asicas

Sejam A e B conjuntos em Z 2, cujos componentes s˜ao a = (a1, a2) e b = (b1, b2),
respectivamente. A transla¸c˜ao, ﬁgura (17), de A por x = (x1, x2), denotada (A)x, ´e deﬁnida
como:

(A)x = {c|c = a + x, para a ∈ A} .

Figura 17 - Conjunto

39

Fonte: Elaborada pelo autor

A reﬂex˜ao de B, ﬁgura (18), denotada ˆB, ´e deﬁnida como:

ˆB = {x|x = −b, para b ∈ B} .

Figura 18 - Reﬂex˜ao

Fonte: Elaborada pelo autor

O complemento, ﬁgura (19), do conjunto A ´e:

Ac = {x|x /∈ A}

Figura 19 - Complemento

40

Fonte: Elaborada pelo autor

Finalmente, a diferen¸ca, ﬁgura (20), entre dois conjuntos A e B, denotada A − B,

´e deﬁnida como:

A − B = {x|x ∈ A, x /∈ B} = A ∩ Bc.

Figura 20 - Diferen¸ca

Fonte: Elaborada pelo autor

2.4.3.2 Dilata¸c˜ao

Sendo A e B conjuntos de Z 2 (imagens bin´arias). A dilata¸c˜ao de A por B ´e deﬁnida

como:

(cid:110)
x|( ˆB)x ∩ A (cid:54)= ∅(cid:111)

A ⊕ B =

Portanto, o processo de dilata¸c˜ao consiste em obter a reﬂex˜ao de B sobre sua origem

41

e depois deslocar esta reﬂex˜ao de x. A dilata¸c˜ao de A por B ´e, ent˜ao, o conjunto de todos os
deslocamentos de x tais que B reﬂetido e A se sobreponham em pelo menos um elemento
n˜ao nulo.

(cid:110)

(cid:111)

A ⊕ B =

x|( ˆB)x ∩ A ⊆ A

O conjunto B ´e normalmente denominado elemento estruturante.
A ﬁgura (21), mostra os efeitos da dilata¸c˜ao de um conjunto A usando dois ele-
mentos estruturantes (B) distintos. Observar que as opera¸c˜oes morfol´ogicas s˜ao sempre
referenciadas a um elemento do conjunto estruturante (neste caso, o elemento central).

Figura 21 - Dilata¸c˜ao

Fonte: Elaborada pelo autor

2.4.3.3 Eros˜ao

Sendo A e B conjuntos de Z 2 (imagens bin´arias). A eros˜ao de A por B ´e deﬁnida

como:

AΘB = {x|(B)x ⊆ A}

O conjunto resultante da eros˜ao de A por B ´e o conjunto de todos os pontos x tais

que, B quando transladado por x, ﬁque contido em A.

A ﬁgura (22), mostra os efeitos da eros˜ao de um conjunto A usando dois elementos

estruturantes (B) distintos.

A dilata¸c˜ao e a eros˜ao s˜ao opera¸c˜oes duais entre si com respeito a complementa¸c˜ao

e reﬂex˜ao. Ou seja,

42

(AΘB)c = Ac ⊕ ˆB.

Figura 22 - Eros˜ao

Fonte: Elaborada pelo autor

2.5 Centro Geom´etrico (COG)

O centro geom´etrico ´e uma importante ferramenta pois serve para se conseguir lo-
calizar o centro de uma imagem captada. Como a imagem captada era a de uma m˜ao
humana, o COG (geometric center) passou a ser o centro da palma da m˜ao. Isso foi ne-
cess´ario para se ter uma referˆencia conhecida para, a partir da´ı, se conseguir identiﬁcar a
ponta dos dedos.

No caso de uma imagem processada os momentos espaciais que formam o COG,

M oments :: mji s˜ao computados como:

(cid:88)

x,y

mji =

(array(x, y).xj.yi)

(2.8)

O momento central, M oments :: muji s˜ao computados como:

(cid:88)

muji =

(array(x, y).(x − x)j.(y − y)i)

x,y

Onde (x, y) ´e o centro geom´etrico:

x =

m10
m00

,

y =

m01
m00

Os momentos centrais normalizados, M oments :: nuij s˜ao computados como:

nuji =

muji

1+j
2+1
00

m

43

(2.9)

(2.10)

(2.11)

Observa-se que mu00 = m00, nu00 = 1, nu10 = mu10 = mu01 = 0, portanto os valores

n˜ao s˜ao armazenados.

Os momentos de um contorno s˜ao deﬁnidos do mesmo modo mas s˜ao computados

usando-se a f´ormula de Green, (RILEY et al., 2006).

Observa-se tamb´em que desde que os momentos de contorno s˜ao calculados usando
a f´ormula de Green, pode-se obter resultados aparentemente estranhos para contornos
com auto-interse¸c˜oes, por exemplo, uma ´area de zeros (M00) para contornos em forma de
borboleta.

2.6 Convex Hull

Um conjunto convexo de pontos de uma ﬁgura ou uma regi˜ao (ﬁgura (23)), ´e con-
vexo se, para todos os pares de pontos do conjunto, os segmentos formados estiverem
inteiramente contidos no conjunto.

O algoritmo utilizado na opera¸c˜ao convex hull objetiva gerar o menor pol´ıgono que
englobe um determinado conjunto de pontos. Opera somente com uma ´unica camada de
entrada por vez, cujo tipo de geometria poder´a ser de qualquer tipo.

Em matem´atica, o convex hull ou envolt´orio convexo de um conjunto X de pontos
no plano euclidiano ou espa¸co euclidiano ´e o menor conjunto convexo que cont´em X . Por
exemplo, quando X ´e um subconjunto limitado do plano, o convex hull, ﬁgura (24), pode
ser visualizado como uma conﬁgura¸c˜ao formada por uma faixa de borracha esticada em
torno de X (BERG et al., 2000).

Figura 23 - Representa¸c˜ao visual do convex hull do conjunto fechado.

44

Fonte adaptada: (KUKURUKU, 2014)

Formalmente, o convex hull pode ser deﬁnido como a intersec¸c˜ao de todos os con-
juntos convexos que cont´em X ou como o conjunto de todas as combina¸c˜oes convexas de
pontos em X . De acordo com a ´ultima deﬁni¸c˜ao, convex hulls podem ser expandidos a
partir de espa¸cos euclidianos para os espa¸cos vetoriais reais arbitr´arios; al´em disso eles
podem tamb´em ser generalizados para matr´oides (estrutura relacionada com matrizes)
orientados (KNUTH et al., 1992).

O problema algor´ıtmico de encontrar o convex hull de um conjunto ﬁnito de pontos
no plano ou outros espa¸cos euclidianos de baixa dimensionalidade ´e um dos problemas
fundamentais da geometria computacional.

Um conjunto de pontos ´e deﬁnido para ser convexo, se ele cont´em os segmentos de
linha de liga¸c˜ao de cada par dos seus pontos. O convex hull de um determinado conjunto
de X pode ser deﬁnido como:

a) O (´unico) conjunto convexo m´ınimo contendo X;

b) A intersec¸c˜ao de todos os conjuntos convexos que cont´em X;

c) O conjunto de todas as combina¸c˜oes convexas de pontos em X;

d) A uni˜ao de todos os simplexos com v´ertices em X.

N˜ao ´e ´obvio que a primeira deﬁni¸c˜ao fa¸ca sentido: por que existe um conjunto con-
vexo m´ınimo ´unico contendo X, para cada X? No entanto, de acordo com a segunda
deﬁni¸c˜ao, a intersec¸c˜ao de todos os conjuntos de regi˜oes convexas contendo X ´e bem de-
ﬁnida, isto ´e, um subconjunto de todos os outros Y conjunto convexo que cont´em X,
porque, Y est´a inclu´ıdo entre os conjuntos a ser interceptado. Assim, ´e exatamente o con-
junto convexo m´ınimo ´unico contendo X. Cada conjunto convexo contendo X deve (pelo
pressuposto de que ´e convexo) conter todas as combina¸c˜oes convexas de pontos em X,
de modo que o conjunto de todas as combina¸c˜oes convexas est´a contido na intersec¸c˜ao
de todos os conjuntos convexos que contˆem X. Por outro lado, o conjunto de todas as
combina¸c˜oes convexas ´e em si um conjunto convexo contendo X, de modo que tamb´em

45

cont´em a intersec¸c˜ao de todos os conjuntos de convexas contendo X, e, portanto, os con-
juntos de dados por estas duas deﬁni¸c˜oes devem ser iguais. Na verdade, de acordo com
o teorema de Carath´eodory, se X ´e um subconjunto de um espa¸co vector N-dimensional,
combina¸c˜oes convexas de no m´aximo n + 1 pontos s˜ao suﬁcientes na deﬁni¸c˜ao acima.
Portanto, o fecho convexo de um conjunto X de trˆes ou mais pontos do plano ´e a uni˜ao de
todos os triˆangulos determinados pela triplos de pontos de X, e mais geralmente o convex
hull no espa¸co N-dimensional ´e a uni˜ao dos simplexos determinados por, no m´aximo, N
+ 1 v´ertices de X.

O convex hull de um conjunto ﬁnito de ponto S ´e o conjunto de todas as combina¸c˜oes
convexas de seus pontos. Em uma combina¸c˜ao convexa, xi de cada ponto de S ´e atribu´ıdo
um peso ou coeﬁciente αi de tal maneira que os coeﬁcientes s˜ao todos n˜ao-negativos e
resumidos a um, e estes pesos s˜ao utilizados para calcular uma m´edia ponderada dos
pontos. Para cada escolha de coeﬁcientes, a combina¸c˜ao resultante ´e convexa de um ponto
no convex hull, e todo o convex hull pode ser formado por escolha de coeﬁcientes de
todas as maneiras poss´ıveis. Expressando isso como uma ´unica f´ormula, o convex hull ´e o
conjunto (BERG et al., 2000; KNUTH et al., 1992; CHAZELLE, 1993):

αi (cid:62) 0 ∀ i

e

αi = 1

(2.12)

De posse dessa informa¸c˜ao, podemos escrever a equa¸c˜ao da seguinte forma:

|S|(cid:88)

i=1

|S|(cid:88)

i=1

(cid:26)

(cid:12)(cid:12)(cid:12)(cid:12) aT

αixi

(2.13)

Um conjunto solu¸c˜ao limitada de um sistema ﬁnito de desigualdades lineares pode

ser escrito matematicamente da seguinte forma:

(cid:27)

P = P (A, b) :=

x ∈ Rd

i x (cid:54) bi for 1 (cid:54) i (cid:54) m

(2.14)

Onde A ∈ Rm×d ´e uma matriz real com linhas aT

i e b ∈ Rm ´e um vetor real com

entradas bi, (HENK et al., 2004).

Figura 24 - Convex hull de um conjunto ﬁnito: analogia com uma tira el´astica (atilho).

Fonte: (CODEPROJECT, 2008)

46

2.7 Convexity Defect (defeitos de convexidade)

Outra forma ´util de compreender a forma de um objeto ou contorno ´e calcular um
casco convexo (convex hull) para o objeto e, em seguida, calcular os seus defeitos de
convexidade (vales encontrados em uma dada imagem) (HOMMA; TAKENAKA, 1985). As
formas de diversos objetos complexos s˜ao bem caracterizados por esses defeitos.

A ﬁgura (25) ilustra o conceito de um defeito de convexidade utilizando a imagem
de uma m˜ao de um ser humano. O casco convexo ´e retratado como uma linha escura ao
redor da m˜ao, e as regi˜oes quadriculadas de A a H s˜ao os defeitos em rela¸c˜ao ao casco.
Como pode ser visto, esses defeitos de convexidade oferecem um meio de caracterizar n˜ao
s´o a m˜ao, mas tamb´em o estado da m˜ao (BRADSKI; KAEHLER, 2008).

Figura 25 - Defeitos de convexidade: a linha escura do contorno em torno da m˜ao ´e um casco
convexo (convex hull); as regi˜oes quadriculadas (A-H) s˜ao defeitos de convexidade
no contorno da m˜ao em rela¸c˜ao ao casco convexo.

Fonte: (SAPACHAN, 2015)

47

3 MATERIAIS E M´ETODOS

Este cap´ıtulo aborda as caracter´ısticas gerais das principais ferramentas utiliza-
das presentes no robˆo m´ovel Robotino (equipamento alvo da pesquisa deste trabalho).
Apresenta-se a seguir a descri¸c˜ao e relevˆancia do projeto Robotino. Nele, ´e realizada a
descri¸c˜ao dos dados coletados por interm´edio dos testes no simulador virtual no robˆo
propriamente dito e a utiliza¸c˜ao da ferramenta open source OpenCV.

3.1 Biblioteca OpenCV

OpenCV (Open Source Computer Vision) ´e uma biblioteca para o desenvolvimento
de aplicativos na ´area de vis˜ao computacional. Atrav´es dela, ´e poss´ıvel fazer a an´alise,
interpreta¸c˜ao e processamento de imagens em tempo real (BRADSKI; KAEHLER, 2008).

Originalmente, desenvolvida pela Intel, em 2000, ´e uma biblioteca multiplataforma,
totalmente livre ao uso acadˆemico e comercial, para o desenvolvimento de aplicativos na
´area de Vis˜ao computacional, bastando seguir o modelo de licen¸ca BSD Intel. O OpenCV
possui m´odulos de Processamento de Imagens e Video I/O, Estrutura de dados, ´Algebra
Linear, GUI (Interface Gr´aﬁca do Usu´ario) B´asica com sistema de janelas independentes,
Controle de mouse e teclado, al´em de mais de 350 algoritmos de Vis˜ao computacional
como: ﬁltros de imagem, calibra¸c˜ao de cˆamera, reconhecimento de objetos, an´alise estru-
tural e outros. O seu processamento ´e em tempo real de imagens.

No inicio do projeto, os objetivos foram deﬁnidos como:

a) Avan¸car a pesquisa em vis˜ao computacional por prover n˜ao apenas c´odigo aberto
mas otimizado para tarefas b´asicas de vis˜ao, de modo que o c´odigo pudesse ser
prontamente lido e transfer´ıvel;

b) Avan¸car aplica¸c˜oes baseadas em vis˜ao computacional por fazer c´odigo port´atil e
otimizado dispon´ıvel de gra¸ca, com uma licen¸ca que n˜ao requer que a aplica¸c˜ao
seja de c´odigo aberto.

A vers˜ao 1.0 foi lan¸cada no ﬁnal do ano de 2006 e foi desenvolvida nas linguagens
de programa¸c˜ao C e C++. Atualmente (primeiro trimestre de 2015) a biblioteca est´a na
vers˜ao 2.4.10 (est´avel) e 3.0 (beta), e d´a suporte tamb´em para desenvolvedores Linux, Mac,
Android e iOS que desejem utiliz´a-la em seus projetos. Possuindo mais de 500 fun¸c˜oes
que podem ser aplicadas nas ´areas de Intera¸c˜ao Homem-Computador (IHC), identiﬁca¸c˜ao
de objetos, reconhecimento de face, rastreamento, reconhecimento de movimentos, dentre
outras.

OpenCV j´a possui um tempo consider´avel de vida e tamb´em possui muitos utilizado-
res. Trata-se de uma ferramenta que facilita o desenvolvimento de projetos que envolvam
manipula¸c˜ao de imagens e que, assim, contribui para aumentar tamb´em a conﬁabilidade
nas aplica¸c˜oes desenvolvidas.

48

A biblioteca OpenCV possui recursos que auxiliam o desenvolvimento de softwares
paro o reconhecimento de imagens analisadas. Para isto ´e necess´ario o uso de fun¸c˜oes e
t´ecnicas de espa¸cos de cores (ﬁltros) para que a an´alise seja efetuada (OPENCV, 2014). ´E
poss´ıvel usar, como exemplo, uma m˜ao humana, ﬁgura (26). Caso seja inserido dentro do
sistema que o padr˜ao a ser reconhecido por ele ser´a uma m˜ao aberta, o sistema procurar´a
na imagem os contornos que sejam similares a picos (ponta dos dedos - convex hull) e vales
(regi˜ao inferior entre os dedos - convex hull defects). Ao se mostrar para a cˆamera uma
m˜ao aberta, as fun¸c˜oes entrar˜ao em a¸c˜ao procurando pelos pontos v´alidos, e o sistema
passar´a a executar os procedimentos estipulados para o reconhecimento deste padr˜ao. No
momento em que a m˜ao for fechada, ela deixar´a de ser reconhecida (inexistˆencia de picos
e vales), e o sistema passar´a acusar a falta de padr˜oes na imagem. ´E por isto que se
deve prever a maior parte das poss´ıveis posi¸c˜oes e gestos da m˜ao, para que ela continue
sendo reconhecida como uma m˜ao mesmo ap´os sofrer, perante o sistema, modiﬁca¸c˜oes de
formato durante a movimenta¸c˜ao.

Figura 26 - T´ecnicas usadas: convex hull (ponta dos dedos) e convex hull defect (vales)

Fonte: (HAMLYNKINECT, 2015)

3.2 Robotino

O Robotino (ﬁgura 27) ´e um sistema rob´otico totalmente funcional, de alta qualidade
m´ovel, com uma unidade omnidirecional composta por trˆes rodas Mecanum (ﬁgura 28),
todas as quais s˜ao individualmente control´aveis, dispostas em um ˆangulo de 120◦. O
aparelho tem um sensor de para-choques em torno da sua circunferˆencia, sensores de
distˆancia infravermelhos, uma cˆamara em cores com resolu¸c˜ao VGA, codiﬁcadores de rodas
de medi¸c˜ao de potˆencia ´otica, para todo o sistema e os diversos motores, bem como um
monitor de voltagem da bateria. O sistema pode ser colocado em servi¸co imediatamente
sem a necessidade de um PC.

Figura 27 - Robotino (vista frontal)

49

Fonte: (FESTO, 2015b)

Figura 28 - Roda Mecanum usada pelo Robotino

Fonte: (FESTO, 2015e)

Al´em disso, como sensores adicionais opcionais, o robˆo pode ser equipado com um
scanner a laser preciso e um girosc´opio al´em de uma esta¸c˜ao para carregamento do robˆo,
ﬁguras 29(a), 29(b) e 29(c) respectivamente. Para a entrada e sa´ıda de sinal o Robotino
tem v´arias interfaces: USB, Ethernet, 8 entradas digitais e 8 entradas anal´ogicas, 8 sa´ıdas
digitais de sa´ıda do motor adicional para a condu¸c˜ao de cargas elevadas e entrada de
encoder adicional (ﬁgura 30).

Figura 29 - Acess´orios do Robotino (a) scanner a laser, (b) girosc´opio, (c) esta¸c˜ao para carrega-

mento

Fonte adaptada: (FESTO, 2015c)

Figura 30 - Robotino (vista frontal explicativa)

50

Fonte: (FESTO, 2015c)

O controlador do Robotino consiste em um PC embarcado com uma cart˜ao compacto
ﬂash, ﬁgura (31), para que os v´arios aplicativos de demonstra¸c˜ao e sistema operacional
(Linux com Kernel alterado) possam ser instalados. Os aplicativos podem ser iniciados
diretamente pelo painel de controle do Robotino para a execu¸c˜ao de testes.

Figura 31 - Cart˜ao de mem´oria

Fonte: (FESTO, 2015c)

O Robotino pode ser programado com o auxilio do software Robotino View em um
PC via rede wireless. O Robotino View (Apenas para o sistema operacional Windows)
´e capaz de transmitir sinais para o controlador do motor, assim como, mostrar, trocar e
avaliar valores dos sensores. O Robotino pode ainda ser programado durante uma opera¸c˜ao
atual com o Robotino View.

O Robotino possui um ponto de acesso via Wi-Fi, ﬁgura (32), que pode ser ﬁxado
diretamente na carca¸ca do robˆo para que o mesmo seja controlado por uma rede sem ﬁo.
Programadores mais experientes podem achar ´util que o robˆo seja programado em

linguagens de programa¸c˜ao de alto/m´edio n´ıvel como:

a) C, C++;

b) Java;

c) . NET;

d) Matlab, Simulink;

Figura 32 - Ponto de acesso Wi-Fi

51

Fonte: Elaborada pelo autor

e) Labview;

f) Microsoft Robotics Developer Studio.

O Robotino pode ser usado para o treinamento de pessoal, treinamento esse que

abrange as seguintes ´areas:

a) Mecˆanica

– Constru¸c˜ao mecˆanica de um sistema de robˆo m´ovel.

b) El´etrica

– Controle das unidades de acionamento.

– Fia¸c˜ao correta de componentes el´etricos.

c) Sensores

– Controle de trajet´oria guiado por sensores.

– Controle de trajet´oria anti colis˜ao com sensores de distˆancia.

– Controle de trajet´oria via processamento de imagem conseguido por inter-

m´edio de fotos de uma webcam.

d) Sistemas de controle por experiˆencia.

– Controle de unidades omnidirecionais.

e) Uso de interface de comunica¸c˜ao.

– Rede sem ﬁo (WI-FI).

f) Comissionamento

– Comissionamento de um sistema de robˆo m´ovel.

3.3 Dados t´ecnicos

Cada uma das trˆes rodas, ﬁgura (28), possui uma unidade omnidirecional, ﬁgura

(33), independente cujos componentes s˜ao listados abaixo:

52

1 Motor DC;

2 Encoder incremental;

3 Roletes para todo tipo de terreno;

4 Redutor com uma rela¸c˜ao de transmiss˜ao de 16:01;

5 Correia dentada;

Figura 33 - Unidade omnidirecional

Fonte: (FESTO, 2015f)

Tabela 2 - Tabela Representativa

Parˆametros
Voltagem

Entrada Digital

Sa´ıda Digital

Valores

24 V DC, 4.5 A

8
8

Entrada Anal´ogica

8 (0 - 10 V)

Sa´ıda dos Rel´es

2

Fonte: (FESTO, 2015f)

53

Tabela 3 - Dados de performance do motor

Motor DC GR(45x25)

Unidade de medida

Voltagem nominal
Velocidade nominal

Torque nominal
Corrente nominal
Torque de partida
Corrente de partida
Velocidade sem carga
Corrente sem carga

Corrente de desmagnetiza¸c˜ao
Momento de massa de in´ercia

Peso do motor

VDC
RPM
Ncm

A

Ncm

A

RPM

A
A

gcm2
gr.

Fonte: (FESTO, 2015f)

24
3600
3.8
0.9
20
4

4200
0.17
6.5
71
390

Tabela 4 - Redutor

Redutor planet´ario (PLG 42 S)

Est´agio ´unico, Nm:

Estagio ´unico, i:

3.5

4:1 - 8:1

Segundo est´agio, Nm:

Segundo estagio, i:

6

16:1 - 64:1

Terceiro est´agio, Nm:

Terceiro estagio, i:

14

100:1 - 512:1

Fonte: (FESTO, 2015f)

Tabela 5 - Roletes para todo tipo de terreno

Roletes todo terreno, driven (ARG 80)

Diˆametro φ

Capacidade de carga m´axima

80 mm
40 Kg

Fonte: (FESTO, 2015f)

Tabela 6 - Cˆamera VGA

54

Especiﬁca¸c˜oes t´ecnicas

Sensor de imagem

Profundidade de cores

PC - conex˜ao

Resolu¸c˜oes de v´ıdeo

Resolu¸c˜oes de imagem est´atica

VGA CMOS

24 bits true colors

USB 1.1

160 x 120, 30 fps (SQCGA)

176 x 144, 30 fps (SCIF)
320 x 240, 30 fps (QVGA)

352 x 288, 30 fps (CIF)
640 x 480 , 15 fps (VGA)

160 x 120 (SQCGA)

176 x 144 (SCIF)
320 x 240 (QVGA)

352 x 288 (CIF)
640 x 480 (VGA)

1024 x 768 (SVGA)

Formato de captura est´atica

BMP, JPG

Fonte: (FESTO, 2015f)

A unidade de controle do Robotino consiste em trˆes elementos:

a) Processador PC 104 compat´ıvel com MOPSlcdVE, 300 MHz, e o sistema opera-

cional Linux com o kernel em tempo real e mem´oria SDRAM de 128 MB;

b) Cart˜ao compacto ﬂash com a API C++ para o controle do Robotino;

c) Ponto de acesso wireless.

A energia el´etrica ´e fornecida por duas baterias recarreg´aveis de 12 V, ﬁgura (34),
com uma estimativa de 4 Ah. Ambas as baterias s˜ao montadas no chassi do robˆo. O Ro-
botino tamb´em vem com duas baterias sobressalentes e um carregador. Ambas as baterias
podem ser carregadas enquanto as outras duas est˜ao sendo usadas.

Figura 34 - Baterias usadas no Robotino

55

Fonte: (FESTO, 2015f)

Sensores para medir a distˆancia de objetos e detectar a velocidade do motor foram
integrados ao Robotino. Um sensor anti-colis˜ao montado em torno da circunferˆencia do
chassis sinaliza contato com objetos.

O Robotino ´e equipado com nove (9) sensores infravermelhos de medi¸c˜ao de distˆan-
cias que s˜ao montados no chassis do robˆo em um ˆangulo entre eles de 40◦. O Robotino
pode escanear todas as ´areas ao redor de objetos com esses sensores. Cada um dos sensores
podem ser consultados individualmente atrav´es da placa de circuitos I/O. Dessa forma os
obst´aculos podem ser evitados e distˆancias seguras podem ser mantidas. Os sensores s˜ao
capazes de medi¸c˜oes precisas para uma distˆancia relativa de 4 a 30 cm. A conex˜ao com os
sensores ´e especialmente simples, incluindo apenas um sinal de sa´ıda anal´ogica e fonte de
alimenta¸c˜ao. A eletrˆonica implantada nos sensores pode determinar a distˆancia e permitir
que o mesmo seja lido como um sinal anal´ogico.

A velocidade real de cada motor individual ´e medida em RPM pelo encoder peri-
odicamente. Se a velocidade real do motor se desviar do ponto de ajuste (setado pelo
usu´ario/programador), o programador pode usar o software Robotino View para conﬁgu-
rar os parˆametros do controlador PID, e dessa forma resolver o problema.

O sensor anti-colis˜ao, est´a instalado sob uma tira de borracha, que est´a ﬁxado em
torno da circunferˆencia do chassi. Uma cˆamara de comuta¸c˜ao est´a localizada dentro de um
perﬁl de pl´astico. Duas superf´ıcies condutoras est˜ao situadas no interior da cˆamara, com
uma determinada folga entre elas. Estas superf´ıcies entram em curto-circuito quando uma
press˜ao m´ınima (choque contra objetos) ´e aplicada `a tira. Um sinal ´e assim transmitido ao
controlador. Colis˜oes com objetos em qualquer ponto sobre a carca¸ca causam a paralisa¸c˜ao
imediata do Robotino.

Na ﬁgura (35) podem ser vistos em detalhes alguns acess´orios do Robotino:

1 Encoder incremental;

2 Sensores anti-colis˜ao sob a tira de borracha;

3 Sensores de medi¸c˜ao de distˆancia.

Figura 35 - Encoder incremental, sensores anti-colis˜ao e sensores de medi¸c˜ao de distˆancia

56

Fonte: (FESTO, 2015f)

O sensor de proximidade indutivo ´e fornecido como um componente adicional. Ele
serve para detectar objetos met´alicos no ch˜ao e ´e usado para controle de trajet´oria. lˆe os
sinais de sa´ıda de for¸ca vari´avel, dependendo se ele est´a localizado no meio ou na borda da
tira de metal. O rastreamento do caminho pode assim ser controlado de diferentes modos.

Tabela 7 - Dados t´ecnicos do sensor de proximidade

Dados T´ecnicos

Voltagem de opera¸c˜ao

15 - 30 VDC

Voltagem de sa´ıda

0 - 10 V

Tipo

SIEA-M12B-UI-S

N´umero da pe¸ca

Diˆametro
Alcance

Montagem

Frequˆencia de troca

Temperatura ambiente

Prote¸c˜ao

538292

M12

0 a 6 mm

Incorporado
−25◦ `a +70◦

1000 Hz

IP 67

Material da caixa

Lat˜ao cromado

Torque m´aximo de aperto

Reprodutibilidade

10 Nm
0.01 mm

Fonte: (FESTO, 2015f)

O ponto de acesso sem ﬁos ´e um componente que permite a comunica¸c˜ao com o robˆo

atrav´es de um endere¸co de rede. Segue abaixo informa¸c˜oes do ponto de acesso:

57

a) O ponto de acesso apresenta baixo consumo de energia. A porta de USB pode

ser utilizada como fonte de alimenta¸c˜ao/comunica¸c˜ao;

b) O ponto de acesso est´a em conformidade com as seguintes normas: IEEE 802.11g

e 802.11b;

c) Ela permite velocidades de transmiss˜ao de at´e 54 Mb/s para o padr˜ao 802-11g
e 11 MB/s para o padr˜ao 802.11b, com um grande alcance de transmiss˜ao (at´e
100 m no interior de edif´ıcios);

d) Rede conﬁ´avel e segura com fun¸c˜ao de criptograﬁa WEP e WPA-PSK;

e) ´E r´apido e simples para conﬁgurar bastando para isso usar o utilit´ario de geren-

ciamento via web.

O cart˜ao compacto do tipo ﬂash j´a cont´em o sistema operacional, as bibliotecas
de fun¸c˜oes e alguns programas inclu´ıdos (demos). As atualiza¸c˜oes podem ser facilmente
instaladas, basta substituir o cart˜ao ﬂash.

Vocˆe pode fazer o download das atualiza¸c˜oes diretamente da homepage do fabricante

(FESTO, 2014).

A interface I/O, ﬁgura (36), torna poss´ıvel ligar sensores e atuadores adicionais. Eles
s˜ao conectados atrav´es de um plug j´a inclu´ıdo. O sistema possui a seguinte conﬁgura¸c˜ao:

a) 8 entradas anal´ogicas (0 a 10 V)(AIN0 para AIN7);

b) 8 entradas digitais (Di0 para ED7);

c) 8 sa´ıdas digitais (DO0 para DO7);

d) 2 rel´es para atuadores adicionais (REL0 e REL1). Os relˆes podem ser ligados ao
contato comum (CO) como Normalmente Fechado (NF) ou normalmente aberto
(NO) para os contatos.

Figura 36 - Interface I/O (imagem esquerda). Interface I/O e suas liga¸c˜oes poss´ıveis (imagem

direita)

58

Fonte: (FESTO, 2015f)

3.4 Algoritmos de Mapeamento

Um algoritmo ´e uma sequˆencia ﬁnita de instru¸c˜oes bem deﬁnidas e n˜ao amb´ıguas,
cada uma das quais pode ser executada mecanicamente num per´ıodo de tempo ﬁnito e
com uma quantidade de esfor¸co ﬁnita (CRUZ, 2008).

O conceito de algoritmo ´e frequentemente ilustrado pelo exemplo de uma receita
culin´aria, embora muitos algoritmos sejam mais complexos, como pode ser evidenciado
na ﬁgura (37), que retrata o algoritmo do software desenvolvido neste trabalho. Eles
podem repetir passos (fazer itera¸c˜oes) ou necessitar de decis˜oes (tais como compara¸c˜oes
ou l´ogica) at´e que a tarefa seja completada. Um algoritmo corretamente executado n˜ao ir´a
resolver um problema se estiver implementado incorretamente ou se n˜ao for apropriado
ao problema.

Um algoritmo n˜ao representa, necessariamente, um programa de computador, e sim
os passos necess´arios para realizar uma tarefa. Sua implementa¸c˜ao pode ser feita por
um computador, por outro tipo de autˆomato ou mesmo por um ser humano. Diferentes
algoritmos podem realizar a mesma tarefa usando um conjunto diferenciado de instru¸c˜oes
em mais ou menos tempo, espa¸co ou esfor¸co do que outros. Tal diferen¸ca pode ser reﬂexo
da complexidade computacional aplicada, que depende de estruturas de dados adequadas
ao algoritmo. Por exemplo, um algoritmo para se vestir pode especiﬁcar que vocˆe vista
primeiro as meias e os sapatos antes de vestir a cal¸ca enquanto outro algoritmo especiﬁca
que vocˆe deve primeiro vestir a cal¸ca e depois as meias e os sapatos. Fica claro que o

primeiro algoritmo ´e mais dif´ıcil de executar que o segundo apesar de ambos levarem ao
mesmo resultado, (SANTOS, 2012).

Figura 37 - Algoritmo do programa de controle

59

Fonte: Elaborada pelo autor

O algoritmo desenvolvido neste trabalho, ﬁgura (37), veriﬁca inicialmente a presen¸ca
de uma cˆamera de v´ıdeo. Caso positivo continua a execu¸c˜ao do programa normalmente.
Caso negativo apresenta uma mensagem de erro e sai do programa. Uma vez que a cˆamera
foi localizada o software inicializa o robˆo e capta as informa¸c˜oes vindas da cˆamera, ap´os
o recebimento das informa¸c˜oes as mesmas s˜ao convertidas do espa¸co de cor RGB para
o espa¸co YCbCr. Uma vez que as imagens foram convertidas para o novo espa¸co de
cores o software procura pelos contornos presentes nas imagens. Caso n˜ao encontre nada
o software continua varrendo indeﬁnidamente todas as novas informa¸c˜oes recebidas at´e
achar o contorno de alguma dessas imagens. Quando o programa reconhece o contorno
o software calcula a ´area da imagem e veriﬁca se ela ´e maior que 5.000 pixels. Caso
negativo ignora a imagem. Caso positivo procura pelas pontas dos dedos na imagem.
Ap´os o reconhecimento da imagem ser feita o software envia para o robˆo m´ovel as ordens
referentes ao gesto interpretado.

60

3.5 Testes experimentais

Todos os testes do sistema rob´otico se deram no Laborat´orio de Manufatura e no

Laborat´orio de Tribologia, ambos situado no N´ucleo Tecnol´ogico da UFRN.

Primeiramente o robˆo foi testado usando-se para isso seus pr´oprios programas de
demonstra¸c˜ao que vieram instalados de f´abrica no robˆo autˆonomo. Os testes foram muito
importantes pois foi poss´ıvel ter acesso `as principais funcionalidades do equipamento,
tais como funcionamento dos sensores, motores, tempo de resposta do sinal enviado do
programa para o robˆo e a conﬁabilidade e robustez do software de demonstra¸c˜ao.

Ap´os todos os testes com os programas de demonstra¸c˜ao terem sido ﬁnalizados,
usou-se o software Robotivo View (software que controla o Robotino via blocos pr´e pro-
gramados), aliado ao software Robotino SIM (software que permite controlar o Robotino
em um ambiente virtual) para testar a eﬁciˆencia e robustez do conjunto de softwares que
vieram junto com o robˆo m´ovel. Os ensaios foram feitos utilizando-se um PC com sistema
operacional Windows 8, uma vez que os softwares fornecidos s´o funcionam no sistema
operacional da Microsoft. Os testes revelaram que era perfeitamente poss´ıvel controlar
o Robotino no espa¸co virtual, e isto, foi especialmente muito importante pois n˜ao seria
necess´ario usar (em um primeiro momento) o robˆo ﬁsicamente (poupando-se assim, muito
tempo durante o planejamento e desenvolvimento do software de controle), visto que, o
uso f´ısico do Robotino s´o podia se feito dentro do complexo da UFRN por quest˜oes de
seguran¸ca patrimonial.

61

4 SISTEMA DE CONTROLE POR GESTOS

Os robˆos m´oveis est˜ao cada vez mais presentes no nosso dia-a-dia, destacando em
especial `a industria, `as institui¸c˜oes de ensino e at´e mesmo em nossas casas. O objetivo
deste trabalho foi mostrar novas t´ecnicas de controle de robˆos com base no reconhecimento
de gestos feitos pelos humanos, facilitando assim a comunica¸c˜ao entre homem e m´aquina
e reduzindo assim a barreira que existem entre as novas tecnologias e seus usu´arios com
rela¸c˜ao a complexidade de gerenciamento e comandos executados pelo robˆo.

4.1 Inicializa¸c˜ao do robˆo

Durante o inicio da execu¸c˜ao do programa o primeiro ponto a ser tratado ´e a existˆen-
cia de uma cˆamera de v´ıdeo. Caso a mesma seja localizada, o programa trata de inicializar
as API’s do Robotino. Inicializa¸c˜ao essa que leva em considera¸c˜ao o IP do robˆo que ´e pas-
sado ao programa pelo roteador do pr´oprio Robotino. Ap´os isso o Robotino estar´a pronto
para receber e executar a qualquer comando enviado ao mesmo.

4.2 Fun¸c˜oes, Headers e Classe Utilizadas para Inicializa¸c˜ao do Robˆo

Para a utiliza¸c˜ao correta do Robotino fez-se uso de algumas rotinas (C++) para

controle completo do robˆo, destacando-se para isso:

• O tratamento de erros de inicializa¸c˜ao;
• A inicializa¸c˜ao propriamente dita;
• Inicializa¸c˜ao dos atuadores;
• Controle individual dos motores de todas as rodas do robˆo;
• Finalmente a ﬁnaliza¸c˜ao das atividades do robˆo.

Seguem abaixo alguns c´odigos fonte (C++) utilizados para acionamento do Robo-

tino:

• Includes utilizadas (headers):

1 // Robotino

#i n c l u d e ” r e c / r o b o t i n o /com/ a l l . h ”

3 #i n c l u d e ” r e c / c o r e l t / u t i l s . h ”
#i n c l u d e ” r e c / c o r e l t / Timer . h ”

• Classe utilizada:

62

u s i n g namespace cv ;

2 u s i n g namespace s t d ;

u s i n g namespace r e c : : r o b o t i n o : : com ;

4

c l a s s MyCom : p u b l i c Com

6 {

p u b l i c :

8 MyCom( )

{
}

10

12

14

16

18

20

22

24

26

28

30

32

34 } ;

v o i d e r r o r E v e n t ( E r r o r e r r o r , c o n s t c h a r∗ e r r o r S t r i n g )
{

s t d : : c e r r << ”E r r o r : ” << e r r o r S t r i n g << s t d : : e n d l ;

}

v o i d connectedEvent ( )
{

s t d : : c o u t << ”Connected . ” << s t d : : e n d l ;

}

v o i d c o n n e c t i o n C l o s e d E v e n t ( )
{

s t d : : c o u t << ”Connection c l o s e d . ” << s t d : : e n d l ;

}

v o i d modeChangedEvent ( b o o l
{

i s P a s s i v e M o d e )

( i s P a s s i v e M o d e )

s t d : : c o u t << ”Connected i n t p a s s i v e mode . ” << s t d : : e n d l ;

i f
{

}

}

36 MyCom com ;

Motor motor1 ;
38 Motor motor2 ;
Motor motor3 ;

40 OmniDrive omniDrive ;

Bumper bumper ;

42 s t d : : s t r i n g hostname ;

• Inicializa¸c˜ao do robˆo:

v o i d I n i c i a l i z a r o b o ( v o i d )

2 {

63

t r y
{

}
c a t c h ( c o n s t
{

}
c a t c h ( c o n s t
{

4

6

8

10

12

14

16

18

i n i t ( hostname ) ; // c o n e c t a com o s e r v i ¸c o r o b o t i n o

r e c : : r o b o t i n o : : com : : ComException& e )

s t d : : c e r r << ”Com E r r o r : ” << e . what ( ) << s t d : : e n d l ;

s t d : : e x c e p t i o n& e )

s t d : : c e r r << ”Erro : ” << e . what ( ) << s t d : : e n d l ;

}
c a t c h ( . . . )
{

s t d : : c e r r << ”Erro d e s c o n h e c i d o . . . ” << s t d : : e n d l ;

}

}

• Inicializa¸c˜ao dos atuadores:

1 v o i d i n i t ( c o n s t

{

s t d : : s t r i n g& hostname )

// I n i t i a l i z e

t h e a c t o r s

// Connect
s t d : : c o u t << ”Connecting . . . ” << s t d : : e n d l ;
com . s e t A d d r e s s ( hostname . c s t r ( ) ) ;

com . c o n n e c t ( ) ;

s t d : : c o u t << s t d : : e n d l << ”Connected ” << s t d : : e n d l ;

3

5

7

9

11

}

• Controla a velocidade individual de cada roda do robˆo:

v o i d d r i v e ( f l o a t X,

2 {

f l o a t Y,

f l o a t A)

r e c : : c o r e l t : : Timer t i m e r ;
t i m e r . s t a r t ( ) ;

i f

( com . i s C o n n e c t e d ( ) && f a l s e == bumper . v a l u e ( ) ) // && t i m e r .

msecsElapsed ( ) < 1 0 0 0 0 )

{

omniDrive . s e t V e l o c i t y (X, Y, A) ;

4

6

8

64

com . waitForUpdate ( ) ;

}

10

}

• Finaliza as atividades do robˆo:

1 v o i d d e s t r o y ( v o i d )

{

com . d i s c o n n e c t ( ) ;

3

}

4.3 Elabora¸c˜ao do Programa de Controle

Para o desenvolvimento de t´ecnicas de controle de gestos (vis˜ao computacional)
usou-se a biblioteca OpenCV 2.4.10 para facilitar a integra¸c˜ao entre C++ (visual Studio
C++) e a API 1.1 do Robotino.

A constru¸c˜ao do c´odigo de computa¸c˜ao levou em considera¸c˜ao v´arios pontos im-
portantes para que o robˆo respondesse de forma adequada aos gestos apresentados. Ini-
cialmente ´e passado para o programa o endere¸co IP default (172.26.201.1) passado pelo
roteador do Robotino. Caso o usu´ario queira passar outro endere¸co IP para o programa
basta apenas inserir o mesmo na linha de execu¸c˜ao, por exemplo, nome programa.exe [IP]
<enter>, ﬁgura (38).

Figura 38 - Exemplo de utiliza¸c˜ao da linha de comando

Fonte: Elaborada pelo autor

O programa de controle de gestos tamb´em pode testar as rea¸c˜oes do robˆo utilizando-
se para isso o simulador virtual do Robotino, ﬁgura (39), que pode ser baixado direto do
site da Festo (Robotino SIM) para veriﬁcar se o robˆo responderia adequadamente aos
comandos, sem que o mesmo esteja presente ﬁsicamente. Isto ´e muito importante, pois,

nem sempre ´e poss´ıvel ter o robˆo a disposi¸c˜ao durante todo o tempo de testes. O IP
utilizado para os testes foi o endere¸co local 127.0.0.1:8080.

Figura 39 - Exemplo de utiliza¸c˜ao da linha de comando

65

Fonte: Elaborada pelo autor

Ap´os a passagem do endere¸co o programa checa se a cˆamera foi devidamente inici-
alizada, caso negativo, apresenta uma mensagem de erro e sai do programa. Se a cˆamera
tiver sido devidamente inicializada o programa executa uma rotina para inicializar o robˆo,
usando-se para isso a API 1.1 do Robotino. Ap´os a checagem da inicializa¸c˜ao do robˆo o
programa apresenta uma tela contendo uma legenda onde constam todos os gestos com-
preendidos pelo robˆo para servir de orienta¸c˜ao ao usu´ario, como pode ser visto na ﬁgura
(40).

Figura 40 - Janela ativa contendo a legenda de reconhecimento de gestos

Fonte: Elaborada pelo autor

66

Como pode ser visto na ﬁgura (40), ap´os o reconhecimento da imagem resultar
em 0 (ausˆencia de dedos) a ordem enviada para o robˆo m´ovel ´e para que o mesmo gire
em c´ırculos. Se o reconhecimento resultar em 1 (dedo) o robˆo segue sua trajet´oria em
linha reta. Caso o reconhecimento seja 2 (dedos) o robˆo recebe a ordem para virar para
a direita imediatamente. Sendo o reconhecimento da imagem igual a 3 o robˆo recebe a
ordem para virar para a esquerda. Se o reconhecimento da imagem for 4 o robˆo anda para
traz. Finalmente se o reconhecimento da imagem for igual a 5 o robˆo cessa as atividades
e ﬁca aguardado o encerramento do programa ou o recebimento de outra ordem.

Nas ﬁguras (41) e (42) pode-se ver a representa¸c˜ao de todos os gestos interpretados

pelo programa durante a execu¸c˜ao.

Figura 41 - Visualiza¸c˜ao dos pontos interpretados: 0, 1 e 2

Fonte: Elaborada pelo autor

Figura 42 - Visualiza¸c˜ao dos pontos interpretados: 3, 4 e 5

Fonte: Elaborada pelo autor

Ap´os a inicializa¸c˜ao das rotinas citadas anteriormente o programa passa a utilizar
dois ﬁltros: ﬁltro Mediana e o ﬁltro Gaussiano, que s˜ao ﬁltros utilizados para borrar
parcialmente a imagem e com isso se conseguir uma minimiza¸c˜ao de falsos positivos. Para
os ﬁltros em quest˜ao foi necess´ario fazer alguns testes pr´aticos para se determinar qual
seriam os tamanhos de m´ascaras mais adequados para se conseguir o melhor resultado
entre desempenho computacional e identiﬁca¸c˜ao da imagem pelo programa. Ap´os v´arios

67

testes com as m´ascaras: 3x3, 5x5, 7x7, 12x12, 15x15, 17x17 e 20x20, respectivamente, para
o ﬁltro Mediana, tivemos como melhor resultado a m´ascara (matriz) de 17x17. J´a Para
o ﬁltro Gaussiano foi utilizada a mesma estrat´egia utilizada no ﬁltro Mediana, e ap´os os
testes de v´arias m´ascaras, escolheu-se uma m´ascara (matriz) de 25x25. Matrizes acima de
3x3 borram e melhoram um pouco a imagem e matrizes acima de 10x10, borram, melhoram
e destacam a imagem como um todo, por´em, impactam diretamente na velocidade de
execu¸c˜ao do programa, deixando o sistema mais lento. Depois da utiliza¸c˜ao dos ﬁltros o
programa converte o espa¸co de cor RGB (default) para o espa¸co YCbCr, pois ´e o espa¸co
de cor mais adequado para a representa¸c˜ao dos tons de pele do ser humano, fato esse
constatado ap´os os testes de outros espa¸cos de cores.

J´a no novo espa¸co de cor YCbCr os parˆametros Cb e Cr s˜ao reorganizados para uni-
rem o tom adequado de pele do usu´ario. Finalmente foram utilizados os dois ´ultimos ﬁltros
que s˜ao o ﬁltro de Dilata¸c˜ao e o ﬁltro de Eros˜ao ambos com m´ascaras de 5x5 (novamente
foram testados v´arios tamanhos de m´ascaras para a escolha dos tamanhos de m´ascaras
mais adequados em quest˜ao), que juntamente com os ﬁltros citados anteriormente s˜ao
usados para a redu¸c˜ao dos falsos positivos.

Prosseguindo-se com a execu¸c˜ao do programa o mesmo analisa as imagens captadas
pela cˆamera e tra¸ca o contorno externo, ﬁgura (43), do que foi achado. Isto ajuda no
processamento pois diminui a quantidade de pixels que ser˜ao calculados pelo processador.
O contorno ´e ent˜ao armazenado em uma matriz e de posse dessa informa¸c˜ao o programa
calcula a ´area m´axima do contorno e veriﬁca se o mesmo possui uma ´area maior ou igual
a 5.000 pixels (o valor de ´area de 5.000 pixels foi conseguidos atrav´es de alguns testes
de execu¸c˜ao do programa de controle). Caso a ´area calculada seja maior ou igual a 5.000
pixels, o programa prossegue normalmente, caso contr´ario, o mesmo continua analisando
as imagens que chegam at´e que uma delas satisfa¸ca a condi¸c˜ao inicial. A veriﬁca¸c˜ao da
´area ´e importante pois minimiza a quantidade de falsos positivos (pontos indesejados que
aparecem na imagem), uma vez que n˜ao leva em considera¸c˜ao imagens distantes da cˆamera
(a partir de 60 cm ou mais de distˆancia da cˆamera), tais imagens, exatamente por estarem
distantes n˜ao s˜ao importantes para execu¸c˜ao correta do programa.

Figura 43 - Contorno da imagem captada pela cˆamera (imagem da esquerda) e imagem real

captada (imagem da direita).

68

Fonte: Elaborada pelo autor

Ap´os a capta¸c˜ao do contorno e seu respectivo c´alculo de ´area o programa calcula o
COG (Centro Geom´etrico - ﬁgura (44)) da imagem (ponto vermelho no centro da imagem
captada) para que seja usado como referˆencia para se encontrar a(s) ponta(s) do(s) dedo(s)
para a interpreta¸c˜ao dos gestos para serem passados ao robˆo.

Figura 44 - Ponto vermelho central: COG com sua respectiva coordenada (x, y).

Fonte: Elaborada pelo autor

Com o c´alculo e localiza¸c˜ao das coordenadas (x, y) do COG conclu´ıdas o programa
utiliza duas rotinas denominadas de convex hull e convexity defect que s˜ao respons´aveis
respectivamente para procurar os picos (ponta dos dedos - ﬁngertips) e os vales (espa¸co
entre os dedos) das imagens percebidas pela cˆamera para conseguir reconhecer quantos
dedos est˜ao sendo visualizados e interpretados pelo programa. As informa¸c˜oes coletadas
s˜ao armazenadas em duas matrizes para, posteriormente, serem utilizadas com outras
t´ecnicas agregadas para se conseguir reduzir os falsos positivos.

De posse das informa¸c˜oes contidas nas matrizes, usamos essas informa¸c˜oes para
conseguirmos o ponto m´edio de cada vizinhan¸ca de conjunto de pontos armazenados.
Isso ´e importante para diminuirmos a incidˆencia de pontos que n˜ao s˜ao desejados (falsos
positivos) uma vez que teremos apenas um par de coordenadas (x, y) para cada aglomerado

69

de pontos pr´oximos. O programa considera que pontos pr´oximos s˜ao os pontos que est˜ao `a
uma distˆancias m´axima de 5 pixels (A distˆancia de 5 pixels foi conseguida por interm´edio
de testes pr´aticos em tempo de execu¸c˜ao do software) uns dos outros. Atrav´es desse
artif´ıcio conseguimos separar tanto as nuvens contendo as coordenadas das vizinhan¸cas
de pontos quanto seus pontos m´edios.

Finalmente o programa tem todas as informa¸c˜oes dispon´ıveis para a correta inter-
preta¸c˜ao da informa¸c˜ao visual e consequentemente repassa para o robˆo m´ovel a a¸c˜ao que
o mesmo deve executar de acordo com a ordem pr´e deﬁnida. A legenda explicativa pode
ser visualizada de acordo com a ﬁgura (40).

70

5 AN ´ALISE E DISCUSS ˜AO DOS RESULTADOS

Este cap´ıtulo foi desenvolvido para tratar informa¸c˜oes pertinentes ao desenvolvi-

mento do programa de controle.

5.1

´Area M´ınima da Imagem

Mediante a realiza¸c˜ao de testes experimentais, foi poss´ıvel estabelecer algumas cons-
tantes que passaram a ser usadas no programa referido, dentre eles podem ser citadas a
´area m´ınima da imagem. A ´area min´ıma deﬁnida no programa de computa¸c˜ao precisa
ser maior ou igual `a 5.000 pixels. Isso foi um dos fatores necess´arios para se conseguir
uma redu¸c˜ao de alguns problemas causados por falsos positivos (pontos indesejados que
atrapalham a interpreta¸c˜ao da imagem como um todo). Dessa forma o programa ignora
todas as imagens fornecidas que n˜ao atendam a deﬁni¸c˜ao inicial imposta durante o pro-
cessamento, com isso, se consegue reduzir o efeito indesejado dos falsos positivos, pois
todas as imagem que s˜ao captadas pela webcam que n˜ao estejam pr´oximas o suﬁciente da
cˆamera (range de 15 `a 60 cm de distˆancia da cˆamera) ou que, sejam menores que 5.000
pixels ser˜ao desprezadas, e como as mesmas n˜ao s˜ao processadas, se reduz tanto o es-
for¸co computacional aplicado sobre o processador quanto os ru´ıdos nas imagens. A ﬁgura
(45a), representa uma imagem interpretada corretamente e a ﬁgura (45b) representa uma
imagem com a presen¸ca de falsos positivos.

Figura 45 - (a) Interpreta¸c˜ao correta da imagem. (b) Presen¸ca de falsos positivos na imagem

(pontos 1, 2 e 3)

Fonte: Elaborada pelo autor

A ﬁgura (46), mostra como parte da imagem ´e desprezada (parte vis´ıvel do ombro)
pois a mesma possui uma ´area m´ınima de enquadramento menor que 5.000 pixels, mesmo
estando dentro do range de 15 `a 60 cm de distˆancia da cˆamera, dando ˆenfase apenas a
imagem da m˜ao.

Figura 46 - Parte do ombro ´e desprezada

71

Fonte: Elaborada pelo autor

5.2 Problemas Enfrentados Durante o Desenvolvimento do Programa de

Controle e Suas Respectivas Solu¸c˜oes

Durante a elabora¸c˜ao e execu¸c˜ao das v´arias vers˜oes do programa de controle via
gestos (foram sete vers˜oes no total) foi poss´ıvel nos depararmos com v´arios problemas
que n˜ao haviam sido pensados ou previstos anteriormente. No entanto, as diﬁculdades
encontradas motivaram o desenvolvimento ﬁnal do software de controle, pois foi poss´ıvel
perceber a evolu¸c˜ao cont´ınua do controle de reconhecimento de imagens e gestos ap´os a
implementa¸c˜ao de cada nova vers˜ao.

Dentre os principais problemas e poss´ıveis solu¸c˜oes que ocorreram durante o processo

de planejamento, cria¸c˜ao e execu¸c˜ao do projeto destacam-se os seguintes itens:

• Falsos positivos:

– Problema:

∗ Um dos maiores problema enfrentados, pois a incidˆencia dos mesmo ´e
inerente ao uso de espa¸co de cores (no caso do programa foi utilizado
o espa¸co de cor YCbCr) e a ilumina¸c˜ao do local na qual o software
ser´a executado. Os falsos positivos s˜ao ru´ıdos encontrados na imagem
que enganam o programa de controle, pois destaca pontos que n˜ao
deveriam existir, tais pontos resultam em informa¸c˜oes que ser˜ao mal
interpretadas pelo programa gerando tanto pontos fantasmas como
aumento do processamento geral do software, a ﬁgura (45b) mostra o
aparecimento dos falsos positivos.

– Solu¸c˜ao:

∗ Para a solu¸c˜ao/diminui¸c˜ao dos problemas encontrados com os falsos
positivos, foram utilizados ﬁltros para suavizar as imagens captadas,
algoritmos para calcular a ´area m´ınima das imagens que poderiam ser

72

processadas pelo software (m´ınimo de 5.000 pixels) e algoritmos de
convex hull e convexity defect para a escolha correta dos v´arios pontos
de coordenadas das vizinhan¸cas que eram encontradas no programa.
Para cada massa de pontos vizinhos era setando apenas uma coorde-
nada (x,y), com isso, foi poss´ıvel reduzir ru´ıdos na imagem.

• Filtros:

– Problema:

∗ Durante a elabora¸c˜ao e o planejamento do programa foram testados
seis ﬁltros: Mediana, Gaussiano, Dilata¸c˜ao, Eros˜ao, Borr˜ao e Adaptivo
Bilateral, isso foi necess´ario em virtude de cada ﬁltro possuir suas pe-
culiaridades individuais, sendo importante assim, conhecer na pr´atica
como cada um deles iria atuar no reconhecimento da imagem. Os ﬁl-
tros foram cruciais para uma melhor interpreta¸c˜ao e suaviza¸c˜ao das
imagem captadas pela webcam, por´em, em fun¸c˜ao da quantidade de
ﬁltros e do tamanho de suas respectivas m´ascaras utilizadas o esfor¸co
computacional para interpreta¸c˜ao das informa¸c˜oes foi sensivelmente
incrementado. No total foram testados os seguintes ﬁltros:

· O ﬁltro Mediana foi testado com m´ascaras de (3x3), (5x5), (10x10),
(17x17) e (22x22);
· O ﬁltro Gaussiano foi testado com m´ascaras de (5x5), (10x10),
(17x17), (22x22) e (25x25);
· Os ﬁltros Dilata¸c˜ao e Eros˜ao foram testados com m´ascaras de
(3x3), (5x5) e (10x10);
· O ﬁltro Borr˜ao foi testado com m´ascaras de (5x5) e (10x10);
· O ﬁltro Adaptivo Bilateral foi testado com m´ascaras de (5x5) e
(8x8);

– Solu¸c˜ao:

∗ Sem a utiliza¸c˜ao de nenhum dos ﬁltros, veriﬁcou-se que o desempenho
do programa foi m´aximo, por´em, o reconhecimento das imagens fo-
ram muito baixas ou at´e nulas. Ap´os testes durante o desenvolvimento
do software foram utilizados quatro ﬁltros (Mediana, Gaussiano, Di-
lata¸c˜ao e Eros˜ao), tendo-se conseguido com isso um bom desempenho
e reconhecimento de imagens. Continuando-se com os testes optou-
se pela aplica¸c˜ao de todos os seis ﬁltros, onde, veriﬁcou-se que o de-
sempenho ﬁnal do programa (com rela¸c˜ao aos quatro ﬁltros testados
anteriormente) caiu muito com ganhos t´ımidos ou nulos de melhoria
da imagem, sendo assim, optou-se pelo uso de quatro desses ﬁltros

73

para que o desempenho do software n˜ao fosse t˜ao baixo. A escolha dos
quatro ﬁltros se deu exclusivamente atrav´es de testes em tempo de
execu¸c˜ao. Infelizmente n˜ao foi poss´ıvel reduzir o tamanho das m´asca-
ras para algo em torno de (3x3), para priorizar o desempenho ﬁnal do
programa, pois, com a redu¸c˜ao das mesmas (os testes de redu¸c˜ao do
tamanho das m´ascaras dos ﬁltros foram feitos durante a execu¸c˜ao do
software), a incidˆencia de ru´ıdos na imagem aumentou de forma signi-
ﬁcativa. O resultado ﬁnal da carga de processamento do software ﬁnal
ﬁcou dentro do esperado, portanto, a escolha tanto dos ﬁltros quanto
dos seus tamanhos de m´ascaras se mostrou uma decis˜ao acertada.

∗ Os ﬁltros escolhidos foram:

· O ﬁltro Mediana foi usado com uma m´ascara de (17x17);
· O ﬁltro Gaussiano foi usado com uma m´ascara de (25x25);
· Os ﬁltros Dilata¸c˜ao e Eros˜ao foram usados com uma m´ascara de
(5x5).

Vale ressaltar que a escolha do tamanho de todas as m´ascaras dos ﬁltros esco-
lhidos se deu exclusivamente ap´os a realiza¸c˜ao de testes em tempo de execu¸c˜ao
do programa, sendo portanto os mais adequados para o controle ﬁnal.

• Cˆamera:

– Problema:

∗ A princ´ıpio foi pensado na utiliza¸c˜ao da cˆamera do pr´oprio Robotino,
por´em, em fun¸c˜ao da sua baixa resolu¸c˜ao (VGA - 640x480) essa op¸c˜ao
foi descartada, pois poderia gerar ru´ıdos na imagem captada.

– Solu¸c˜ao:

∗ A cˆamera utilizada, ﬁgura ((47) - ponto 3), para a capta¸c˜ao das in-
forma¸c˜oes foi a do pr´oprio notebook (resolu¸c˜ao HD), por´em n˜ao havia
a presen¸ca de um sensor de profundidade (como os encontrados no
Xbox - Kinect) que iria ajudar na diminui¸c˜ao dos falsos positivos. No
entanto, foi poss´ıvel termos bons resultados na capta¸c˜ao das imagens
para o processamento do software, uma vez que, o reconhecimento da
ponta dos dedos foi feito de forma satisfat´oria para ser usado no con-
trole do robˆo autˆonomo (Robotino).

Figura 47 - Notebook utilizado, ASUS G750JX

74

Fonte: (ASUS, 2015)

5.3 Estat´ısticas do processo

Durante o desenvolvimento do programa deﬁnitivo de controle do Robotino foi neces-
s´aria a cria¸c˜ao de sete vers˜oes (programas de computa¸c˜ao) para se chegar `a algo satisfat´orio
e funcional.

Pode-se perceber de acordo com a ﬁgura (48), que na primeira vers˜ao do programa
existe uma incidˆencia muito grande de falsos positivos, defeitos esses que s˜ao ampliados
em fun¸c˜ao da ilumina¸c˜ao do local onde se est´a realizando os ensaios e do espa¸co de cores
utilizado. Neste programa foi utilizado o espa¸co de cores YCbCr (mais adequado para
trabalhos de identiﬁca¸c˜ao usando-se o tom de pele dos seres humanos), por´em a l´ogica de
controle dos contornos ainda n˜ao estava bem desenvolvida.

O grande problema enfrentado nessa etapa de desenvolvimento foi a escolha cor-
reta dos ﬁltros que seriam aplicados (com seus tamanhos de m´ascaras adequados) e uma
implementa¸c˜ao mais eﬁciente da l´ogica de programa¸c˜ao que n˜ao considerava de forma
eﬁciente a redu¸c˜ao dos falsos positivos e a utiliza¸c˜ao correta das informa¸c˜oes da matriz
de contornos.

Depois da ocorrˆencia desses problemas (em tempo de execu¸c˜ao), optou-se por de-
senvolver uma segunda vers˜ao do programa de controle para minimizar os problemas
encontrados anteriormente e de certa forma manter uma vers˜ao de seguran¸ca, caso algo
de errado acontecesse, em fun¸c˜ao da mudan¸ca da estrutura de programa¸c˜ao do programa
original.

Figura 48 - Programa 1: Contorno da imagem captada pela cˆamera (imagem da esquerda) e

imagem real captada (imagem da direita).

75

Fonte: Elaborada pelo autor

No desenvolvimento da segunda vers˜ao do programa pode-se observar que o reco-
nhecimento do contorno melhorou, em fun¸c˜ao da implementa¸c˜ao de uma melhor l´ogica da
utiliza¸c˜ao das informa¸c˜oes da matriz de contorno e varia¸c˜ao no range da escala de cores
para melhor adequa¸c˜ao do tom de pele, ﬁgura (49). No entanto os falsos positivos continu-
avam atrapalhando o reconhecimento correto da imagem, sendo necess´ario a abordagem
de uma l´ogica de programa¸c˜ao que levasse em considera¸c˜ao a cria¸c˜ao de uma ´area m´axima
de reconhecimento de imagem para se reduzir a incidˆencia de falsos positivos e reduzir o
esfor¸co computacional sobre o processador do computador.

Figura 49 - Programa 2: Contorno da imagem captada pela cˆamera (imagem da esquerda) e

imagem real captada (imagem da direita).

Fonte: Elaborada pelo autor

Na implementa¸c˜ao da terceira e quarta vers˜oes, ﬁgura (50), foi adotada uma ´area
de reconhecimento de imagem para um m´ınimo de 5.000 pixels ´uteis, uma vez que, com a
utiliza¸c˜ao dessa t´ecnica de ﬁltragem de ´area foi poss´ıvel eliminar alguns ru´ıdos que podem
ser vistos facilmente nas imagens processadas pelas vers˜oes 1 e 2, ﬁguras (48) e (49).

76

Chegou-se ao n´umero de 5.000 pixels ap´os a realiza¸c˜ao de uma s´erie de ensaios. Ap´os a
delimita¸c˜ao da ´area foi poss´ıvel se concentrar em reduzir os falsos positivos, uma vez que
o aparecimento dos mesmos ´e algo inerente a obten¸c˜ao da imagem (espa¸co de cores +
ilumina¸c˜ao do ambiente de teste) e por isso n˜ao ´e poss´ıvel elimin´a-lo em 100%.

Figura 50 - Programas 3 e 4: Contorno da imagem captada pela cˆamera (imagem da esquerda),
imagem negativa ´util da imagem (centro) e imagem real captada (imagem da direita).

Fonte: Elaborada pelo autor

Durante o processo de avalia¸c˜ao do software, veriﬁcou-se que seria necess´ario o de-
senvolvimento de uma quinta vers˜ao para se veriﬁcar a inclus˜ao dos conceitos de convex
hull (se¸c˜ao 2.6) e convex defect (se¸c˜ao 2.7) para identiﬁca¸c˜ao de picos e vales na imagem,
informa¸c˜oes essas que seriam cruciais para o reconhecimentos das pontas dos dedos na
imagem recebida, ﬁgura (51).

Figura 51 - Programa 5: Contorno da imagem captada pela cˆamera (imagem da esquerda) e

imagem real captada (imagem da direita).

Fonte: Elaborada pelo autor

Nas vers˜oes seis e sete (´ultima) foram implementadas as fun¸c˜oes de inicializa¸c˜ao do
robˆo e dos sensores propriamente dito, e a implementa¸c˜ao dos ﬁltros Mediana, utilizando-se
uma matriz 17x17 (Subse¸c˜ao 2.4.1), Gaussiano, utilizando-se uma matriz 25x25 (Subse-
¸c˜ao 2.4.2) e ﬁltros de dilata¸c˜ao e eros˜ao, utilizando-se uma matriz 5x5 (Subse¸c˜ao 2.4.3).
Al´em de melhorias nos algoritmos de tamanho do contorno, deﬁni¸c˜ao ﬁnal do range do

77

espa¸co de cores YCbCr e reﬁnamento dos c´odigos de programa¸c˜ao que controla as fun¸c˜oes
respons´aveis pelo convex hull e convex defect para minimizar o aparecimento de falsos po-
sitivos e interpreta¸c˜ao errˆonea da imagem captada. Todas as imagens em quest˜ao podem
ser visualizadas nas ﬁguras (41) e (42).

Ap´os a implementa¸c˜ao da ´ultima vers˜ao o programa funcionou de forma esperada

tanto com o simulador RobotinoSIM como no robˆo propriamente dito.

A tabela (8) foi criada para mostrar a evolu¸c˜ao das interpreta¸c˜oes das informa¸c˜oes

ao longo do desenvolvimento das vers˜oes criadas.

Tabela 8 - Informa¸c˜oes sobre percentuais de reconhecimento de imagem

Progs.

Rec. da Imagem M˜ao fechada (0)

Dedos (1)

P1
P2
P3
P4
P5
P6
P7

Fraco, 65%
Fraco, 70%

Moderado, 78%
Moderado, 82%

Bom, 88%
Bom, 91%
´Otimo, 94%

N˜ao Aplicado
N˜ao Aplicado
N˜ao Aplicado
N˜ao Aplicado

N˜ao Aplicado
N˜ao Aplicado
N˜ao Aplicado
N˜ao Aplicado

Dedos (2)

N˜ao Aplicado
N˜ao Aplicado
N˜ao Aplicado
N˜ao Aplicado

Dedos (3)

N˜ao Aplicado
N˜ao Aplicado
N˜ao Aplicado
N˜ao Aplicado

Dedos (4)

N˜ao Aplicado
N˜ao Aplicado
N˜ao Aplicado
N˜ao Aplicado

Dedos (5)

N˜ao Aplicado
N˜ao Aplicado
N˜ao Aplicado
N˜ao Aplicado

50%
70%
89%

60%
90%
96%

50%
90%
93%

50%
90%
92%

60%
88%
91%

55%
90%
90%

Fonte: Elaborada pelo autor

Pode-se veriﬁcar que o intuito inicial da confec¸c˜ao das quatro primeiras vers˜oes era o
de aprimorar o reconhecimento das imagens recebidas, visto que, para que o software ﬁnal
pudesse funcionar adequadamente as imagens teriam que ser percebidas com o menor
n´umero de ru´ıdos poss´ıvel. S´o a partir da vers˜ao cinco (o reconhecimento correto da
imagem j´a estava se tornando uma realidade), foi poss´ıvel o tratamento dos gestos da
m˜ao humana com uma melhor qualidade visual.

As vers˜oes 1 e 2 possu´ıam um reconhecimento de imagem ainda n˜ao satisfat´orio,
n˜ao sendo poss´ıvel a identiﬁca¸c˜ao de nenhum gesto recebido pela cˆamera. As vers˜oes 3 e
4 j´a tiveram um reconhecimento de imagem melhor, por´em, devido a presen¸ca de v´arios
falsos positivos n˜ao foi poss´ıvel o reconhecimento adequado dos gestos. A partir da vers˜ao
5, com um reconhecimento aproximado de 88%, j´a foi poss´ıvel veriﬁcar o reconhecimento
de alguns gestos, mesmo n˜ao sendo de forma completa e exata, sendo portanto necess´ario
o desenvolvimento das vers˜oes 6 e 7 para conseguir-se um bom reconhecimento de gestos
para controlar o robˆo m´ovel.

78

6 CONSIDERA ¸C ˜OES FINAIS

Neste cap´ıtulo s˜ao feitas as considera¸c˜oes ﬁnais sobre todo o trabalho que foi ne-
cess´ario para o desenvolvimento e conclus˜ao do software de controle, al´em de sugest˜oes
para trabalhos futuros que podem melhorar ainda mais o programa de reconhecimento de
gestos.

6.1 Conclus˜oes

Com base nas pesquisas feitas ao longo deste trabalho foi poss´ıvel veriﬁcar que o uso
de vis˜ao computacional para reconhecimento de gestos e controle de robˆos m´oveis ainda ´e
algo em estado muito inicial no Brasil. Acredita-se que este trabalho possa trazer contri-
bui¸c˜oes importantes nesta linha de pesquisa, uma vez que foi poss´ıvel o reconhecimento
de gestos humanos por interm´edio de rotinas implementadas pelo autor deste trabalho
aliado a rotinas desenvolvidas pela biblioteca de c´odigo aberto OpenCV.

J´a com a experiˆencia obtida durante o desenvolvimento de aplica¸c˜oes utilizando
a biblioteca OpenCV, veriﬁcou-se que a mesma realmente oferece facilidades e avan¸cos
importantes para a cria¸c˜ao de aplica¸c˜oes de vis˜ao computacional. Pode-se dizer que esta
biblioteca (j´a que a mesma est´a em constante atualiza¸c˜ao) possui potencial para aumentar
de forma signiﬁcativa as pesquisas na ´area de vis˜ao computacional, pois al´em de ser uma
biblioteca rica em funcionalidades (fun¸c˜oes e procedimentos, al´em de uma boa documen-
ta¸c˜ao), ´e uma biblioteca de uso gratuito e de c´odigo-fonte aberto.

Com respeito ao Robotino, foi poss´ıvel conﬁrmar que o mesmo ´e realmente muito
´util, pois al´em de ser um robˆo autˆonomo m´ovel de alta qualidade e tecnologia embarcada
muito bem criada, possui uma integra¸c˜ao muito boa com a maioria das linguagens de
programa¸c˜ao de auto n´ıvel, al´em de possuir dois softwares nativos que s˜ao o Robotino
VIEW e o Robotino SIM que s˜ao respectivamente respons´aveis pela programa¸c˜ao de
blocos para facilitar a programa¸c˜ao do robˆo e de um ambiente virtual para facilitar os
testes do Robotino sem a presen¸ca f´ısica do mesmo.

Outro ponto que n˜ao pode deixar de ser comentado foi a utiliza¸c˜ao de uma infraes-
trutura muito boa por parte da UFRN (Universidade Federal do Rio Grande do Norte)
que propiciou a utiliza¸c˜ao dos Laborat´orios de Manufatura, Tribologia e Laborat´orios de
Inform´atica de forma irrestrita para a conclus˜ao com ˆexito deste trabalho.

Os principais pontos observados neste trabalho foram a importˆancia dos ﬁltros na
suaviza¸c˜ao das imagens, do controle dos espa¸cos de cores para adequar os padr˜oes dos
diversos tipos de tons de pele dos seres humanos e da interpreta¸c˜ao dos gestos humanos
para controle de um robˆo m´ovel, conceito esse que pode ser utilizado para controlar
qualquer tipo de equipamento, facilitando assim a interpreta¸c˜ao das informa¸c˜oes pelo
software e melhorando seu desempenho ﬁnal, al´em da facilidade com a intera¸c˜ao mais
intuitiva entre o homem e a m´aquina.

79

6.2 Sugest˜oes para Trabalhos Futuros

Para trabalhos futuros pode-se melhorar a velocidade geral do algoritmo com a
otimiza¸c˜ao das fun¸c˜oes de ﬁltros e das fun¸c˜oes de controle da localiza¸c˜ao dos picos e vales
da imagem. Al´em da possibilidade de reconhecimento de gestos mais soﬁsticados com a
integra¸c˜ao completa de m´ultiplas m˜aos, propiciando inclusive a interpreta¸c˜ao de sinais,
dire¸c˜oes e sentidos que por ventura sejam captados pela cˆamera. Isto poderia deixar o uso
do robˆo m´ovel mais intuitivo e simples, podendo ser ser utilizado por qualquer usu´ario
experiente ou novato, melhorando a integra¸c˜ao entre homem/m´aquina.

A utiliza¸c˜ao de uma cˆamera de alta deﬁni¸c˜ao aliada a um sensor de profundidade
infra vermelho (algo similar pode ser visto no Kinect, que pode ser utilizado no Microsoft
Xbox One) pode ajudar muito no reconhecimento de articula¸c˜oes do corpo humano. O
pr´oprio Kinect, ﬁgura (52), seria uma boa plataforma para aquisi¸c˜ao de imagens, uma vez
que al´em do pre¸co relativamente baixo o Kinect (menos de $ 200,00 d´olares) possui:

• Cerca de 23 cm de comprimento;
• Cˆamera Full HD;
• Suporta linguagens de programa¸c˜ao como C++, C#, VB.Net, Cx e JavaScript.
• Sensor de profundidade infra vermelho;
• Microfone embutido;
• Processador embarcado e software;
• Detecta 48 pontos de articula¸c˜ao do nosso corpo, ou seja, possui uma precis˜ao

muito boa.

Figura 52 - Kinect utilizado no Xbox One

Fonte: (MICROSOFT, 2015)

A utiliza¸c˜ao conjunta das bibliotecas OpenCV e OpenNI, que possuem excelentes
rotinas para utiliza¸c˜ao no campo de vis˜ao computacional. Inclusive o pr´oprio OpenNI, a
qual possui fun¸c˜oes e procedimentos para serem utilizadas com o Kinect.

Aquisi¸c˜ao de novos acess´orios para o robotino, pois o mesmo oferece uma gama
muito grande para ampliar ainda mais o uso e a facilidade de programa¸c˜ao do mesmo.
Dentre os principais acess´orios que podem ser adquiridos, destacam-se:

80

• Pin¸ca El´etrica: para permitir que o Robotino possa agarrar objetos, com for¸ca

m´axima de aperto de 140 N, ﬁgura (53a);

• M´odulo Forklift: para permitir que o Robotino possa levantar objetos, com ca-

pacidade de i¸camento de at´e 4 Kg, ﬁgura (53b);

• Laser Scanner: para permitir que o Robotino possa fazer o mapeamento da ´area
de trabalho, localiza¸c˜ao e navega¸c˜ao, assim como reconhecimento de obst´aculos.
Alcance entre 20 - 5600 mm, ﬁgura (53c).

Figura 53 - Acess´orios do Robotino

Fonte: (FESTO, 2015a)

81

REFERˆENCIAS

AGUIRRE L. A.; DA SILVA, A. C. M. D. A. W. Enciclop´edia de Autom´atica:
Controle e Automa¸c˜ao. S˜ao Paulo: Editora Blucher, 2007. Vol.3 (Rob´otica). 20

ARA ´UJO, S. A. de; LIBRANTZ, A. F. H. Vis˜ao e inteligˆencia computacionais aplicadas
a navega¸c˜ao autˆonoma de robˆos. Exacta, Universidade Nove de Julho, v. 4, n. 2, p.
343–352, 2006. 18

ASUS. ASUS Manual Notebook G750JX. 2015. Dispon´ıvel em:
<https://docs.google.com/viewer?url=http%3A%2F%2Fdlcdnet.asus.com%2Fpub%
2FASUS%2Fnb%2FG750JW%2FPG_eManual_G750JW_VER7780.pdf>. Acesso em: 16 de
mar¸co 2015. 74

BERG, M. D.; KREVELD, M. V.; OVERMARS, M.; SCHWARZKOPF, O. C.
Computational geometry. [S.l.]: Springer, 2000. 43, 45

BOUGHEN, N. LightWave 3D 7.5 Lighting. [S.l.]: Wordware Publishing, Inc., 2003.
28

BRADSKI, G.; KAEHLER, A. Learning OpenCV: Computer vision with the
OpenCV library. [S.l.]: O’Reilly Media, Inc., 2008. 30, 46, 47

. Learning OpenCV: Computer vision in C++ with the OpenCV

library. [S.l.]: O’Reilly Media, Inc., 2012. 37, 38

CHAI, D.; NGAN, K. N. Face segmentation using skin-color map in videophone
applications. Circuits and Systems for Video Technology, IEEE Transactions
on, IEEE, v. 9, n. 4, p. 551–564, 1999. 26

CHAZELLE, B. An optimal convex hull algorithm in any ﬁxed dimension. Discrete &
Computational Geometry, Springer, v. 10, n. 1, p. 377–409, 1993. 45

CODEPROJECT. Convex Hull. 2008. Dispon´ıvel em:
<http://www.codeproject.com/Articles/29275/Convex-Hull>. Acesso em: 23 de
fevereiro 2015. 45

CROWLEY, J. L.; CHRISTENSEN, H. I.; CHEHIKIAN, A. Vision as process: basic
research on computer vision systems. [S.l.]: Springer Science & Business Media,
1994. 17

CRUZ, A. J. d. O. Algoritmos. N´ucleo de Computa¸c˜ao Eletrˆonica da
Universidade Federal do Rio de Janeiro. P´agina visitada em, v. 12, 2008. 58

DOMINGUES, D. M. G. Ciberespa¸co e rituais: tecnologia, antropologia e criatividade.
Horizontes Antropol´ogicos, SciELO Brasil, v. 10, n. 21, p. 181–197, 2004.

DU, H.; TO, T. Hand gesture recognition using kinect. Techical Report, Boston
University, 2011. 36

FAIRCHILD, M. D. Color appearance models. [S.l.]: John Wiley & Sons, 2013. 23

82

FESTO. Festo. 2014. Dispon´ıvel em: <http://www.festo-didactic.com/br-pt/
sistemas-de-ensino/robotino/?fbid=YnIucHQuNTM3LjIzLjIwLjg1OA>. Acesso em:
01 de fevereiro 2014. 18, 57

. Festo Didactic. 2015. Dispon´ıvel em:

<http://www.festo-didactic.com/br-pt/sistemas-de-ensino/robotino/?fbid=
YnIucHQuNTM3LjIzLjIwLjg1OA&page=2&offset=0&showitems=8>. Acesso em: 17 de
mar¸co 2015. 80

. Robotino vista frontal. 2015. Dispon´ıvel em:

<http://sysmagazine.com/posts/113536/>. Acesso em: 21 de fevereiro 2015. 49

. Robotino vista frontal explicativa. 2015. Dispon´ıvel em:

<https://www.festo-didactic.com%2Fdownload.php%3Fname%3DRobotino_Campus%
2520Party.pdf>. Acesso em: 21 de fevereiro 2015. 49, 50

. Robotino with hokuyo. 2015. Dispon´ıvel em:

<http://en.wikipedia.org/wiki/Robotino#mediaviewer/File:
Robotino_with_Hokuyo_URG-04LX-UG01.jpg>. Acesso em: 21 de fevereiro 2015. 18

. Roda mecanum. 2015. Dispon´ıvel em:

<http://grabcad.com/library/festo-wheel-robot-robotino>. Acesso em: 21 de
fevereiro 2015. 49

. Unidade omni direcional. 2015. Dispon´ıvel em: <http://www.

festo-didactic.com/int-en/services/printed-media/technical-documentation/
robotino-manual-544305.htm?fbid=aW50LmVuLjU1Ny4xNy4zMi45MDguNjcwOA>.
Acesso em: 21 de fevereiro 2015. 52, 53, 54, 55, 56, 58

FILHO, O. M.; NETO, H. V. Processamento digital de imagens. [S.l.]: Brasport,
1999. 38

FORSYTH, D. A.; PONCE, J. Computer vision: a modern approach. [S.l.]:
Prentice Hall Professional Technical Reference, 2002. 16

FRIEDEN, B. R. A new restoring algorithm for the preferential enhancement of edge
gradients. In: INTERNATIONAL SOCIETY FOR OPTICS AND PHOTONICS.
Image Processing. [S.l.], 1976. p. 44–48. 35

GATTER, M. Getting it right in print: digital pre-press for graphic designers.
[S.l.]: Laurence King Publishing, 2005. 24

GLASNER, KARL. Mathematics. 2014. Dispon´ıvel em:
<http://math.arizona.edu/~kglasner/math456/greens.pdf>. Acesso em: 20 de
fevereiro 2014. 32

HAMLYNKINECT. Hand detection algorithms. 2015. Dispon´ıvel em:
<http://hamlynkinect.wikispaces.com/Hand+Detection+Algorithms>. Acesso em:
22 de fevereiro 2015. 48

HENK, M.; RICHTER-GEBERT, J.; ZIEGLER, G. M. 16 basic properties of convex
polytopes. Handbook of discrete and computational geometry, CRC Press,
p. 355, 2004. 45

83

HOMMA, K.; TAKENAKA, E.-i. An image processing method for feature extraction of
space-occupying lesions. Journal of nuclear medicine: oﬃcial publication,
Society of Nuclear Medicine, v. 26, n. 12, p. 1472–1477, 1985. 46

HONDA. Honda. 2015. Dispon´ıvel em: <http:
//thecoolgadgets.com/wp-content/uploads/2011/04/honda-asimo-soccer.jpg>.
Acesso em: 21 de fevereiro 2015. 17

HORVAT, L. Digital Imaging: Essential Skills. [S.l.]: Focal Press, 2003. 24

HUANG, T.; YANG, G.; TANG, G. A fast two-dimensional median ﬁltering algorithm.
Acoustics, Speech and Signal Processing, IEEE Transactions on, IEEE, v. 27,
n. 1, p. 13–18, 1979. 35

HUGHES, T. J. Multiscale phenomena: Green’s functions, the dirichlet-to-neumann
formulation, subgrid scale models, bubbles and the origins of stabilized methods.
Computer methods in applied mechanics and engineering, Elsevier, v. 127, n. 1,
p. 387–401, 1995. 32

IBA, S. An architecture for gesture-based control of mobile robots. The Institute for
Complex Engineered Systems Carnegie Mellon University, Carnegie Mellon
University, v. 1, n. 1, p. 1–7, 1999. 17

IMAGESURVEY. 2010. Dispon´ıvel em:
<http://www.imagesurvey.com.br/2010/03/filtro-de-mediana/>. Acesso em: 18
de fevereiro 2014. 34

IRAJI, M. S.; TOSINIA, A. Skin color segmentation in ycbcr color space with adaptive
fuzzy neural network (anﬁs). International Journal of Image, Graphics & Signal
Processing, v. 4, n. 4, 2012. 26

J ´ACOBO, J. E. A. Desenvolvimento de um Robˆo Autˆonomo M´ovel Vers´atil
utilizando Arquitetura Subsumption. Disserta¸c˜ao (Disserta¸c˜ao de Mestrado) —
Universidade Estadual de Campinas, 2001. 18

JAYANT, N. Average-and median-based smoothing techniques for improving digital
speech quality in the presence of transmission errors. Communications, IEEE
Transactions on, IEEE, v. 24, n. 9, p. 1043–1045, 1976. 35

JENNINGS, S. Artist’s Color Manual: The Complete Guide to Working with
Color. [S.l.]: Chronicle Books, 2003. 24

JOBLOVE, G. H.; GREENBERG, D. Color spaces for computer graphics. In: ACM.
ACM siggraph computer graphics. [S.l.], 1978. v. 12, n. 3, p. 20–25. 29

JONES, M. J.; REHG, J. M. Statistical color models with application to skin detection.
International Journal of Computer Vision, Springer, v. 46, n. 1, p. 81–96, 2002. 26

KANG, S. K.; NAM, M. Y.; RHEE, P. K. Color based hand and ﬁnger detection
technology for user interaction. In: IEEE. Convergence and Hybrid Information
Technology, 2008. ICHIT’08. International Conference on. [S.l.], 2008. p.
229–236. 27

84

KAURA, H. K. Gesture controlled robot using image processing. International
Journal of Advanced Research in Artiﬁcial Intelligence, IJARAI, v. 2, n. 5, p.
69–77, 2013. 17

KNUTH, D. E.; KNUTH, D. E.; KNUTH, D. E. Axioms and hulls. [S.l.]:
springer-Verlag Berlin, 1992. 44, 45

KUKURUKU. Building a minimal Convex Hull. 2014. Dispon´ıvel em:
<http://kukuruku.co/hub/algorithms/building-a-minimal-convex-hull>.
Acesso em: 23 de fevereiro 2015. 44

LINUX MAGAZINE ONLINE. Linux. 2015. Dispon´ıvel em:
<http://www.linuxnewmedia.com.br/images/uploads/news/ccv_pedestrian.png>.
Acesso em: 21 de fevereiro 2015. 16

MAKE HUMAN. Make human tool for making 3D characters. 2015. Dispon´ıvel
em: <http://forum.makehuman.org/viewtopic.php?f=8&t=1529>. Acesso em: 22 de
fevereiro 2015. 26

MICROSOFT. Microsoft Store. 2015. Dispon´ıvel em:
<https://devices.microsoftstore.com/pt-BR/xbox-one/console.html>. Acesso
em: 17 de mar¸co 2015. 79

MUIR, P. F. Modeling and control of wheeled mobile robots. Tese (Doutorado)
— Citeseer, 1988.

NESTOR PRADO. Nestor prado SCAD 2012. 2012. Dispon´ıvel em:
<http://www.sfdm.scad.edu/faculty/mkesson/vsfx755/wip/best/spring2012/
nestor_prado/project3.html>. Acesso em: 22 de fevereiro 2015. 29

OLIVEIRA, S. L. G. de. Desenvolvimento de um algoritmo baseado no ﬁltro de
gabor para identiﬁca¸cao de impressoes digitais. Tese (Doutorado) —
Universidade do Estado do Rio de Janeiro, 2004. 34

OPENCV. OpenCV. 2014. Dispon´ıvel em: <http://opencv.org/>. Acesso em: 01 de
fevereiro 2014. 19, 48

ORLANDINI, G. Desenvolvimento de Aplicativos Baseados em T´ecnicas de
Vis˜ao Computacional para Robˆo M´ovel Autˆonomo. Disserta¸c˜ao (Disserta¸c˜ao de
Mestrado) — Universidade Metodista de Piracicaba, 2012. 17

POYNTON, C. Digital video and HD: Algorithms and Interfaces. [S.l.]: Elsevier,
2012. 25, 26

PRATT, W. K. Median ﬁltering. in Semiannual Report, Image Processing
Institute, Univ. of Southern California, p. 116–123, 1975. 35

PRINTBIZ. Printbiz espa¸cos de cores RGB e CMYK. 2015. Dispon´ıvel em:
<http://blog.printbiz.jp/archives/306303.html>. Acesso em: 22 de fevereiro
2015. 28

RABINER, L.; SAMBUR, M.; SCHMIDT, C. Applications of a nonlinear smoothing
algorithm to speech processing. Acoustics, Speech and Signal Processing, IEEE
Transactions on, IEEE, v. 23, n. 6, p. 552–557, 1975. 35

85

RILEY, K. F.; HOBSON, M. P.; BENCE, S. J. Mathematical methods for physics
and engineering. [S.l.]: Cambridge University Press, 2006. 43

RUNGE, P. O. Die Farben-Kugel, oder Construction des Verhaeltnisses aller
Farben zueinander. [S.l.]: Hamburg, Germany: Perthes. Albert Henry Munsell (1905).
A Color Notation. Boston, MA: Munsell Color Company. See also Fairchild (2005), and
Munsell Color System and its references, 1810. 29

SABER, E.; TEKALP, A. M. Frontal-view face detection and facial feature extraction
using color, shape and symmetry based cost functions. Pattern Recognition Letters,
Elsevier, v. 19, n. 8, p. 669–680, 1998. 26

SANTOS, E. Educa¸c˜ao online para al´em da ead: um fenˆomeno da cibercultura. SILVA,
M., PESCE, L.; ZUIN, A. Educa¸c˜ao Online: cen´ario, forma¸c˜ao e quest˜oes
did´atico metodol´ogicas. Rio de Janeiro: Wak, 2010.

SANTOS, M. Programa¸c˜ao para computa¸c˜ao. Universidade Federal do Vale de S˜ao
Francisco, 2012. 59

SAPACHAN. Harth of darkness. 2015. Dispon´ıvel em: <http://sapachan.
blogspot.com.br/2010/04/learning-opencv-contour-convexity-and.html>.
Acesso em: 22 de fevereiro 2015. 46

SMITH, A. R. Elder Bethuel Riggs (1757-1835) of Morris County, New Jersey,
and His Family Through Five Generations. [S.l.]: Newbury Street Press, 2006. 29

SOBOTTKA, K.; PITAS, I. A novel method for automatic face segmentation, facial
feature extraction and tracking. Signal processing: Image communication, Elsevier,
v. 12, n. 3, p. 263–281, 1998. 26

SUZUKI, S. et al. Topological structural analysis of digitized binary images by border
following. Computer Vision, Graphics, and Image Processing, Elsevier, v. 30,
n. 1, p. 32–46, 1985. 30

SZELISKI, R. Computer vision: algorithms and applications. [S.l.]: Springer
Science & Business Media, 2010. 21, 30

TECGRAF. Processamento de imagens. 2012. Dispon´ıvel em:
<http://webserver2.tecgraf.puc-rio.br/~mgattass/fcg/trb12/Eliana%
20Goldner/images.html>. Acesso em: 22 de fevereiro 2015. 35

TECMUNDO. Natureza da cor. 2009. Dispon´ıvel em:
<http://www.tecmundo.com.br/video/2481-o-que-e-espaco-de-cores-.htm>.
Acesso em: 23 de fevereiro 2015. 24

THE GLOWING PYTHON. The glowing python. 2015. Dispon´ıvel em:
<http://1.bp.blogspot.com/-mh3ZEdrlTQo/TqGDmyAGLeI/AAAAAAAAAJY/
k1I9sA2Ffo8/s1600/harris.jpg>. Acesso em: 21 de fevereiro 2015. 19

TUKEY, J. W. Exploratory data analysis. Reading, Mass., 1977. 34

UBI. Processamento e reconhecimento de imagem m´edica. 2011. Dispon´ıvel em:
<http://deteccaodeimagem.com.sapo.pt/filtrogaussiano.html>. Acesso em: 22
de fevereiro 2015. 37

86

UNICAMP. Convers˜ao entre espa¸cos de cores RGB e YCbCr. 2001. Dispon´ıvel
em: <http://www.ic.unicamp.br/~cpg/material-didatico/mo442/200101/matlab/
entregues/grupo1/lab2/exercicio_3/index.html>. Acesso em: 23 de fevereiro 2015.
25

VELHO, LUIZ. Material de aula do curso de Image Processing and Computer
Graphics, IMPA. 2008. Dispon´ıvel em:
<http://www.visgraf.impa.br/Courses/ipcg.html>. Acesso em: 17 de fevereiro
2014. 33

VICTORINO, A. Controle de trajet´oria e estabiliza¸c˜ao de robˆos m´oveis
n˜ao-holonˆomicos. Controle de trajet´oria e estabiliza¸c˜ao de robˆos m´oveis
n˜ao-holonˆomicos, 1998. 18

WALDHERR, S. A gesture based interface for human-robot interaction. Kluwer
Academic Publishers, Kluwer Academics, v. 1, n. 1, p. 1–39, 2000. 17

A APˆENDICE

Nesta sess˜ao encontram-se dispon´ıveis c´odigos-fontes de algumas fun¸c˜oes/procedi-

mentos relacionados aos experimentos realizados no programa de vis˜ao computacional.

Fun¸c˜ao Respons´avel pelo C´alculo do COG

Point C Cog ( Mat Tela , Mat Skin )

2 {

4

6

8

10

Moments moment = moments ( Skin ,
d o u b l e xCenter = moment . m10 / moment . m00 ;
d o u b l e yCenter = moment . m01 / moment . m00 ;
Point c e n t e r ( ( i n t ) xCenter ,
( i n t ) yCenter ) ;
s t r i n g s c o g = ” [ ” + t o s t r i n g ( c e n t e r . x ) + ” , ” + t o s t r i n g ( c e n t e r . y ) + ” ] ”

t r u e ) ;

;

cv : : c i r c l e ( Tela , c e n t e r , 8 , S c a l a r ( 0 , 0 , 2 5 5 ) , CV FILLED) ;

// C i r c u l o do C .O.G

cv : : l i n e ( Tela , Po int ( c e n t e r . x − 1 0 , c e n t e r . y ) , Point ( c e n t e r . x + 1 0 ,
c e n t e r . y ) , S c a l a r : : a l l ( 2 5 5 ) , 1 , 8 ) ;

// Cruz H o r i z o n t a l

cv : : l i n e ( Tela , Point ( c e n t e r . x ,

c e n t e r . y − 1 0 ) , Point ( c e n t e r . x ,

c e n t e r . y +

1 0 ) , S c a l a r : : a l l ( 2 5 5 ) , 1 , 8 ) ;

// Cruz V e r t i c a l

12

cv : : putText ( Tela ,

scog , Poi nt ( c e n t e r . x ,

c e n t e r . y ) ,

FONT HERSHEY COMPLEX SMALL, 1 , S c a l a r : : a l l ( 2 5 5 ) , 1 , 8 ) ;

// Coordenada

do C.O.G

r e t u r n c e n t e r ;

14

}

Procedimento para a Extra¸c˜ao de Features da Imagem

1 v o i d s k i n E x t r a c t ( c o n s t Mat &frame , Mat &s k i n A r e a )

{

3 Mat YCbCr ;

v e c t o r <Mat> p l a n e s ;

5

7

9

11

13

cv : : medianBlur ( frame ,
// cv : : b l u r ( frame ,
G a u ss ia nBlu r ( frame ,

frame , 1 7 ) ;

frame , S i z e ( 1 0 , 1 0 ) , Poi nt (−1 ,−1) , 4 ) ;
frame , S i z e ( 2 5 , 2 5 ) , 0 , 0 , BORDER DEFAULT) ;

c v t C o l o r ( frame , YCbCr , CV RGB2YCrCb) ;
s p l i t (YCbCr , p l a n e s ) ;

// Uso de i t e r a t o r s : a c e s s a r o s e l e m e n t o s da m a t r i z
M a t I t e r a t o r <uchar> i t C b = p l a n e s [ 1 ] . begin <uchar >() ;

15 M a t I t e r a t o r <uchar> i t C b e n d = p l a n e s [ 1 ] . end<uchar >() ;

M a t I t e r a t o r <uchar> i t C r = p l a n e s [ 2 ] . begin <uchar >() ;

17 M a t I t e r a t o r <uchar> i t s k i n = s k i n A r e a . begin <uchar >() ;

88

// Espa¸co de c o r YCbCr d i s t r i b u i ¸c ˜a o
// Cor da p e l e humana : 100 <= Cb <= 1 2 7 , 138 <= Cr <= 170
// HSV − S c a l a r ( 0 , 0 , 3 9 ) , S c a l a r ( 3 6 0 , 4 9 , 3 6 0 )
f o r
{

i t C b != i t C b e n d ; ++i t C r , ++it Cb , ++i t s k i n )

( ;

i f

( 1 0 0 <= ∗ i t C b && ∗ i t C b <= 127 && 138 <= ∗ i t C r && ∗ i t C r <= 1 7 0 )
∗ i t s k i n = 2 5 5 ;

e l s e

∗ i t s k i n = 0 ;

}

cv : : d i l a t e ( skinArea ,
cv : : e r o d e ( skinArea ,

skinArea , Mat ( 5 , 5 , CV 8UC1) , Point (−1 , −1) ) ;
skinArea , Mat ( 5 , 5 , CV 8UC1) , Point (−1 , −1) ) ;

19

21

23

25

27

29

31

}

Procedimento para a Amostragem de Textos na Tela

v o i d Texto ( s t r i n g& t e x t , Mat& show ,

i n t e s c a l a x ,

i n t e s c a l a y ,

s t r i n g& t e l a )

textOrg ( 1 0 ∗ e s c a l a x , 30 + e s c a l a y ) ;
f o n t F a c e = FONT HERSHEY COMPLEX SMALL;

Point
i n t
d o u b l e f o n t S c a l e = 2 ;
i n t
S c a l a r c o r = S c a l a r ( 0 , 2 5 5 , 0 ) ;

t h i c k n e s s = 1 ;

// v e r d e

namedWindow ( t e l a , CV WINDOW AUTOSIZE) ;
putText ( show ,
f o n tF a c e ,
imshow ( t e l a ,

t e x t ,
show ) ;

textOrg ,

f o n t S c a l e , cor ,

t h i c k n e s s , 8 ) ;

2 {

4

6

8

10

12 }

4

6

8

10

12

Procedimento para a Amostragem das Legendas na Tela

v o i d Legendas ( Mat &l e g e n d a ,

2 {

s t r i n g &t e l a )

f o n t F a c e = FONT HERSHEY COMPLEX SMALL;

c o n t r o l e = 0 ;

u n s i g n e d i n t
i n t
d o u b l e f o n t S c a l e = 2 ;
i n t
S c a l a r c o r = S c a l a r ( 0 , 2 5 5 , 0 ) ;
v e c t o r <s t r i n g > t e x t o =
{

t h i c k n e s s = 1 ;

” 0 : Robo g i r a em c i r c u l o s . ” ,
” 1 : Robo seguem em f r e n t e . ” ,
” 2 : Robo v i r a a d i r e i t a . ” ,

// v e r d e

89

14

16

18

20

22

24

26

28

” 3 : Robo v i r a a e s q u e r d a . ” ,
” 4 : Robo v a i para t r a z . ” ,
” 5 : Robo para . ”

} ;

namedWindow ( t e l a , CV WINDOW AUTOSIZE) ;

( u n s i g n e d i n t

i = 0 ;

i < t e x t o . s i z e ( ) ;

i ++)

f o r
{

Point ptex ( 1 0 , 30 + c o n t r o l e ) ;

t e x t o . a t ( i ) , ptex ,

f o n tF a c e ,

f o n t S c a l e , cor ,

t h i c k n e s s

putText ( l e g e n d a ,
, 8 ) ;
c o n t r o l e += 5 0 ;

}

imshow ( t e l a ,

l e g e n d a ) ;

}

