Laborat´orio Nacional de Computa¸c˜ao Cient´ıﬁca

Programa de P´os Gradua¸c˜ao em Modelagem Computacional

Programa¸c˜ao Modular e Computa¸c˜ao de Alto

Desempenho em um Simulador de Reservat´orios n˜ao

Convencionais de G´as em Folhelhos

Por

Rafael Nardes Moreira

PETR ´OPOLIS, RJ - BRASIL

MAR ¸CO DE 2016

PROGRAMA ¸C ˜AO MODULAR E COMPUTA ¸C ˜AO DE ALTO

DESEMPENHO EM UM SIMULADOR DE RESERVAT ´ORIOS N ˜AO

CONVENCIONAIS DE G ´AS EM FOLHELHOS

Rafael Nardes Moreira

DISSERTA ¸C ˜AO SUBMETIDA AO CORPO DOCENTE DO LABORAT ´ORIO

NACIONAL DE COMPUTA ¸C ˜AO CIENT´IFICA

COMO PARTE DOS

REQUISITOS NECESS ´ARIOS PARA A OBTEN ¸C ˜AO DO GRAU DE MESTRE

EM CIˆENCIAS EM MODELAGEM COMPUTACIONAL

Aprovada por:

Prof. Eduardo L´ucio Mendes Garcia, D.Sc

(Presidente)

Prof. Elson Magalh˜aes Toledo, D.Sc.

Prof. Jos´e Roberto Pereira Rodrigues, D.Sc.

PETR ´OPOLIS, RJ - BRASIL

MAR ¸CO DE 2016

Moreira, Rafael Nardes 

M837p 

Programação modular e computação de alto desempenho em um simulador 
de reservatórios não convencionais de gás em folhelhos / Rafael Nardes Moreira 
-- Petrópolis, RJ. : Laboratório Nacional de Computação Científica, 2016. 

xv, 117 p. : il. ; 29 cm. 

Orientadores:  Eduardo Lúcio Mendes Garcia e Sandra Mara Cardoso Malta. 

Dissertação (Mestrado) – Laboratório Nacional de Computação Científica, 

2016. 

1. Programação modular 2. Programação orientada a objetos 3.Simulação de

reservatórios 4. Gás de folhelhos  I. Garcia, Eduardo Lúcio Mendes II. Malta, 
Sandra Mara Cardoso III. MCT/LNCC; IV.Título 

CDD – 005.112 

“Ladies and gentlemen, we have detected

gravitational waves. We did it!”

(David Reitze, Caltech physicist and LIGO

lab director.)

“Until this moment, we had our eyes on the

sky and we couldn’t hear the music. The

skies will never be the same.”

(Szabolcs Marka, Columbia University

astrophysicist.)

iv

Dedicat´oria

Dedico este trabalho a meus pais, Fernando

e Maria Helena.

v

Agradecimentos

`A minha fam´ılia e namorada, por oferecem a mim o todo incentivo e

compreens˜ao que se pudesse deles esperar.

Ao meu orientador, Bidu, pelo incentivo ao trabalho, disponibilidade e

proximidade durante o ´ultimo ano.

`A minha coorientadora, Sandra Malta, pelo acompanhamento e opini˜oes em

nossas reuni˜oes peri´odicas.

`A professora Carla Osthoﬀ, pelo acompanhamento do trabalho e pelo

interesse em colaborar, sugerindo ferramentas de software e bibliograﬁa, muito

´uteis ao trabalho.

`A colega Patr´ıcia Costa, pela paciˆencia em responder perguntas e, a cada

uma delas, colocar-se a disposi¸c˜ao para a pr´oxima.

Ao professor Roberto Souto, pela disponibilidade sempre que solicitado e

pela prontid˜ao ao disponibilizar uma m´aquina para experimentos quanto isso de

fez necess´ario.

Aos demais professores do programa de p´os-gradua¸c˜ao do LNCC com os

quais interagi e aprendi durante os dois ´ultimos anos.

`A ANP - Agencia Nacional do Petr´oleo, G´as Natural e Biocombust´ıveis pelo

apoio ﬁnanceiro.

vi

Resumo da Disserta¸c˜ao apresentada ao LNCC/MCT como parte dos requisitos

necess´arios para a obten¸c˜ao do grau de Mestre em Ciˆencias (M.Sc.)

PROGRAMA ¸C ˜AO MODULAR E COMPUTA ¸C ˜AO DE ALTO

DESEMPENHO EM UM SIMULADOR DE RESERVAT ´ORIOS N ˜AO

CONVENCIONAIS DE G ´AS EM FOLHELHOS

Rafael Nardes Moreira

Mar¸co , 2016

Orientador: Eduardo L´ucio Mendes Garcia, D.Sc

Co-orientador: Sandra Mara Cardoso Malta, D.Sc.

A modelagem computacional de reservat´orios ´e o instrumento que permite

a descri¸c˜ao precisa dos fenˆomenos f´ısicos existentes no processo de recupera¸c˜ao de

´oleo e g´as, tendo grande interesse tanto para a ind´ustria quanto para a ciˆencia.

Na ind´ustria de ´oleo e g´as, ´e grande a demanda por simuladores comerciais de

reservat´orios. Por outro lado, simuladores cient´ıﬁcos s˜ao capazes de oferecer

aos pesquisadores do dom´ınio, o controle e a liberdade necess´arios `a atividade

acadˆemica.

Dentre as principais demandas do software cient´ıﬁco em geral est˜ao (i) o

design escal´avel, relacionado ao desenvolvimento de c´odigo de maneira organizada

e modular, contribuindo para sua evolu¸c˜ao e (ii) a execu¸c˜ao escal´avel, relacionada

`a implementa¸c˜ao de t´ecnicas de computa¸c˜ao paralela e de alto desempenho,

em raz˜ao das grandes massas de dados manipuladas e dos modelos num´ericos

computacionalmente intensivos produzidos pela ciˆencia.

Este trabalho trata do emprego de t´ecnicas de programa¸c˜ao modular com

orienta¸c˜ao a objetos e de computa¸c˜ao paralela com OpenMP e MPI em um

simulador cient´ıﬁco, escrito em Fortran e utilizado na modelagem num´erica de

problemas de escoamento em reservat´orios n˜ao convencionais de g´as em folhelhos.

vii

Abstract of Dissertation presented to LNCC/MCT as a partial fulﬁllment of the

requirements for the degree of Master of Sciences (M.Sc.)

MODULAR PROGRAMMING AND HIGH PERFORMANCE

COMPUTING IN A GAS SHALE RESERVOIR SIMULATOR

Rafael Nardes Moreira

March, 2016

Advisor: Eduardo L´ucio Mendes Garcia, D.Sc

Co-advisor: Sandra Mara Cardoso Malta, D.Sc.

Computer modeling of reservoirs is the tool that provides the accurate

description of the existing physical phenomena in the oil and gas recovery process,

being of interest to both the industry and science. In oil and gas industry, the

demand of commercial simulators is remarkable. At the same time, scientiﬁc

simulators are able to provide researchers with the freedom and control needed

by the academic activity.

Among the major demands of scientiﬁc software are: (i) the scalable design,

which is correlated with organized and modular code development, and (ii) the

scalable execution, related to the implementation of techniques for parallel and

high performance computing, due to the large amount of manipulated data and

the compute-intensive numerical models produced by science.

This dissertation aims to the application of techniques for modular object-

oriented programming and parallel computing, with OpenMP and MPI, in a

scientiﬁc simulator, developed in Fortran and used in the numerical modeling of

problems related to gas ﬂow on unconventional gas-shale reservoirs.

viii

Sum´ario

1 Introdu¸c˜ao

1.1 O simula¸c˜ao de reservat´orios no LNCC . . . . . . . . . . . . . . . .

1.2 O modelo 2 escalas: Fratura e Bloco . . . . . . . . . . . . . . . . .

1.3 Programa¸c˜ao Modular com Orienta¸c˜ao a Objetos

. . . . . . . . . .

1.4 Computa¸c˜ao paralela para desempenho do simulador

. . . . . . . .

1.5 Organiza¸c˜ao da disserta¸c˜ao . . . . . . . . . . . . . . . . . . . . . . .

2 Demandas da evolu¸c˜ao do software cient´ıﬁco

2.1 Programa¸c˜ao Modular

. . . . . . . . . . . . . . . . . . . . . . . . .

2.1.1 Princ´ıpios da Programa¸c˜ao Modular

. . . . . . . . . . . . .

2.1.2 Abstra¸c˜oes . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.1.3

Justiﬁcativas, vantagens e desvantagens da programa¸c˜ao

modular . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.1.4 T´ecnicas de modulariza¸c˜ao . . . . . . . . . . . . . . . . . . .

2.1.5 Programa¸c˜ao Orientada a Objetos . . . . . . . . . . . . . . .

2.2 Computa¸c˜ao Paralela . . . . . . . . . . . . . . . . . . . . . . . . . .

2.2.1 Arquiteturas Paralelas . . . . . . . . . . . . . . . . . . . . .

2.2.2 Mem´oria compartilhada . . . . . . . . . . . . . . . . . . . .

2.2.3 Mem´oria Distribu´ıda . . . . . . . . . . . . . . . . . . . . . .

2.2.4 Ambientes de Computa¸c˜ao Paralela . . . . . . . . . . . . . .

3 Refatora¸c˜ao de um m´odulo do simulador com inclus˜ao do paradigma

ix

1

3

4

7

8

9

11

13

15

17

21

25

32

37

38

41

42

43

orientado a objetos

3.1 Evolu¸c˜ao do simulador de escoamentos em meios porosos . . . . . .

3.2 Organiza¸c˜ao do simulador em arquivos para compila¸c˜ao separada .

3.3 Orienta¸c˜ao a Objetos no M´odulo dos Sistemas de Equa¸c˜oes . . . . .

3.3.1

Identiﬁca¸c˜ao das Entidades de Interesse . . . . . . . . . . . .

3.3.2

Introduzindo os Atributos e M´etodos das Classes

. . . . . .

48

49

57

60

60

63

3.3.3

Implementa¸c˜ao de Orienta¸c˜ao a Objetos em Fortran: Heran¸ca 66

3.3.4

Implementa¸c˜ao do Polimorﬁsmo em Fortran: . . . . . . . . .

68

3.3.5 A nova organiza¸c˜ao do simulador com os novos m´odulos

contendo classes . . . . . . . . . . . . . . . . . . . . . . . . .

70

4 Paraleliza¸c˜ao do simulador 2 escalas de Shale Gas

4.1 An´alise inicial do perﬁl serial de desempenho da aplica¸c˜ao com intel

VTUNE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.2 A estrat´egia de paraleliza¸c˜ao com OpenMP . . . . . . . . . . . . . .

4.3 Testes de desempenho I: Paraleliza¸c˜ao com OpenMP . . . . . . . .

4.3.1 Ambiente de Execu¸c˜ao e Metodologia dos testes . . . . . . .

4.3.2 Resultados do experimento (A) com OpenMP . . . . . . . .

4.3.3 Resultados do experimento (B) com OpenMP . . . . . . . .

4.3.4 Discuss˜ao dos resultados e otimiza¸c˜ao com OpenMP . . . . .

78

78

84

92

95

96

97

98

4.4 A estrat´egia de paraleliza¸c˜ao com MPI

. . . . . . . . . . . . . . . . 104

4.5 Testes de desempenho II: Paraleliza¸c˜ao com OpenMP + MPI . . . . 108

4.5.1 Ambiente de Execu¸c˜ao e Metodologia dos testes . . . . . . . 108

4.5.2 Resultados do experimento (A) com OpenMP+MPI . . . . . 109

4.5.3 Resultados do experimento (B) com OpenMP+MPI . . . . . 111

4.5.4 Discuss˜ao dos resultados com MPI

. . . . . . . . . . . . . . 112

5 Conclus˜oes e perspectivas

Referˆencias Bibliogr´aﬁcas

x

114

117

Lista de Figuras

Figura

1.1 Geometria realista de um reservat´orio de g´as em folhelhos.

. . . . .

1.2 Geometria idealizada de um reservat´orio de g´as em folhelhos. . . . .

1.3 Modelo Fratura/Bloco . . . . . . . . . . . . . . . . . . . . . . . . .

2.1 A arquitetura SISD . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.2 A arquitetura SIMD . . . . . . . . . . . . . . . . . . . . . . . . . .

2.3 A arquitetura MIMD . . . . . . . . . . . . . . . . . . . . . . . . . .

3.1 Vetor est´atico ´unico comportando m´ultiplos vetores menores de

diferentes tamanhos deﬁnidos em tempo de execu¸c˜ao.

. . . . . . . .

3.2 Organiza¸c˜ao em 3 n´ıveis dos m´ultiplos arquivos de fonte

. . . . . .

3.3 A organiza¸c˜ao do simulador em arquivos-fonte . . . . . . . . . . . .

3.4 A estrutura b´asica de classes: Sistemas e Estruturas de Dados . . .

5

6

7

39

40

41

50

57

59

63

3.5 A estrutura de classes de Sistemas de Equacoes com maiores detalhes. 64

3.6 A estrutura de classes de Estruturas de Dados com maiores detalhes. 65

3.7 Organiza¸c˜ao do simulador com os novos m´odulos fonte.

. . . . . . .

70

4.1 Percentual do tempo total de execu¸c˜ao por rotina com solver Gauss

80

4.2 Percentual do tempo total de execu¸c˜ao por rotina com solver intel

Pardiso . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.3 Tempo total de execu¸c˜ao do caso experimental com solver Gauss . .

80

82

4.4 Tempo total de execu¸c˜ao do caso experimental com solver intel Pardiso 82

4.5 An´alise da rotina processadorBloco com solver Gauss . . . . .

83

xi

4.6 An´alise da rotina processadorBloco com solver Pardiso . . . .

83

4.7 An´alise de condi¸c˜oes de corrida feita com a ferramente Intel Inspector. 91

4.8 A organiza¸c˜ao de objetos de sistemas e estruturas de dados na vers˜ao

paralelizada com OpenMP . . . . . . . . . . . . . . . . . . . . . . .

92

4.9 Perﬁs de press˜ao ao longo da fratura (30,5 metros) para: 1 mˆes, 6

meses, 1, 5, 10, 20 e 30 anos. . . . . . . . . . . . . . . . . . . . . . .

4.10 Produ¸c˜ao acumulada em Kg . . . . . . . . . . . . . . . . . . . . . .

4.11 Speedups da vers˜ao com OpenMP para o experimento (A).

. . . . .

4.12 Speedups da vers˜ao com OpenMP para o experimento (B).

. . . . .

4.13 Tempos de execu¸c˜ao por cada problema do bloco para o experimento

95

95

97

98

(A).

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

99

4.14 Tempos de execu¸c˜ao por cada problema do bloco para o experimento

(B).

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

99

4.15 Speedups

com a

cl´ausula schedule(dynamic) para

o

experimento (A).

. . . . . . . . . . . . . . . . . . . . . . . . . . . . 101

4.16 Speedups

com a

cl´ausula schedule(dynamic) para

o

experimento (B).

. . . . . . . . . . . . . . . . . . . . . . . . . . . . 101

4.17 Tempos de execu¸c˜ao por cada problema do bloco (Fratura com 400

elementos e malhas de 600 elementos no bloco) . . . . . . . . . . . . 102

4.18 Inclina¸c˜oes das linhas de tendˆencia das ﬁguras 4.13 e 4.17 em fun¸c˜ao

do tamanho das malhas do bloco.

. . . . . . . . . . . . . . . . . . . 102

4.19 Speedups

com a

cl´ausula schedule(dynamic) para

o

experimento (A) com malhas do bloco aumentadas para 600

elementos.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103

4.20 Speedups

com a

cl´ausula schedule(dynamic) para

o

experimento (B) com malhas do bloco aumentadas para 600

elementos.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103

4.21 A estrat´egia de paraleliza¸c˜ao em processos com MPI.

. . . . . . . . 106

xii

4.22 M´edias dos tempos de execu¸c˜ao (em minutos) da vers˜ao com

OpenMP+MPI para o experimento (A).

. . . . . . . . . . . . . . . 110

4.23 Speedups da vers˜ao com OpenMP+MPI para o experimento (A). . . 111

4.24 M´edias dos tempos de execu¸c˜ao (em minutos) da vers˜ao com

OpenMP+MPI para o experimento (B).

. . . . . . . . . . . . . . . 112

4.25 Speedups da vers˜ao com OpenMP+MPI para o experimento (B). . . 112

xiii

Lista de Tabelas

Tabela

4.1 Parˆametros f´ısicos relativos ao campo realista utilizado nos testes.

Adaptado de [Costa (2015)]

. . . . . . . . . . . . . . . . . . . . . .

93

4.2 Tempos de execu¸c˜ao (em minutos) da vers˜ao com OpenMP para o

experimento (A).

. . . . . . . . . . . . . . . . . . . . . . . . . . . .

96

4.3 Tempos de execu¸c˜ao (em minutos) da vers˜ao com OpenMP para o

experimento (B).

. . . . . . . . . . . . . . . . . . . . . . . . . . . .

97

4.4 Tempos de execu¸c˜ao (em minutos) da vers˜ao com OpenMP+MPI

para o experimento (A).

. . . . . . . . . . . . . . . . . . . . . . . . 110

4.5 Tempos de execu¸c˜ao (em minutos) da vers˜ao com OpenMP+MPI

para o experimento (B).

. . . . . . . . . . . . . . . . . . . . . . . . 111

xiv

Lista de Siglas e Abreviaturas

• TAD: Tipo Abstrato de Dados

• MEF: M´etodo dos Elementos Finitos

• UC: Unidade de Controle

• ULA: Unidade L´ogico-Atitm´etica

• CPU: Central Processing Unit

• VPU: Vector Processing Unit

• GPU: Graphics Processing Unit

xv

Cap´ıtulo 1

Introdu¸c˜ao

Reservat´orios n˜ao convencionais de g´as em folhelhos, em inglˆes, shale gas

reservoirs, representam atualmente uma fonte de energia altamente relevante para

muitos pa´ıses. Nos EUA, por exemplo, tais reservas podem representar importante

papel na autossuﬁciˆencia energ´etica do pa´ıs, dentro de alguns anos. A grande

quantidade de potenciais reservas deste tipo existentes, inclusive no Brasil, motiva e

justiﬁca os estudos na ´area, os quais se fazem ainda mais relevantes em decorrˆencia

da necessidade do emprego de t´ecnicas de explora¸c˜ao espec´ıﬁcas, adequadas `as

caracter´ısticas peculiares de tais reservat´orios, como a baixa permeabilidade de

forma¸c˜oes geol´ogicas e a existˆencia de m´ultiplos n´ıveis de porosidade.

A modelagem computacional dos reservat´orios ´e o instrumento que permite

a descri¸c˜ao precisa dos fenˆomenos f´ısicos existentes no processo de recupera¸c˜ao do

g´as, tendo interesse tanto para a ind´ustria, que busca a explora¸c˜ao eﬁciente, como

para a ciˆencia, assistindo o desenvolvimento de modelos f´ısicos e matem´aticos,

capazes de relacionar grandezas envolvidas nas equa¸c˜oes que regem os fenˆomenos

f´ısicos.

Na ind´ustria de ´oleo e g´as, ´e grande a demanda por simuladores comerciais

como por exemplo o CMG (Computer Modeling Group) ou ECLIPSE. Entretanto,

simuladores cient´ıﬁcos tˆem caracter´ısticas e importˆancia pr´oprias. Simuladores

cient´ıﬁcos oferecem a cientistas e pesquisadores do dom´ınio das aplica¸c˜oes,

ﬂexibilidade e possibilidades de ajustes que atendem de forma mais adequada suas

1

demandas espec´ıﬁcas.

Muitas vezes,

simuladores comerciais

s˜ao produtos completos e que

englobam uma grande quantidade de componentes, alguns dos quais podem

n˜ao ser de interesse dos trabalhos acadˆemicos.

Simuladores cient´ıﬁcos, em

contraste, frequentemente permitem maior liberdade na realiza¸c˜ao de experimentos

espec´ıﬁcos, permitindo maior controle e, inclusive, maior precis˜ao em determinados

casos de interesse.

Este trabalho trata do emprego de t´ecnicas de programa¸c˜ao modular e

de computa¸c˜ao de alto desempenho em um simulador cient´ıﬁco, baseado na

implementa¸c˜ao computacional do m´etodo de elementos ﬁnitos proposta em Hughes

(1987) e utilizado na simula¸c˜ao num´erica de problemas de escoamento monof´asico

de g´as em reservat´orios n˜ao convencionais em folhelhos, de acordo com os modelos

f´ısico e matem´atico propostos em Costa (2015).

Dentre as motiva¸c˜oes para o desenvolvimento deste trabalho est˜ao:

(i)

A relativa carˆencia, no ˆambito do desenvolvimento de software cient´ıﬁco,

principalmente quando comparado ao software comercial e corporativo, de certos

cuidados, conhecimentos e pr´aticas que suportem o desenvolvimento ordenado,

modular, e escal´avel de software e (ii) O fato de que o software cient´ıﬁco em

geral demanda t´ecnicas de computa¸c˜ao de alto desempenho, tanto por manipular

grandes massas de dados quanto por estar associado frequentemente a modelos

num´ericos computacionalmente intensivos.

Segundo Rouson et al. (2011), s˜ao caracter´ısticas desej´aveis em um software

cient´ıﬁco o design escal´avel, obtido com desenvolvimento ordenado de c´odigo,

programa¸c˜ao modular e conceitos de projeto software, e a execu¸c˜ao escal´avel,

obtida com uso de t´ecnicas de computa¸c˜ao paralela e de alto desempenho. A

contribui¸c˜ao que se deseja oferecer neste trabalho engloba, portanto, dois aspectos:

(i) A organiza¸c˜ao est´atica do c´odigo-fonte e (ii) A sua execu¸c˜ao.

Quanto ao primeiro aspecto, este trabalho compreende a reestrutura¸c˜ao de

um dos m´odulos do simulador, aquele referente `a constru¸c˜ao e `a solu¸c˜ao dos

2

sistemas de equa¸c˜oes lineares referentes ao m´etodo de elementos ﬁnitos. Para

isso s˜ao desenvolvidos novos m´odulos incorporando o paradigma de programa¸c˜ao

orientado a objetos para substituir o m´odulo original. Com a incorpora¸c˜ao

de conceitos de orienta¸c˜ao a objetos, pretende-se promover a facilita¸c˜ao do

entendimento humano e da usabilidade das funcionalidades providas pelos novos

m´odulos, principalmente por meio do encapsulamento de dados e opera¸c˜oes,

promovendo a abstra¸c˜ao de detalhes.

J´a em rela¸c˜ao `a contribui¸c˜ao do trabalho no que se refere `a computa¸c˜ao de

alto desempenho, s˜ao utilizados padr˜oes de programa¸c˜ao paralela para sistemas

computacionais baseados em mem´oria uniﬁcada e distribu´ıda, respectivamente

o OpenMP e o MPI, no desenvolvimento de estrat´egias de paraleliza¸c˜ao para o

modelo adotado neste simulador.

1.1

O simula¸c˜ao de reservat´orios no LNCC

Pesquisadores da ´area de m´etodos num´ericos para equa¸c˜oes diferenciais

do LNCC tˆem trabalhado desde 1988 com implementa¸c˜oes do M´etodo de

Elementos Finitos (MEF) baseadas no c´odigo apresentado em Hughes (1987).

Suas implementa¸c˜oes atendem a demandas em diferentes aplica¸c˜oes cient´ıﬁcas,

todas tendo como ponto comum modelos matem´aticos que recaem sobre a solu¸c˜ao

num´erica de equa¸c˜oes diferenciais.

Dentre as aplica¸c˜oes que utilizam o m´etodo de elementos ﬁnitos no LNCC,

est˜ao as desenvolvidas pelo grupo de simula¸c˜oes computacionais de reservat´orios

de petr´oleo e g´as. O grupo tem desenvolvido ao longo do tempo simuladores

relacionados `a ciˆencia de escoamentos e transportes em meios porosos, atendendo

a demandas internas espec´ıﬁcas, seguindo um processo continuamente evolutivo,

incorporando aos poucos fenˆomenos relacionados, por exemplo, `a hidrodinˆamica,

ao transporte de solutos e a processos geomecˆanicos.

Apesar da alta demanda por simuladores na ind´ustria, o LNCC, enquanto

institui¸c˜ao atuante na ´area da ciˆencia, desenvolve simuladores cient´ıﬁcos, que

3

tratam de fenˆomenos espec´ıﬁcos, sendo mais precisos para esses casos. Embora

sejam caracterizados pela existˆencia de limita¸c˜oes, simuladores cient´ıﬁcos s˜ao como

bancadas de um laborat´orio, que servem a um grupo e permitem experimentos com

maior liberdade.

O simulador de reservat´orios n˜ao convencionais de shale gas desenvolvido e

utilizado no LNCC n˜ao possui um projeto de longo prazo.

Isso signiﬁca que o

grupo n˜ao tem a cria¸c˜ao de um grande simulador como meta. Entretanto, novas

funcionalidades tˆem sido incorporadas ao simulador com o passar do tempo, dando

origem a diferentes vers˜oes que s˜ao adaptadas para atender a diferentes demandas.

Embora as contribui¸c˜oes deste trabalho possam ser incorporadas em

diferentes vers˜oes deste ou de outros simuladores similares, baseados na mesma

implementa¸c˜ao original do MEF e que compartilham o mesmo m´odulo para

solu¸c˜ao de sistemas de equa¸c˜oes, a vers˜ao do simulador utilizada neste trabalho

compreende a simula¸c˜ao num´erica de problemas de escoamento monof´asico do g´as

nos reservat´orios n˜ao convencionais em folhelhos, nos blocos de rocha matriz e nas

fraturas hidr´aulicas induzidas, de acordo com modelo proposto em Costa (2015),

cujas linhas gerais s˜ao abordadas na se¸c˜ao 1.2.

1.2

O modelo 2 escalas: Fratura e Bloco

A baixa permeabilidade das forma¸c˜oes geol´ogicas onde ocorre o shale gas

implica na necessidade da indu¸c˜ao de fraturas pelo processo de faturamento

hidr´aulico para que seja constitu´ıdo um meio suﬁcientemente perme´avel por onde

o g´as possa ﬂuir mais facilmente. Modelos hidrodinˆamicos considerando dois n´ıveis

de porosidade (microblocos e ﬁssuras induzidas) constituem a base do estado da

arte da modelagem de reservat´orios de shale gas.

No interior das ﬁssuras provocadas pelo faturamento (ﬁssuras induzidas),

pode-se modelar o escoamento do g´as por meio das equa¸c˜oes diferenciais que

regem o escoamento monof´asico de ﬂuido compress´ıvel (g´as) em um meio s´olido

r´ıgido, conforme modelo em Costa (2015). Os microblocos da rocha matriz atuam

4

armazenando g´as e funcionam, em tal modelo, como fontes de massa para o

problema de escoamento nas fraturas induzidas.

O simulador tomado como ponto de partida neste trabalho obedece `a

modelagem f´ısica e matem´atica do dom´ınio desenvolvida em Costa (2015).

Em linhas gerais, os reservat´orios de g´as em folhelhos, caracterizados pela

perfura¸c˜ao horizontal, s˜ao tratados nesse modelo como uma sequˆencia de blocos

de rocha matriz com fraturas induzidas peri´odicas,

igualmente espa¸cadas, ao

longo de toda a extens˜ao do po¸co horizontal. Idealmente, as fraturas devem ser

consideradas entidades bidimensionais, enquanto os blocos devem ser considerados

tridimensionais, conforme ilustrado na ﬁgura 1.1, na qual pode-se observar o

escoamento radial do g´as da fratura em dire¸c˜ao ao po¸co.

Figura 1.1: Geometria realista de um reservat´orio de g´as em folhelhos.

Entretanto, o modelo de Costa (2015) possui simpliﬁca¸c˜oes, considerando

um alongamento da interface entre fratura e po¸co por toda a largura da fratura,

eliminando a hip´otese de escoamento radial do g´as. Considera-se que o g´as ﬂui da

fratura para o po¸co em apenas uma dire¸c˜ao. A nova conﬁgura¸c˜ao ´e mostrada na

ﬁgura 1.2.

5

Figura 1.2: Geometria idealizada de um reservat´orio de g´as em folhelhos.

Desconsiderando o escoamento radial nas fraturas, o modelo utiliza, ainda,

uma redu¸c˜ao de dimens˜ao que passa tratar fraturas como linhas e blocos como

planos. Para a simula¸c˜ao num´erica do escoamento na fratura, o modelo adota

uma malha unidimensional de elementos ﬁnitos, ao passo que, para o bloco,

adotam-se m´ultiplas malhas unidimensionais, como esquematizado na ﬁgura 1.3.

Blocos atuam como fontes de massa para o problema das fraturas. Em um

dado bloco, cada um dos v´arios problemas unidimensionais est´a relacionado a

um ponto da malha unidimensional do problema da fratura. O modelo, portanto,

caracteriza-se pela resolu¸c˜ao de um n´umero elevado de problemas de elementos

ﬁnitos independentes nos blocos, o que resulta em m´ultiplos sistemas lineares

independentes. Esta caracter´ıstica, como ser´a visto no cap´ıtulo 4, ´e diretamente

relacionada `as estrat´egias de paraleliza¸c˜ao desenvolvidas.

6

Figura 1.3: Modelo Fratura/Bloco

1.3

Programa¸c˜ao Modular com Orienta¸c˜ao a Objetos

A vers˜ao do simulador tomada como ponto de partida neste trabalho, ap´os

evolu¸c˜oes incrementais nos ´ultimos anos, possui algum n´ıvel de modularidade.

O c´odigo-fonte est´a subdividido em m´odulos que agrupam funcionalidades

suﬁcientemente relacionadas. Apesar disso, neste trabalho desejamos promover

mais um passo evolutivo quanto `a modularidade deste simulador cient´ıﬁco e

este avan¸co est´a relacionado com a reformula¸c˜ao que incorpora o paradigma de

orienta¸c˜ao a objetos em um de seus m´odulos.

De acordo com Rouson et al. (2011), grande parte dos conceitos de projeto

de software que norteiam o desenvolvimento ordenado e eﬁciente de programas

est´a relacionada ao paradigma de programa¸c˜ao orientado a objetos. A refatora¸c˜ao

do m´odulo em quest˜ao envolve encapsulamento de sistemas de equa¸c˜oes, solvers

e estruturas de dados relacionadas aos sistemas com um arcabou¸co orientado a

objetos, oferecendo facilidades para a utiliza¸c˜ao de diferentes op¸c˜oes de solvers,

os quais podem ser internos, completamente implementados dentro da aplica¸c˜ao

7

L x x = 0 Poço Γ1x = L Γ2Fratura Bloco cient´ıﬁca cliente, ou externos, com uso de bibliotecas matem´aticas desenvolvidas

por terceiros e que incluem solvers. Est´e ´e o caso do Intel Pardiso, parte da

biblioteca Intel MKL.

A maior modularidade, especialmente por manifestar-se sob a forma do

paradigma de orienta¸c˜ao a objetos, contribuir´a para melhor usabilidade e para

a capacidade de evolu¸c˜ao deste simulador. Conceitos centrais de orienta¸c˜ao a

objetos como encapsulamento de dados e opera¸c˜oes, heran¸ca e polimorﬁsmo ser˜ao

explorados para que o m´odulo original dˆe origem a outros menores, sob forma de

classes, as quais se relacionam de forma tal que se possa promover a facilita¸c˜ao

do entendimento das funcionalidades e do comportamento de cada unidade de

software.

Classes funcionar˜ao como entidades encapsuladoras, contribuindo para o

agrupamento de dados e opera¸c˜oes relacionados `as entidades de interesse por

elas representadas: os sistemas de equa¸c˜oes e as estruturas de dados para

armazenamento de matrizes esparsas realtivas aos sistemas, cujos dados poder˜ao

ser manipulados por mais de um tipo de solver.

O novo conjunto de m´odulos com orienta¸c˜ao a objetos permitir´a a este e a

outros outros simuladores similares baseados na mesma implementa¸c˜ao original

do MEF, maior facilidade para a substitui¸c˜ao dos solvers em uso por outras

op¸c˜oes, facilitando e promovendo maior isolamento nos trabalhos realizados para

a incorpora¸c˜ao de solvers externos ou mesmo para a implementa¸c˜ao de novas

alternativas internas neste quesito.

1.4

Computa¸c˜ao paralela para desempenho do simulador

Do entendimento do modelo introduzido na se¸c˜ao 1.2, percebe-se que neste

simulador existe uma situa¸c˜ao convidativa ao desenvolvimento de estrat´egias de

paraleliza¸c˜ao que se aproveitem da total independˆencia entre os m´ultiplos sistemas

de equa¸c˜oes que precisam ser resolvidos para a simula¸c˜ao do escoamento nos blocos.

Assim sendo, neste trabalho s˜ao propostas duas solu¸c˜oes que se valem desse fato,

8

a serem discutidas com detalhes no cap´ıtulo 4.

A implementa¸c˜ao do paradigma de orienta¸c˜ao a objetos funcionar´a como

facilitadora para o desenvolvimento de tais solu¸c˜oes, as quais ir˜ao envolver a

coexistˆencia de m´ultiplos sistemas de equa¸c˜oes independentes em mem´oria. As

classes tornar˜ao essa tarefa natural e intuitiva, dada a possibilidade de que

tenhamos m´ultiplas instˆancias, ou objetos, da classe que representa os sistemas

de equa¸c˜oes.

A ideia b´asica por tr´as da solu¸c˜oes paralelas desenvolvidas neste trabalho

poder´a ser enxergada como uma abordagem de granularidade grossa para a

execu¸c˜ao paralela de um modelo, uma vez que n˜ao tratar´a da paraleliza¸c˜ao

de solvers, mas sim da execu¸c˜ao simultˆanea de v´arias instˆancias de problemas

completos, cada qual demandando a solu¸c˜ao de um sistema independente.

O paralelismo estar´a, portanto, em um n´ıvel de abstra¸c˜ao acima das

implementa¸c˜oes internas de solvers.

Isto signiﬁca que qualquer solver utilizado

poder´a beneﬁciar-se de uma estrat´egia de paraleliza¸c˜ao ´unica e previamente

desenvolvida.

As solu¸c˜oes paralelas desenvolvidas utilizam os padr˜oes de programa¸c˜ao

paralela OpenMP e MPI, voltados a sistemas computacionais de mem´oria uniﬁcada

e distribu´ıda, respectivamente. Ambas ter˜ao a mesma proposta b´asica: permitir

que v´arias unidades de execu¸c˜ao (threads ou processos) resolvam, em paralelo,

v´arios problemas referentes aos blocos, cada um dos quais compreende duas etapas:

a constru¸c˜ao e a solu¸c˜ao dos sistemas de equa¸c˜oes referentes `a implementa¸c˜ao do

m´etodo dos elementos ﬁnitos.

1.5

Organiza¸c˜ao da disserta¸c˜ao

Este trabalho est´a organizado da seguinte forma:

No cap´ıtulo 2

apresentamos, de maneira conceitual, duas grandes demandas do software

cient´ıﬁco em geral, a programa¸c˜ao modular e a computa¸c˜ao paralela, fornecendo

justiﬁcativas para sua relevˆancia e explorando conceitualmente importantes

9

t´ecnicas de desenvolvimento de software que visam atendˆe-las.

No cap´ıtulo 3 descrevemos o processo de refatora¸c˜ao do m´odulo referente aos

sistemas de equa¸c˜oes lineares. S˜ao apresentadas evolu¸c˜oes da organiza¸c˜ao est´atica

do c´odigo-fonte pelas quais o c´odigo do simulador de escoamentos passou desde sua

vers˜ao original e, em seguida, com maior detalhamento, as evolu¸c˜oes feitas neste

trabalho com a incorpora¸c˜ao do paradigma de orienta¸c˜ao a objetos.

No cap´ıtulo 4 ´e descrito o processo de desenvolvimento das solu¸c˜oes paralelas

utilizando OpenMP e MPI para o simulador. S˜ao realizados, ainda, testes de

desempenho, interpreta¸c˜ao e cr´ıtica dos os resultados, seguidas de um ciclo de

otimiza¸c˜ao.

Finalmente, no cap´ıtulo 5, s˜ao apresentadas as conclus˜oes do trabalho.

10

Cap´ıtulo 2

Demandas da evolu¸c˜ao do software

cient´ıﬁco

Neste cap´ıtulo s˜ao tratados de forma geral duas grandes demandas e frentes

de evolu¸c˜ao do software cient´ıﬁco: a modulariza¸c˜ao e o desempenho computacional.

Tais caracter´ısticas est˜ao relacionadas ao tempo de vida dos produtos de software.

Naturalmente, produtos efˆemeros n˜ao necessitam, em geral, apresentar tais

caracter´ısticas, mas, produtos de software da ciˆencia moderna cada vez menos

frequentemente se encaixam nessa categoria.

Rouson et al. (2011) deixa claro que os grandes problemas da ciˆencia moderna

recaem no desenvolvimento de modelos f´ısicos e matem´aticos que combinam

dinˆamicas de diversas esferas do conhecimento para resolver problemas que muitas

vezes englobam fenˆomenos cujas escalas de tempo e espa¸co s˜ao separadas por v´arias

ordens de magnitude.

Isso constitui desaﬁos do ponto de vista da estabilidade,

consistˆencia e acur´acia das simula¸c˜oes computacionais cient´ıﬁcas. Sendo assim,

nota-se que as caracter´ısticas, o tamanho e a complexidade dos problemas da

ciˆencia moderna fazem com que seus produtos de software tendam a possuir longa

vida e longos ciclos evolutivos.

A primeira classe de demandas do software cient´ıﬁco aparece ao notarmos que

a ciˆencia se baseia em modelos que evoluem no tempo, incorporam mais detalhes,

tornam-se mais completos e tamb´em mais complexos. Assim, o software cient´ıﬁco

deve ser um produto com certo n´ıvel de maleabilidade, adaptabilidade, facilidade

11

de incorpora¸c˜ao de funcionalidades e capacidade de evolu¸c˜ao. Tal tipo de demandas

traz consigo desaﬁos relacionados diretamente `a organiza¸c˜ao est´atica e estrutural

do c´odigo-fonte.

Em adi¸c˜ao `as

referidas demandas estruturais,

existem as demandas

relacionadas ao desempenho de execu¸c˜ao.

O software cient´ıﬁco est´a, em

geral, embasado em modelos num´ericos computacionalmente intensivos, os

quais frequentemente possuem alto custo computacional. Entretanto, todas as

simula¸c˜oes cient´ıﬁcas precisam ser realizadas dentro de um limite de tempo

aceit´avel, que satisfa¸ca as expectativas de seus usu´arios. As demandas relacionadas

ao desempenho trazem desaﬁos relacionados `a utiliza¸c˜ao eﬁciente dos recursos de

hardware dispon´ıveis.

Quer seja no projeto, no desenvolvimento ou na execu¸c˜ao dos programas, o

termo “escalabilidade” tem ganhado importˆancia nas discuss˜oes acerca do software

cient´ıﬁco. Dizemos que um software ´e escal´avel quando est´a preparado para evoluir

e crescer, seja pela incorpora¸c˜ao de novas funcionalidades ou pelo aumento do

seu desempenho, o que lhe torna capaz de manipular com eﬁciˆencia uma carga

crescente de trabalho por meio da utiliza¸c˜ao do m´aximo poss´ıvel dos recursos

computacionais dispon´ıveis. Nesse contexto Rouson et al. (2011) anuncia dois tipos

de escalabilidade: a escalabilidade do design e a escalabilidade no desempenho ou

execu¸c˜ao.

A escalabilidade do design reﬂete-se em um projeto de software voltado para

o crescimento e a evolu¸c˜ao. Projetar de maneira escal´avel resulta em um processo

de desenvolvimento escal´avel que, por sua vez, se faz notar no c´odigo-fonte. ´E

frequentemente desej´avel que o software cient´ıﬁco tenha c´odigo compreens´ıvel, com

boa usabilidade, adaptabilidade e facilidade de expans˜ao. O c´odigo que apresenta

esse tipo de escalabilidade ´e desenvolvido de forma ordenada e modular, oferecendo

facilidades ao entendimento humano e aos trabalhos para sua evolu¸c˜ao e expans˜ao

futuras.

Quanto `a execu¸c˜ao, desenvolver c´odigos com desempenho escal´avel signiﬁca

12

desenvolver c´odigos que s˜ao capazes de n˜ao apenas de beneﬁciar-se do m´aximo

poss´ıvel dos recursos computacionais dispon´ıveis, mas tamb´em de ter ganhos de

desempenho, caso mais recursos de hardware sejam disponibilizados. Dadas as

caracter´ısticas do hardware atual, veremos que c´odigos com desempenho escal´avel

exploram o paralelismo de forma eﬁciente.

Nas pr´oximas se¸c˜oes deste cap´ıtulo exploraremos conceitos de dois t´opicos

diretamente relacionados com as escalabilidades de design e de execu¸c˜ao: A

Programa¸c˜ao Modular e A Computa¸c˜ao Paralela, respectivamente.

2.1

Programa¸c˜ao Modular

O desenvolvimento de software na ´epoca dos primeiros computadores digitais,

muito em fun¸c˜ao das enormes limita¸c˜oes da m´aquinas daquele tempo, era

marcado por t´ecnicas que visavam extrair o m´aximo dos recursos computacionais

dispon´ıveis. Todo o paradigma do desenvolvimento n˜ao privilegiava, e nem poderia,

a boa ordena¸c˜ao e organiza¸c˜ao do ﬂuxo de execu¸c˜ao dos programas em detrimento

de sua performance. Era pr´atica comum negligenciar as caracter´ısticas relacionadas

`a ordena¸c˜ao e `a clareza do c´odigo escrito em fun¸c˜ao da necessidade de construir-se

um ﬂuxo de execu¸c˜ao o mais eﬁciente poss´ıvel.

O n´umero de vari´aveis de um programa, por exemplo, precisava ser

controlado com muito mais rigor do que fazemos hoje em um programa moderno.

Precisava-se limitar o n´umero de vari´aveis por economia no uso de mem´oria.

Sabemos que, do ponto de vista puramente pr´atico, n˜ao h´a problema algum que

uma mesma vari´avel assuma diferentes funcionalidades ao longo de um programa.

Entretanto, n˜ao ´e dif´ıcil perceber que o fato de uma vari´avel possuir v´arias

fun¸c˜oes ao longo do c´odigo pode representar um preju´ızo ao entendimento, uma

vez que impossibilita a utiliza¸c˜ao de um nome de vari´avel relacionado a uma

funcionalidade espec´ıﬁca. Os programas da ´epoca dos primeiros computadores

eram, portanto, desenvolvidos quase inteiramente com foco na sua viabilidade em

termos de desempenho e na sua funcionalidade, o que para ´epoca j´a constitu´ıa um

13

grande desaﬁo. Um bloco monol´ıtico de c´odigo era a apresenta¸c˜ao mais comum

de um programa de computador dessa ´epoca. Dividir um programa em v´arios

blocos ou por¸c˜oes de c´odigo j´a demandava um gerenciamento de mem´oria custoso `a

´epoca. Essas caracter´ısticas faziam com que programas fossem escritos geralmente

por apenas um programador, uma vez que n˜ao se podia ter por¸c˜oes minimamente

independentes.

`A medida que os computadores, ambientes de programa¸c˜ao e linguagens

evolu´ıram, os programas tamb´em foram crescendo em tamanho e complexidade.

Cada vez problemas maiores e mais complexos podiam ser resolvidos por meios

computacionais com menor esfor¸co. Ind´ustria, servi¸cos e ciˆencia passaram a n˜ao

somente explorar, mas, inclusive, depender fortemente de sistemas de computa¸c˜ao

para realiza¸c˜ao de muitas de suas atividades. Dessa forma, ﬁca claro que o software

passava a demandar n´ıveis mais altos de qualidade e conﬁabilidade para que os

riscos relacionados `a sua utiliza¸c˜ao sejam controlados ou minimamente toler´aveis.

Acompanhando a evolu¸c˜ao dos recursos computacionais, veio uma mudan¸ca

no fator limitante ou fator inibidor do desenvolvimento de software (Varej˜ao, 2004).

Os recursos computacionais avan¸caram de forma a permitir certo conforto aos

desenvolvedores. N˜ao era mais necess´ario guiar o desenvolvimento de software

majoritariamente por quest˜oes relacionadas `a viabilidade t´ecnica. Recursos

computacionais passaram a n˜ao ser mais t˜ao limitadores quanto a otimiza¸c˜ao do

trabalho dos pr´oprios programadores. A produtividade se tornava um fator chave.

Precisava-se buscar aperfei¸coamento no processo de desenvolvimento, o que traria

n˜ao apenas maior produtividade, mas tamb´em maior conﬁabilidade ao produto

desenvolvido.

O conceito de um programa monol´ıtico e de um ´unico desenvolvedor passou

a n˜ao ser mais adequado `a nova realidade. Os programas passavam a ser cada

vez maiores, mais complexos e mais cr´ıticos. Grandes equipes de desenvolvimento

eram necess´arias e estas precisavam ser cada vez mais produtivas. Deviam buscar

t´ecnicas que permitissem a cria¸c˜ao de novos programas de forma eﬁciente, com

14

divis˜ao de trabalho e facilita¸c˜ao da reutiliza¸c˜ao do trabalho anterior.

Problemas complexos podem, em geral, ser reduzidos a um conjunto de

subproblemas menores com solu¸c˜oes independentes. A essa estrat´egia ´e dado o

nome de “dividir para conquistar”. Sua ado¸c˜ao permite n˜ao apenas a divis˜ao do

trabalho entre os integrantes de uma equipe de desenvolvimento, mas tamb´em

favorece a reutiliza¸c˜ao de solu¸c˜oes j´a implementadas (Varej˜ao, 2004). Uma solu¸c˜ao

´unica para um problema grande e complexo diﬁcilmente ser´a reutilizada, por´em as

solu¸c˜oes de subproblemas associados podem ser reutilizadas com maior facilidade,

dado que v´arios problemas complexos podem possuir passos intermedi´arios ou

subproblemas em comum, cuja solu¸c˜ao j´a tenha sido implementada em um

momento anterior.

2.1.1

Princ´ıpios da Programa¸c˜ao Modular

Embora programas de computador possam ser simplesmente entendidos

como sequˆencias de instru¸c˜oes, uma realidade bastante diferente se faz ver sob

a ´otica das linguagens de programa¸c˜ao de alto n´ıvel como C, Fortran ou Java.

Mais do que conjuntos instru¸c˜oes indivis´ıveis, programas s˜ao compostos por

elementos com certo grau de complexidade e que possuem algum signiﬁcado para

o entendimento humano. Tais elementos, por sua vez, podem ser compostos

por outros menores. Todos combinados constituem um todo que chamamos de

programa ou sistema de software.

Considerando-se a grande variedade de linguagens e paradigmas de

programa¸c˜ao al´em de diferentes granularidades, os elementos componentes podem

se apresentar sob diferentes formas como fun¸c˜oes, procedimentos, tipos de dados,

classes ou arquivos de fonte, para citar alguns exemplos. Ao longo deste texto,

utilizamos os termos “elementos de programa¸c˜ao (ou de software)” e

“unidades de programa¸c˜ao (ou de software)” de maneira indistinta e num

sentido muito generalizado, sem qualquer deﬁni¸c˜ao restritiva, de forma que possam

se referir a quaisquer componentes de um sistema de software.

15

Segundo von Staa (2000), programas de porte m´edio em diante necessitam

ser particionados em segmentos de c´odigo minimamente independentes. No

desenvolvimento de programas com porte suﬁcientemente grande, ´e essencial que

se promova a organiza¸c˜ao do ambiente de trabalho, minimizando o n´umero de

falhas e as consequentes perdas geradas com o retrabalho para corrigi-las. A

programa¸c˜ao modular ´e o instrumento capaz de oferecer maiores garantias de

que o desenvolvimento se dar´a de maneira organizada (von Staa, 2000).

A ideia que deﬁne o princ´ıpio fundamental da programa¸c˜ao modular ´e

o particionar o c´odigo-fonte e esta se aproxima da estrat´egia de “dividir para

conquistar”. Desenvolver software modular ´e basicamente desenvolver software

promovendo o particionamento de c´odigo e o encapsulamento1 de dados e

processos em m´ultiplas unidades de software, cada uma correspondendo a uma

parcela de c´odigo com certo grau de independˆencia em sua gera¸c˜ao, manipula¸c˜ao

e edi¸c˜ao.

As unidades de software respons´aveis pela segmenta¸c˜ao do c´odigo-fonte

de um programa modular recebem o nome de m´odulos fonte. Mais adiante,

exploraremos o conceito de m´odulo mais adotado na literatura e ser˜ao discutidas

com mais detalhes as t´ecnicas de implementa¸c˜ao da programa¸c˜ao modular. Nesse

momento estamos interessados apenas em promover o entendimento de m´odulos

enquanto unidades que rompem com conceito de um programa monol´ıtico, ou

indivis´ıvel, confrontando a programa¸c˜ao monol´ıtica com a programa¸c˜ao modular.

Por hora, ´e suﬁciente que pensemos conceitualmente em m´odulos como por¸c˜oes de

c´odigo com certo grau de independˆencia que:

(i) Encapsulam processos para a solu¸c˜ao de subproblemas

(ii) Encapsulam dados que representam certos aspectos de um dom´ınio modelado

A forma de apresenta¸c˜ao dos m´odulos

tem car´ater

secund´ario no

entendimento e na conceitua¸c˜ao essencial da programa¸c˜ao modular. O fator mais

1 Encapsulamento ´e a propriedade de certos elementos de programa¸c˜ao agruparem, em seu
interior, dados que sejam, em algum grau, logicamente relacionados e processos que operam sobre
esses dados.

16

relevante aqui n˜ao ´e a forma que cada m´odulo ter´a, uma vez que isso pode

estar fortemente conectado a caracter´ısticas espec´ıﬁcas de certas tecnologias ou

linguagens de programa¸c˜ao. O que deve, sim, ser observado com maior destaque ´e

forma como o particionamento de c´odigo, associado ao encapsulamento promovido

pelos m´odulos pode contribuir para a solu¸c˜ao eﬁciente de problemas grandes e

complexos.

O particionamento de c´odigo faz com que a solu¸c˜ao de um problema complexo

possa ser decomposto na solu¸c˜ao de muitos problemas menores e mais simples de

forma coordenada, promovendo divis˜ao de trabalho, organiza¸c˜ao e facilita¸c˜ao do

entendimento humano. O encapsulamento, por sua vez, ajuda a tornar invis´ıvel

aos usu´arios detalhes desnecess´arios de implementa¸c˜ao, levando ao rompimento

de barreiras de complexidade.

No estudo das linguagens de programa¸c˜ao, um m´odulo ´e frequentemente

deﬁnido como um elemento de software que corresponde a uma unidade

suﬁcientemente individualizada, ao ponto de que possa ser desenvolvida e

compilada separadamente das demais partes que comp˜oem o sistema de software

como um todo. Em (von Staa, 2000) e (Varej˜ao, 2004) as deﬁni¸c˜oes adotadas

seguem o mesmo caminho, conceituando m´odulos como unidades compil´aveis de

forma independente. Um m´odulo pode conter diversos elementos como classes,

procedimentos, fun¸c˜oes, declara¸c˜oes de tipos, constituindo, todos juntos, uma

´unica unidade de compila¸c˜ao. Embora ambos os autores tenham dado destaque

semelhante ao quesito compila¸c˜ao individual, von Staa (2000) deixa claro que n˜ao

existe consenso absoluto na literatura acerca do conceito de m´odulo.

2.1.2

Abstra¸c˜oes

Segundo Varej˜ao (2004), a abstra¸c˜ao ´e um conceito muito importante e

vastamente utilizado em todas ´areas da ciˆencia, sendo de suma importˆancia para

a resolu¸c˜ao de problemas complexos. A modelagem e a representa¸c˜ao da realidade

para a posterior resolu¸c˜ao de um problema de interesse ´e tarefa comum na ciˆencia.

17

Devemos perceber que a representa¸c˜ao da realidade jamais poder´a ser feita em toda

sua riqueza de detalhes. A abstra¸c˜ao nos permite representar de forma seletiva

apenas os aspectos da realidade que s˜ao fundamentais na resolu¸c˜ao do problema

de interesse, escondendo os detalhes irrelevantes em um determinado contexto.

Na ´area das linguagens de programa¸c˜ao, Varej˜ao (2004) estabelece duas

perspectivas b´asicas nas quais a abstra¸c˜ao se insere. Primeiramente, a simples

utiliza¸c˜ao de uma linguagem de programa¸c˜ao de alto n´ıvel j´a representa um n´ıvel

de abstra¸c˜ao. Para o trabalho de um programador, ´e suﬁciente que ele enxergue o

computador como uma m´aquina capaz de entender comandos de uma linguagem de

programa¸c˜ao de alto n´ıvel, ainda que isso n˜ao seja verdade. O programador est´a

nesse momento abstraindo o hardware e as linguagens de baixo n´ıvel, dado que

esses s˜ao detalhes irrelevantes em seu contexto de trabalho. A segunda perspectiva

onde utiliza-se o conceito de abstra¸c˜ao ´e no entendimento de que as linguagens de

programa¸c˜ao oferecem ao programador os meios necess´arios para que ele tamb´em

crie as suas pr´oprias abstra¸c˜oes. ´E nessa perspectiva que se faz clara a forte

rela¸c˜ao entre abstra¸c˜oes e a programa¸c˜ao modular. Os m´odulos e seus elementos

componentes s˜ao as entidades capazes de promover as abstra¸c˜oes.

As abstra¸c˜oes criadas pelo programador podem ser usadas por ele ou

por outros programadores em diferentes momentos e na resolu¸c˜ao de diferentes

problemas. Deve-se perceber que o conhecimento da implementa¸c˜ao interna de

todos m´odulos ou outros elementos de programa¸c˜ao que modelam a solu¸c˜ao de

subproblemas muitas vezes n˜ao ´e fundamental para a resolu¸c˜ao do problema maior

de interesse. Assim, podemos dizer que m´odulos podem ter sua implementa¸c˜ao

interna abstra´ıda pelos programadores. Um programador interessado em resolver

dado problema computacional pode fazer uso de v´arios m´odulos, mesmo sem

conhecer coisa alguma sobre sua implementa¸c˜ao interna, desde que conhe¸ca bem

a funcionalidade oferecida por eles e a forma como eles devem se comunicar com o

18

mundo exterior, ou seja, sua interface2 .

Uma interface constitui um conjunto de regras ou padr˜oes para que se possa

estabelecer a comunica¸c˜ao entre os elementos. A assinatura de uma fun¸c˜ao, por

exemplo, estabelece um padr˜ao, por meio dos argumentos da fun¸c˜ao, de como ela

poder´a trocar dados com meio exterior, representando, portanto, a sua interface.

De acordo com Varej˜ao (2004), programas de computador podem ser

deﬁnidos como “conjuntos de instru¸c˜oes descrevendo como realizar processos para

manipular, alterar e produzir dados”. Considerando a deﬁni¸c˜ao anterior e sabendo

do papel dos m´odulos enquanto elementos capazes de promover abstra¸c˜oes por

meio do encapsulamento de processos e de dados, podemos classiﬁcar as abstra¸c˜oes

criadas pelo programador em dois tipos: Abstra¸c˜oes de processos e Abstra¸c˜oes

de dados.

2.1.2.1

Abstra¸c˜oes de processos

As abstra¸c˜oes de processos se d˜ao sobre o ﬂuxo de controle dos programas e

tratam de encapsular as seq¨uˆencias de instru¸c˜oes e comandos que descrevem um

processo. Os elementos de programa¸c˜ao utilizados para que seja feita a abstra¸c˜ao

de processos s˜ao os subprogramas. Al´em de encapsular c´odigo, eles deﬁnem

regi˜oes limitados dentro de um programa onde podemos criar vari´aveis, constantes

e tipos para utiliza¸c˜ao em escopo local. Subprogramas podem ser do tipo fun¸c˜ao

ou procedimento. Fun¸c˜oes representam abstra¸c˜oes de express˜oes e possuem um

valor de retorno. J´a os procedimentos representam abstra¸c˜oes de comandos, uma

vez que apenas encapsulam um ﬂuxo de controle deﬁnindo um novo comando que

passa a ser dispon´ıvel aos usu´arios. Em contraste com as fun¸c˜oes, os procedimentos

n˜ao retornam valores. O ponto comum ´e que subprogramas em geral atuam no

sentido de aumentar o conjunto de instru¸c˜oes oferecidos por uma linguagem de

programa¸c˜ao com o uso do conceito de abstra¸c˜oes.

2 Interface ´e o meio ou o conjunto de regras e deﬁni¸c˜oes pelo qual os m´odulos ou seus
elementos constituintes, como classes, procedimentos e fun¸c˜oes, se comunicam com o usu´ario ou
outros elementos.

19

Quando um programador escreve subprogramas, fun¸c˜oes ou procedimentos,

ele est´a criando abstra¸c˜oes no sentido de que esconde ou encapsula os detalhes

de implementa¸c˜ao de determinado ﬂuxo de execu¸c˜ao. Com isso est˜ao sendo

providas novas funcionalidades com reutiliza¸c˜ao facilitada para o futuro. As

abstra¸c˜oes criadas pelo programador na forma de subprogramas acabam por

estender as funcionalidades ou o conjunto de instru¸c˜oes oferecidos pela linguagem

de programa¸c˜ao. Combinam comandos e express˜oes de forma a oferecer outros

novos. Em qualquer momento futuro, o programador pode fazer uso das abstra¸c˜oes

para pensar em resolver problemas maiores. Nesse momento, ele poder´a ter

suas aten¸c˜oes focadas em um n´ıvel mais alto, na resolu¸c˜ao de seu problema de

interesse, cujas etapas de resolu¸c˜ao podem ser facilmente resolvidas com o uso de

subprogramas.

2.1.2.2

Abstra¸c˜oes de dados

Abstra¸c˜oes de dados, de forma similar `as abstra¸c˜oes de processos, combinam

v´arios dados de forma a oferecer um novo dado ou tipo de dado. Embora ainda

em um n´ıvel bastante baixo, ao enxergarmos a mem´oria de um computador como

um conjunto de c´elulas de mem´oria, e as c´elulas de mem´oria, por sua vez, como

cole¸c˜oes de bits, estamos fazendo abstra¸c˜ao de dados. Todas as linguagens de

programa¸c˜ao de alto n´ıvel oferecem abstra¸c˜oes de dados em um n´ıvel mais alto ao

fornecerem tipos de dados como inteiros, ponto ﬂutuante e vetores. Programadores

de linguagens de alto n´ıvel utilizam esses tipos de dados sem pensar na forma como

de fato s˜ao armazenados em mem´oria.

O que podemos observar ´e que apenas as abstra¸c˜oes de dados oferecidas pelas

linguagens de programa¸c˜ao n˜ao se demonstraram suﬁcientes para a programa¸c˜ao

com qualidade. ´E preciso permitir aos programadores que criem suas pr´oprias

abstra¸c˜oes, combinando dados dispon´ıveis de forma a dar origem a seus pr´oprios

tipos de dados. Tipos de dados como listas, pilhas e ﬁlas s˜ao exemplos de abstra¸c˜oes

de dados criadas pelo programador.

20

2.1.3

Justiﬁcativas, vantagens e desvantagens da programa¸c˜ao

modular

A programa¸c˜ao modular traz consigo uma s´erie de vantagens para o processo

de desenvolvimento de software. Nesta se¸c˜ao ser˜ao apresentadas justiﬁcativas que

d˜ao suporte a seu emprego.

• Facilita o rompimento e barreiras de complexidade al´em de permitir

a distribui¸c˜ao de trabalho, por adotar a estrat´egia de “dividir para

conquistar”. Sabemos que problemas grandes e complexos tˆem muitas

vezes sua solu¸c˜ao facilitada se forem pensados como uma s´erie de problemas

menores a serem solucionados por m´odulos independentes. M´odulos

independentes podem ser desenvolvidos por diversos programadores

trabalhando em paralelo.

Identiﬁca-se, nesse sentido, uma potencial

economia de tempo de desenvolvimento, uma vez que v´arias pessoas

poder˜ao trabalhar paralelamente no desenvolvimento do programa.

• Permite o re´uso de c´odigo. M´odulos podem e idealmente devem ser

constru´ıdos de forma que possam ser usados em outros programas. Se

os programadores desenvolvem m´odulos de forma bem documentada,

deixando sua funcionalidade clara para potenciais clientes, pode-se

obter consider´avel economia de esfor¸co de trabalho.

´E comum que

programas e projetos diferentes podem se valer de m´odulos provedores

de funcionalidades demandadas recorrentemente no desenvolvimento de

programas.

Se considerarmos programas pertencentes a um mesmo

dom´ınio de aplica¸c˜ao, a tendˆencia ´e de que o re´uso seja ainda mais

vantajoso.

´E comum que empresas ou institui¸c˜oes desenvolvam v´arios

produtos de software dentro de um mesmo dom´ınio de aplica¸c˜ao.

Desenvolver software modular pode representar a constru¸c˜ao de um ativo

de software que permite `as corpora¸c˜oes serem mais velozes e competitivas

no desenvolvimento de seus sistemas.

21

• Permite o desenvolvimento incremental e o gerencimento do processo

de desenvolvimento de software. A programa¸c˜ao modular facilita o

estabelecimento de muitas baselines de m´odulos fonte.

Isso signiﬁca

ter um maior grau de controle nas altera¸c˜oes realizadas. M´odulos

entregues podem ter sua altera¸c˜ao suﬁcientemente controlada, evitando

contratempos advindos de altera¸c˜oes indisciplinadas no c´odigo. `A medida

que novos m´odulos ou m´odulos j´a existentes s˜ao alterados, o sistema

de software como um todo evolui de maneira incremental. Cada nova

itera¸c˜ao representa uma nova vers˜ao denominada construto. Dessa forma,

a programa¸c˜ao modular torna mais natural a evolu¸c˜ao incremental dos

programas.

• Reduz o custo de compila¸c˜ao. Se considerarmos que m´odulos s˜ao unidades

independentemente compil´aveis, ﬁca claro que altera¸c˜oes pontuais em

certos m´odulos n˜ao ir˜ao requerer que todo o programa seja recompilado.

Em projetos de pequeno porte, a compila¸c˜ao pode n˜ao representar uma

tarefa custosa, mas em grandes projetos pode requerer grandes esfor¸cos e

consumir tempo dos programadores.

• Permite que otimiza¸c˜oes de desempenho sejam feitas de maneira gradual.

Os m´odulos de um programa podem ser perfeitamente funcionais ainda

que n˜ao sejam idealmente otimizados. Num primeiro momento, os

desenvolvedores podem estar interessados em desenvolver unicamente uma

vers˜ao funcional do programa. Cada um dos m´odulos pode ainda passar,

individualmente, por um processo evolutivo no que diz respeito ao seu

desempenho. A modulariza¸c˜ao oferece maior ﬂexibilidade na decis˜ao de

quais m´odulos devem ser otimizados e em quais momentos ao longo do

processo de desenvolvimento. Mais do que isso, oferece a possibilidade de

que se decida se todos os m´odulos precisam realmente ser otimizados. ´E

comum que se depare com situa¸c˜oes nas quais poucas tarefas representam

grande parte do custo computacional do programa como um todo. Nesse

22

caso, os esfor¸cos de otimiza¸c˜ao podem ser concentrados nos m´odulos mais

custosos computacionalmente.

Existem, entretanto, contrapartidas advindas da utiliza¸c˜ao da programa¸c˜ao

modular. Algumas responsabilidades e compromissos precisam ser assumidos pelos

programadores:

• ´E preciso pensar de forma modular e estar disposto a desenvolver c´odigo

com isso em mente. Isso signiﬁca estar disposto a particionar o c´odigo,

provendo funcionalidades, mesmo que isso n˜ao pare¸ca a forma mais r´apida

de resolver um problema imediato.

• ´E necess´ario especiﬁcar os m´odulos e idealmente document´a-los de forma

a facilitar o utiliza¸c˜ao dos mesmos por outros programadores. Se estamos

interessados em encapsulamento de c´odigo, n˜ao nos interessa que os

usu´arios precisem ler o c´odigo-fonte para compreender a funcionalidade

de cada m´odulo.

• ´E preciso tomar decis˜oes justiﬁcadas sobre como particionar o c´odigo, como

abstrair, por que abstrair. Saber identiﬁcar potenciais trechos de c´odigo

com alta possibilidade de reutiliza¸c˜ao, ou cujo entendimento humano seja

dif´ıcil, ´e fundamental nessa tarefa.

• ´E preciso saber dividir tarefas de forma eﬁciente, garantindo que o trabalho

de um programador n˜ao afete as interven¸c˜oes dos demais integrantes de

uma equipe.

• ´E preciso conhecer as interfaces dos m´odulos, bem como respeit´a-las.

Sobretudo, ´e necess´ario que haja um bom projeto de arquitetura para

que o processo de desenvolvimento se dˆe de forma ordenada e para que

possam ser auferidos os benef´ıcios esperados da programa¸c˜ao modular.

• ´E preciso sabe de que forma poder´a ser assegurada a qualidade de tudo

aquilo que ´e produzido a partir da integra¸c˜ao de diversos m´odulos.

23

Como visto, existem algumas responsabilidades a serem assumidas pelos

programadores interessados em desenvolver software modular. Muitas vezes estes

podem se questionar, por exemplo, sobre o qu˜ao justiﬁc´avel ´e a programa¸c˜ao

modular, o qu˜ao dispendiosos ser˜ao os esfor¸cos para se determinar como

exatamente deve ser particionado o c´odigo do programa, quais ser˜ao as formas

de apresenta¸c˜ao dos m´odulos, como deve ser especiﬁcado cada m´odulo, como deve

se dar a intera¸c˜ao entre os diversos m´odulos de um programa e quais ser˜ao as regras

para isso.

Nos casos em que o programa a ser desenvolvido objetiva resolver

computacionalmente um problema de grande propor¸c˜ao e alta complexidade, o

cen´ario ´e mais favor´avel `a programa¸c˜ao modular. Nesses casos, onde h´a barreiras de

complexidade que intimidam o programador, ´e praticamente natural que ele pense

de maneira modular, dada a sua diﬁculdade em resolver o problema como um todo

de uma s´o vez. Em problemas menos complexos, no entanto, ´e compreens´ıvel que

o programador se sinta tentado a seguir uma l´ogica monol´ıtica. Frequentemente

ele n˜ao deseja fazer decis˜oes de projeto ou mesmo pensar como a segmenta¸c˜ao do

c´odigo deve se dar.

Apesar de nem sempre a programa¸c˜ao modular parecer atrativa aos

programadores, as situa¸c˜oes nas quais ela realmente n˜ao vale a pena s˜ao

muito restritas.

Por mais que o problema a ser resolvido n˜ao apresente

muita complexidade, a programa¸c˜ao modular oferece benef´ıcios que v˜ao al´em da

supera¸c˜ao de barreiras de complexidade.

Em von Staa (2000) ´e apresentada uma justiﬁcativa econˆomica para a

programa¸c˜ao modular, baseada em m´etodos de estima¸c˜ao de tamanho, esfor¸co

e custo de desenvolvimento de programas. Os m´etodos empregados est˜ao baseados

em uma s´erie de parˆametros obtidos por meio de estat´ısticas colhidas de um grande

n´umero de projetos reais. O estudo confronta o esfor¸co de desenvolvimento modular

com o esfor¸co monol´ıtico, considerando diferentes parˆametros como, por exemplo,

o tamanho do programa em linhas de c´odigo, o n´umero de m´odulos e o tamanho

24

m´edio dos m´odulos.

A conclus˜ao de tal estudo ´e que, exceto em programas muito pequenos,

o esfor¸co de programa¸c˜ao modular ´e bastante menor que o esfor¸co monol´ıtico,

mesmo quando consideramos os esfor¸cos adicionais advindos da necessidade

de especiﬁca¸c˜ao e implementa¸c˜ao das interfaces e da integra¸c˜ao dos diferentes

m´odulos. Soma-se a isso o fato de que, conforme o tamanho do programa aumenta,

a diferen¸ca de esfor¸co entre as duas formas de desenvolvimento se torna muito mais

evidente, sempre em favor da abordagem modular.

2.1.4

T´ecnicas de modulariza¸c˜ao

Programas de grande porte s˜ao caracterizados por envolver um grande

n´umero de linhas de c´odigo, grandes equipes de desenvolvimento e geralmente

constituem uma solu¸c˜ao computacional complexa para problemas igualmente

complexos. J´a foi exposto que o padr˜ao de um programa monol´ıtico n˜ao ´e adequado

a esse tipo de sistemas. ´E para assistir principalmente o desenvolvimento desse tipo

de programas que foram desenvolvidas as t´ecnicas de modulariza¸c˜ao.

As t´ecnicas tratam de implementar em c´odigo os conceitos da programa¸c˜ao

modular discutidos at´e aqui,

como a segmenta¸c˜ao do c´odigo-fonte,

o

encapsulamento e as abstra¸c˜oes. Embora tenham seu foco principal em assistir

o desenvolvimento de grandes programas, muitas das t´ecnicas n˜ao deixam de ser

´uteis mesmo a programas de pequeno porte.

Como visto anteriormente, no estudo das linguagens de programa¸c˜ao, os

m´odulos s˜ao entendidos como unidades de software compil´aveis separadamente.

Cada m´odulo pode ser composto de v´arios elementos distintos como fun¸c˜oes,

procedimentos, vari´aveis, constantes e tipos. Elementos como esses s˜ao reunidos

em um m´odulo quando est˜ao de alguma forma relacionados e possuem objetivo

comum. De acordo com Varej˜ao (2004) m´odulos bem projetados possuem um

´unico objetivo claro, bem como uma boa deﬁni¸c˜ao de interface com os demais

m´odulos.

25

O entendimento do prop´osito de um m´odulo e o conhecimento exato

das funcionalidades oferecidas por ele ´e preocupa¸c˜ao do usu´ario. A forma

como o objetivo ´e atingido, entretanto, ´e preocupa¸c˜ao do implementador. Por

usu´ario entendemos um programador que ir´a utilizar os recursos do m´odulo no

desenvolvimento de um outro elemento de software, que pode ser, por exemplo,

outro m´odulo ou um programa completo. Podemos dizer que esses elementos s˜ao

clientes3 do m´odulo em quest˜ao.

A seguir exploraremos algumas t´ecnicas de modulariza¸c˜ao, em uma

abordagem independente de linguagem de programa¸c˜ao.

Ser˜ao apresentadas

as principais estrat´egias de implementa¸c˜ao desenvolvidas para que se pudesse

desenvolver programas de forma mais compartimentada, organizada e eﬁciente

do que com o uso da abordagem monol´ıtica. As estrat´egias podem se aplicar

internamente a um ´unico arquivo de c´odigo-fonte ou mesmo subdividir os

programas em m´ultiplos arquivos com compila¸c˜ao separada.

2.1.4.1

Subprogramas

O desenvolvimento do conceito de subprogramas (procedimentos e fun¸c˜oes)

representou o primeiro passo para que o desenvolvimento de software pudesse

caminhar na dire¸c˜ao da programa¸c˜ao modular.

Subprogramas permitem

o particionamento do programa em diversas por¸c˜oes de c´odigo logicamente

relacionadas.

A ideia ´e que cada subprograma represente claramente uma funcionalidade

bem deﬁnida, funcionando como um instrumento para a abstra¸c˜ao de processos,

conforme visto na se¸c˜ao 2.1.2.

Subprogramas, portanto,

jamais devem ser

entendidos como instrumentos de modulariza¸c˜ao que particionam o c´odigo levando-

se em conta sua extens˜ao, o que resultaria em modulariza¸c˜ao de baixa qualidade.

N˜ao existe raz˜ao para que subprogramas tenham tamanhos semelhantes. O

que deve ser levado em conta ´e identiﬁca¸c˜ao de funcionalidades que possam ser

3 Clientes s˜ao todos os elementos de programa¸c˜ao que fazem uso de funcionalidades ou dados

providos por outros elementos.

26

abstra´ıdas.

O uso de subprogramas evita que trechos de c´odigo sejam exaustivamente

repetidos ao longo de um programa e facilita o re´uso de c´odigo. Se um determinado

processo ou ﬂuxo de execu¸c˜ao precisa ser realizado muitas vezes, ´e poss´ıvel

ter m´ultiplas chamadas a um ´unico subprograma.

Isso facilita n˜ao somente a

implementa¸c˜ao, mas tamb´em a manuten¸c˜ao do programa. No momento em que

uma altera¸c˜ao no processo implementado no subprograma precisa ser feita, ser´a

poss´ıvel alterar o c´odigo de forma localizada. Em contraste, programas com trechos

de c´odigo exaustivamente repetidos demandariam uma busca exaustiva por todas

as ocorrˆencias do trecho para sua altera¸c˜ao.

Subprogramas permitem, ainda, que o processo descrito pelos mesmos seja

aplicado sobre conjuntos de dados diferentes. Isso ´e poss´ıvel gra¸cas ao processo

de parametriza¸c˜ao, por meio do qual podemos fornecer dados aos subprogramas.

Dessa forma, procedimentos e fun¸c˜oes podem operar sobre dados diferentes em

cada chamada, resultando, respectivamente, em comportamentos ou valores de

retorno distintos.

2.1.4.2

Pacotes

No desenvolvimento de grandes sistemas de software,

implementar a

modulariza¸c˜ao exclusivamente por meio de subprogramas n˜ao ´e, em geral, uma

estrat´egia considerada suﬁciente. Esse tipo de modulariza¸c˜ao acaba promovendo

uma segmenta¸c˜ao de c´odigo com granularidade muito ﬁna. No caso de sistemas

muito grandes, isso faz com que se tenha um n´umero muito grande de elementos de

programa¸c˜ao espalhados pelo c´odigo. ´E necess´ario haver um n´ıvel de modulariza¸c˜ao

com granularidade maior.

Al´em disso, dado que a programa¸c˜ao modular ´e marcada pelo re´uso de c´odigo,

nota-se, especialmente em grandes sistemas, que existe uma grande possibilidade

de conﬂitos de nomes entre os elementos das diversas fontes de c´odigo reutilizados.

´E preciso que haja alguma forma de solucionar os conﬂitos de nome.

27

Pacotes s˜ao elementos de programa¸c˜ao com nome pr´oprio que agrupam

diversos outros elementos, sendo alguns vis´ıveis para os usu´arios e outros n˜ao. Ao

mesmo tempo em que representam uma forma de modulariza¸c˜ao de granularidade

grossa, s˜ao elementos capazes de resolver conﬂitos de nome, justamente pelo fato

de possu´ırem nomes pr´oprios. Se existir um conﬂito de nomes entre elementos

provenientes de m´ultiplas fontes de c´odigo, sendo as fontes m´ultiplos pacotes, ´e

poss´ıvel especiﬁcar corretamente de qual pacote ´e o elemento que se quer acessar.

Basta que se utilize a especiﬁca¸c˜ao completa do nome, que inclui o nome do pacote

e o nome do elemento componente em quest˜ao.

2.1.4.3

Tipos de dados

Como discutido na se¸c˜ao 2.1.2, novos tipos de dados representam abstra¸c˜oes

de dados feitas pelo programador. Re´unem uma cole¸c˜ao de dados relacionados

em um ´unico elemento de programa¸c˜ao. Os usu´arios desses tipos de dados podem

enxerg´a-los como um todo coeso, sem que seja necess´ario pensar em como foram

implementados.

Dados do tipo registro, como o struct em C ou structure e record do Fortran

(posteriormente substitu´ıdos pela declara¸c˜ao TYPE) s˜ao denominados tipos de

dados simples. Esse tipo de modulariza¸c˜ao consiste em reunir e combinar um

grupo de dados relacionados em uma ´unica entidade encapsuladora nomeada. O

grupo de dados nomeado pode assim ser tratado como um todo e as vantagens

incluem facilita¸c˜ao do re´uso e melhor legibilidade do c´odigo. O principal ponto

fraco desse tipo de modulariza¸c˜ao ´e n˜ao permitir o ocultamento da informa¸c˜ao,

visto que o acesso aos dados internos do tipo ´e livre para os seus usu´arios. O

ocultamento dos dados ´e permitido pelos Tipos Abstratos de Dados e ser´a discutido

a seguir.

Por sua vez, os Tipos Abstratos de Dados (TADs) s˜ao elementos de

software nos quais uma determinada estrutura de dados ´e criada para representar

um novo tipo. Esta ´e tornada conhecida somente atrav´es das opera¸c˜oes realizadas

28

sobre seus dados. O implementador do TAD decide como ir´a representar os valores

do tipo abstrato e implementa um conjunto de opera¸c˜oes, na forma de fun¸c˜oes

e procedimentos que operam sobre os dados do novo tipo. Para implementa¸c˜ao

de TADs, ´e necess´ario que a linguagem forne¸ca recursos para o ocultamento da

informa¸c˜ao, tornando a implementa¸c˜ao interna do TAD invis´ıvel para o usu´ario,

o que normalmente ´e feito com a especiﬁca¸c˜ao de sua interface. Nela s˜ao expostos

somente os componentes que devem ser p´ublicos, em geral apenas as opera¸c˜oes

oferecidas pelo TAD, sendo os dados internos invis´ıveis aos usu´arios.

A estrat´egia de tornar p´ublicas apenas as opera¸c˜oes e manter os dados

inacess´ıveis aos usu´arios

(information hiding)

implica que o conjunto de

valores internos do tipo ´e acessado e modiﬁcado sempre de maneira indireta,

exclusivamente pela execu¸c˜ao de opera¸c˜oes consultoras e atualizadoras,

respectivamente. Opera¸c˜oes atualizadoras podem ainda proteger os dados contra

altera¸c˜oes feitas de maneira incorreta, impedindo que os dados assumam valores

indevidos. Opera¸c˜oes consultoras/atualizadoras s˜ao frequentemente denominadas

getters/setters, mutators/accessors ou, de forma geral, fun¸c˜oes de acesso. O

usu´ario, portanto, apenas utiliza o TAD como uma caixa preta para resolver seu

problema, sem acesso direto aos dados.

Al´em das opera¸c˜oes de acesso, nos TADs frequentemente existem as

opera¸c˜oes construtoras e destrutoras. De acordo com Varej˜ao (2004) as

construtoras s˜ao respons´aveis por criar e inicializar os TADs e devem ser executadas

antes de quaisquer outras opera¸c˜oes para que se garanta o perfeito funcionamento

das mesmas. As opera¸c˜oes destrutoras, utilizadas quando o uso do TAD n˜ao ´e

mais necess´ario, s˜ao respons´aveis por atividades de ﬁnaliza¸c˜ao, dentre as quais

destaca-se a desaloca¸c˜ao da regi˜ao de mem´oria utilizada pelo TAD.

2.1.4.4

M´ultiplos arquivos e Compila¸c˜ao Separada

As t´ecnicas de modulariza¸c˜ao abordadas at´e o momento podem ser aplicadas

para a modulariza¸c˜ao de programas em um arquivo de fonte ´unico. Com o

29

crescimento dos programas, entretanto, esse tipo de abordagem come¸ca a tornar-se

invi´avel, uma vez que traz problemas pr´aticos tanto para o desenvolvimento quanto

para a manuten¸c˜ao e re´uso de c´odigo.

Programadores

frequentemente

encontram diﬁculdade

e

tˆem sua

produtividade comprometida na escrita e na manuten¸c˜ao de programas com

grandes propor¸c˜oes, se estes estiverem completamente contidos em um ´unico

arquivo fonte. Todas as vezes em que se faz necess´aria uma altera¸c˜ao no c´odigo-

fonte, o ´unico e extenso arquivo de fonte deve ser examinado pelo programador a

ﬁm de que se encontre a regi˜ao que precisa ser alterada. Se dois programadores

pretendem alterar, cada um uma parte espec´ıﬁca do programa, eles precisar˜ao

alterar o mesmo arquivo e ao ﬁm do trabalho, haver´a um esfor¸co adicional

para que se possa unir as altera¸c˜oes feitas por ambos. Quando o trabalho de

um programador precisa ser interrompido e depois retomado, tem-se um custo

adicional para que, novamente, se vasculhe o extenso arquivo a ﬁm de encontrar o

local a partir do qual o trabalho deve continuar.

Outra quest˜ao relevante nesse contexto ´e o fato de que muitos elementos

de programa¸c˜ao de um programa podem ser reutilizados n˜ao somente dentro de

um programa ´unico, mas tamb´em em programas distintos. ´E comum, tanto nas

empresas quanto na comunidade cient´ıﬁca, que grupos de pessoas interessadas em

desenvolver aplica¸c˜oes com algum grau de semelhan¸ca interajam entre si. Muitas

aplica¸c˜oes diferentes podem compartilhar trechos em comum. Programas em um

´unico arquivo fonte diﬁcultam o processo de re´uso, pois exigem que a funcionalidade

a ser utilizada em outros programas seja localizada dentro de um extenso arquivo

e copiada para o programa que far´a uso da mesma.

Naturalmente, dividir os programas em arquivos separados surge como

solu¸c˜ao para os problemas citados. Cada arquivo pode conter trechos de c´odigo

associados a uma funcionalidade ou a um grupo de funcionalidades suﬁcientemente

relacionadas, segundo algum crit´erio l´ogico de divis˜ao que fa¸ca sentido para

a equipe de desenvolvedores. Os pr´oprios arquivos acabam funcionando como

30

indexadores para que os programadores encontrem mais rapidamente as regi˜oes

do c´odigo que desejam reaproveitar ou mesmo onde desejam realizar altera¸c˜oes.

Entretanto, os m´ultiplos arquivos, por si s´o, n˜ao resolvem um problema

comum a grandes projetos de software: o custo da compila¸c˜ao. Em programas

pequenos, o custo em tempo com a compila¸c˜ao pode, muitas vezes,

ser

negligenciado. Em grandes projetos, por´em, o custo em tempo para a compila¸c˜ao

de todo o c´odigo se torna algo impactante na produtividade dos programadores.

Quando adota-se a estrutura de modulariza¸c˜ao por arquivos, o interesse ´e que seja

poss´ıvel a compila¸c˜ao em separado de cada um dos m´odulos fonte, de forma que,

com altera¸c˜oes localizadas em m´odulos espec´ıﬁcos, n˜ao seja necess´ario recompilar

todo o c´odigo-fonte, mas somente somente os m´odulos alterados.

Conforme destacado (Varej˜ao, 2004), para que se possa permitir a compila¸c˜ao

em separado dos m´odulos, surge a necessidade de certo relaxamento na veriﬁca¸c˜ao

de erros por parte do compilador. Consideremos, a t´ıtulo de exemplo, que em um

determinado arquivo fonte foi escrita uma cole¸c˜ao de procedimentos e fun¸c˜oes para

serem utilizadas em outros arquivos fontes clientes. Quando s˜ao feitas altera¸c˜oes

nas unidades clientes, ´e interessante que estas possam ser recompiladas sem que

seja requerida a recompila¸c˜ao do arquivo que cont´em a cole¸c˜ao de subprogramas

utilizados. Com a recompila¸c˜ao sendo feita exclusivamente nas unidades clientes

alteradas, o compilador n˜ao poder´a veriﬁcar se todas as sub-rotinas utilizadas por

elas de fato existem nos m´odulos onde supostamente devem estar implementadas.

Mais do que isso, ´e preciso saber se as rotinas est˜ao sendo utilizadas da maneira

correta quanto ao tipo dos argumentos passados, por exemplo.

Para que a compila¸c˜ao em separado possa coexistir com a veriﬁca¸c˜ao de

tipos, uma estrat´egia usada em muitas linguagens de programa¸c˜ao ´e subdividir

cada um dos m´odulos fonte em dois arquivos: arquivo de interface e arquivo de

implementa¸c˜ao.

Nos arquivos ou m´odulos de interface, tamb´em chamados de m´odulos de

31

deﬁni¸c˜ao4 , existem somente declara¸c˜oes e deﬁni¸c˜oes de elementos como vari´aveis,

tipos e subprogramas que ser˜ao usados por arquivos/m´odulos clientes. Os arquivos

de implementa¸c˜ao, ou m´odulos de implementa¸c˜ao5 , por sua vez, s˜ao onde est˜ao

contidas todas as implementa¸c˜oes completas dos elementos de software declarados

no arquivo de interface.

Nos m´odulos clientes, o que se faz usualmente ´e importar apenas os arquivos

de deﬁni¸c˜ao (header ﬁles). A inclus˜ao dos m´odulos de deﬁni¸c˜ao gera um impacto

praticamente desprez´ıvel no custo de compila¸c˜ao das unidades clientes, visto que

tais m´odulos correspondem apenas a deﬁni¸c˜oes de vari´aveis, constantes, tipos e

assinaturas de subprogramas. Apesar do baixo custo de compila¸c˜ao, os arquivos de

deﬁni¸c˜ao possuem as informa¸c˜oes suﬁcientes para que se possa realizar a veriﬁca¸c˜ao

de tipos.

Sendo assim, com a inclus˜ao dos m´odulos de deﬁni¸c˜ao em todos os arquivos

clientes, a compila¸c˜ao torna-se, al´em de r´apida, segura. A unidades clientes podem

ser recompiladas em separado, sem que seja preciso realizar a recompila¸c˜ao dos

m´odulos de implementa¸c˜ao utilizados, cuja compila¸c˜ao ´e custosa, e sem tampouco

que seja necess´ario abrir m˜ao do rigor nas veriﬁca¸c˜oes de erro do compilador.

No processo de compila¸c˜ao de programas divididos em v´arios arquivos,

quando os m´odulos de implementa¸c˜ao s˜ao fornecidos ao compilador, originam-

se, para cada um dos m´odulos compilados, um correspondente m´odulo objeto.

Finalmente os m´odulos objeto s˜ao integrados por um programa especial ligador

(linker), respons´avel por unir os arquivos objeto e gerar um ´unico arquivo

execut´avel.

2.1.5

Programa¸c˜ao Orientada a Objetos

Orienta¸c˜ao a Objetos ´e um paradigma para o desenvolvimento de

programas particularmente interessante quando precisa-se modelar e resolver

4 M´odulo de deﬁni¸c˜ao ´e o elemento que cont´em apenas o c´odigo de deﬁni¸c˜ao da interface

do m´odulo fonte.

5 M´odulo de implementa¸c˜ao ´e o elemento que implementa de fato todas as funcionalidades

providas pelo m´odulo fonte, encapsulando todo c´odigo das mesmas.

32

computacionalmente problemas que possam ser expressos em termos de diversos

conjuntos de dados minimamente relacionados,

submetidos a conjuntos de

opera¸c˜oes.

Isso se deve ao fato de que a programa¸c˜ao Orientada a Objetos ´e

fortemente focalizada em entidades do mundo real e nos processos e opera¸c˜oes

realizados por tais entidades ou aos quais seus dados se sujeitam.

As classes podem representar de maneira eﬁciente entidades do mundo real.

Permitem a deﬁni¸c˜ao de novos tipos oferecendo meios para o encapsulamento de

dados e de oprea¸c˜oes sobre esse dados. Os primeiros sob forma de seus atributos,

e as ´ultimas na forma de seus procedimentos ou fun¸c˜oes, denominados m´etodos.

A estrutura de classes se assemelha da decomposi¸c˜ao de programas em m´odulos;

entretanto, diferente de m´odulos, os quais podem ser simplesmente arquivos

contendo cole¸c˜oes de fun¸c˜oes, as classes podem ser instanciadas, correspondendo

cada instˆancia a um objeto6 em mem´oria. Classes funcionam como modelos que

cont´em as informa¸c˜oes necess´arias para a aloca¸c˜ao dos dados dos objetos e para a

inicializa¸c˜ao de seus atributos.

Na se¸c˜ao 2.1.4.3 vimos que, com os Tipos Abstratos de Dados (TADs), os

programadores s˜ao capazes de criar novos tipos, promovendo encapsulamento de

dados e processos, com ocultamento da informa¸c˜ao e prote¸c˜ao de dados. Em boa

parte das linguagens de programa¸c˜ao, a estrutura de dividir m´odulos fonte em

dois arquivos, um de interface e um outro de implementa¸c˜ao, discutida na se¸c˜ao

2.1.4.4, ´e a estrat´egia adotada para a implementa¸c˜ao de TADs. Com o advento de

linguagens de programa¸c˜ao Orientadas a Objetos, as classes surgiram como uma

nova maneira de se implementar os TADs.

As classes oferecem todos os recursos dos TADs, como encapsulamento

de dados e processos, ocultamento da informa¸c˜ao, fun¸c˜oes de acesso, opera¸c˜oes

construtoras e destrutoras. Al´em de oferecerem todos os mecanismos necess´arios

para a implementa¸c˜ao de um TAD, classes oferecem prote¸c˜ao de dados por meio

de modiﬁcadores de acesso, apresentam benef´ıcios em legibilidade, redigibilidade e

6 Um Objeto ´e uma instˆancia de uma classe. Pode-se instanciar (construir, criar) m´ultiplos

objetos em mem´oria a partir de uma ´unica classe

33

conﬁabilidade (Varej˜ao, 2004).

Segundo von Staa (2000), com utiliza¸c˜ao de linguagens de programa¸c˜ao

orientadas a objetos e tomando-se algum cuidado ao projetar classes, ´e poss´ıvel

que elas se tornem integralmente utiliz´aveis em diferentes programas, viabilizando

o re´uso de quantidades signiﬁcativas de c´odigo. O autor destaca que tal forma de

re´uso verbatim7 est´a entre as principais formas de redu¸c˜ao de custos e melhoria

na qualidade de software. Esta ´e uma motiva¸c˜ao para uma das contribui¸c˜oes

do presente trabalho: a cria¸c˜ao de um arcabou¸co com os recursos do paradigma

de programa¸c˜ao orientada a objetos para um m´odulo de um simulador cient´ıﬁco,

com potencial de re´uso em outros programas. No cap´ıtulo 3 ser´a abordada a a

refatora¸c˜ao do m´odulo.

As classes tornam ainda especiais as opera¸c˜oes construtoras e destrutoras.

As opera¸c˜oes construtoras, nas classes chamadas de m´etodos construtores,

s˜ao executadas quando da instˆancia de um objeto da classe e s˜ao respons´aveis

basicamente por inicializar os campos da classe com valores “neutros”, como, por

exemplo, zero no caso de membros de valor num´erico, ou valor NULL para o caso

de ponteiros. Alternativamente, podem ser atribu´ıdos valores com signiﬁcado

de “valor indeﬁnido” mais facilmente detect´aveis para a localiza¸c˜ao de erros de

inicializa¸c˜ao, caso os campos venham a ser usados antes de que a eles seja

atribu´ıdo qualquer valor. Os m´etodos destrutores, por sua vez, s˜ao respons´aveis por

opera¸c˜oes de ﬁnaliza¸c˜ao realizadas quando o uso das classes n˜ao ´e mais necess´ario.

Dentre tais opera¸c˜oes, a mais comum ´e a desaloca¸c˜ao das regi˜oes de mem´oria dos

objetos que n˜ao ser˜ao mais usados.

Com TADs, a modelagem de v´arias entidades do mundo real com aspectos

em comum produz aspectos indesej´aveis que s˜ao resolvidos com o uso classes.

Para representar, por exemplo, as entidades Pessoa, Aluno e Professor com TADs

precisamos de 3 tipos de dados com muitos atributos em comum. Aluno e Professor

s˜ao, ambos, pessoas. No entanto, como o uso de TADs n˜ao podemos representar as

7 Re´uso Verbatim ´e a forma de re´uso na qual n˜ao s˜ao feitas altera¸c˜oes na unidade reutilizada.

34

entidades Aluno e Professor aproveitando aquilo que ´e comum `as duas e tamb´em

`a entidade Pessoa. Isso cria enorme quantidade de replica¸c˜ao de c´odigo. Com a

orienta¸c˜ao a objetos, pode-se criar estruturas de classe baseadas em heran¸ca, um

dos conceitos mais fundamenais da orienta¸c˜ao a objetos ao lado do polimorﬁsmo.

2.1.5.1

Heran¸ca

A heran¸ca ´e um relacionamento entre classes onde uma classe denominada

herdeira, ou classe ﬁlha, especializa, reﬁna ou torna mais particulares as

propriedades de outra classe denominada superclasse, ou classe m˜ae. No exemplo

citado anteriormente, as entidades Pessoa, Aluno e Professor, se representadas

por classes, poderiam conﬁgurar uma rela¸c˜ao de heran¸ca. As classes Aluno

e Professor s˜ao herdeiras da classe Pessoa e por isso possuem os mesmos

atributos e m´etodos da classe Pessoa. Diz-se que a rela¸c˜ao de heran¸ca ´e transitiva,

pois uma classe herdeira pode, por sua vez, ser superclasse de outras herdeiras. Por

exemplo, professor (que herda de Pessoa) pode ser superclasse das herdeiras

ProfessorSubstituto e ProfessorTitular.

Membros adicionais podem existir nas classes herdeiras, visto que

especializam as superclasses. No caso citado, tanto Aluno como Professor

herdariam, por exemplo, os atributos nome e sobrenome da superclasse Pessoa.

Aluno poderia implementar mais membros como nota1, nota2, os m´etodos

aprovar() e reprovar(); Professor, por sua vez, poderia implementar os

membros salario, dataDeAdmissão e o m´etodo calcularSalario().

Qualquer objeto instanciado de uma superclasse n˜ao pode ser reﬁnado

por imposi¸c˜ao de tipos (type casting) para um objeto de classe herdeira. Se

instanciarmos um objeto Pessoa1, da classe Pessoa, este n˜ao pode ser

especializado por casting de tipos para um objeto do tipo Aluno, pois nem todos

os membros de Aluno s˜ao conhecidos pelo objeto da classe Pessoa. Se criarmos,

em vez disso, um objeto Aluno1 da classe Aluno, este poder´a ser promovido a

um objeto da classe Pessoa e, inclusive, ser posteriormente reﬁnado novamente

35

para um Aluno, mas jamais para um Professor.

2.1.5.2

Polimorﬁsmo

Outro conceito extremamente importante do paradigma orientado a objetos,

diretamente relacionado com o conceito de heran¸ca, ´e o polimorﬁsmo, palavra

cujas origens gregas remetem `a ideia de ”muitas faces”. O conceito de heran¸ca

permite que m´etodos de superclasses sejam redeﬁnidos nas classes herdeiras.

Nesse contexto, redeﬁnir signiﬁca escrever novamente tais fun¸c˜oes, com assinaturas

idˆenticas, por´em com implementa¸c˜oes internas peculiares, em cada uma das classes

ﬁlhas. Fun¸c˜oes dessa natureza s˜ao ditas fun¸c˜oes polimorfas, e constituem o

tipo de polimorﬁsmo mais comum implementado pela maioria das linguagens de

programa¸c˜ao, denominado polimorﬁsmo universal de subclasses.

Imaginemos uma estrutura com uma superclasse Pol´ıgono e duas classes

herdeiras, Quadrado e Triangulo, as quais possuem implementa¸c˜oes distintas de

um m´etodo calcularArea com assinatura idˆentica. A deﬁni¸c˜ao sobre qual

implementa¸c˜ao ser´a aplicada depende da classe do objeto em quest˜ao e ao qual

o m´etodo se aplica. De acordo com von Staa (2000), essa propriedade promove

facilidades para a programa¸c˜ao, visto que o pr´oprio objeto determina como ele

deve ser processado, retirando do programador a responsabilidade de determinar

a forma como os objetos devem ser processados, dependendo de sua classe.

Al´em do caso discutido, o polimorﬁsmo pode aparecer ainda de outras

maneiras. Caso um m´etodo seja implementado v´arias vezes com assinaturas

diferentes, ocorre o tipo de polimorﬁsmo denominado sobrecarga de m´etodos.

Sr o comportamento dos m´etodos variar de acordo com convers˜oes impl´ıcitas de

tipos sobre os dados recebidos em seus parˆametros, o polimorﬁsmo ´e conhecido

como polimorﬁsmo de coer¸c˜ao.

36

2.2

Computa¸c˜ao Paralela

´E fato

que

as

simula¸c˜oes

computacionais

cient´ıﬁcas

utilizam-se

frequentemente de modelos numericamente intensivos. Visando validar teorias

e fazer predi¸c˜oes acerca de seus objetos de estudo, ´e comum que os cientistas

realizem um grande n´umero de simula¸c˜oes com tais modelos, utilizando diferentes

conjuntos de parˆametros de entrada. Dessa forma, ´e natural que existam

demandas de desempenho nos modelos computacionais cient´ıﬁcos, de forma que

as simula¸c˜oes possam ser realizadas em tempos de execu¸c˜ao compat´ıveis com os

interesses da ciˆencia.

Os maiores e mais desﬁadores problemas da ciˆencia moderna geralmente

lidam com combina¸c˜oes de fenˆomenos de diferentes ´areas do conhecimento,

englobando uma enorme gama de escalas de tempo e espa¸co. Nas simula¸c˜oes

computacionais cient´ıﬁcas, as escalas maiores de tempo e espa¸co est˜ao relacionadas

com a extens˜ao do dom´ınio dos problemas modelados, enquanto as menores

est˜ao relacionadas com a resolu¸c˜ao desejada ou necess´aria nos modelos.

Modelar problemas diminuindo a extens˜ao dos dom´ınios para que sejam

equipar´aveis `as escalas dos m´ınimos detalhes, ou dos menores fenˆomenos f´ısicos

que se possa descrever,

frequentemente ´e algo que determina requisitos de

mem´oria. Por outro lado, reduzir as janelas de tempo das escalas temporais

aos menores intervalos ´e algo que implicar´a em desaﬁos relacionados ao tempo de

computa¸c˜ao.

Grande parte das discuss˜oes atuais no ˆambito da programa¸c˜ao cient´ıﬁca est˜ao

focadas no desempenho escal´avel (Rouson et al., 2011). No in´ıcio desde cap´ıtulo,

vimos que a escalabilidade de execu¸c˜ao est´a relacionada com o aproveitamento

eﬁciente do m´aximo dos recursos de hardware dispon´ıveis. Al´em disso, um c´odigo

escal´avel, do ponto de vista de sua execu¸c˜ao, deve estar preparado para crescer no

quesito desempenho de forma compat´ıvel com um eventual incremento nos recursos

computacionais oferecidos.

Esses motivos deixam claro o porquˆe de a ciˆencia se importar tanto com

37

a m´axima utiliza¸c˜ao dos recursos computacionais dispon´ıveis. A evolu¸c˜ao do

hardware nas ´ultimas d´ecadas tomou claramente a dire¸c˜ao do desenvolvimento

de m´aquinas cada vez mais paralelas (Kirk e Hwu, 2013). A escalabilidade de

execu¸c˜ao dos programas est´a fortemente condicionada `a explora¸c˜ao eﬁciente de

m´aquinas paralelas.

2.2.1

Arquiteturas Paralelas

´E grande a variedade de arquiteturas de hardware para a computa¸c˜ao

paralela. Redes de esta¸c˜oes de trabalho, clusters e esta¸c˜oes de trabalho com

m´ultiplos processadores s˜ao alguns dos v´arios ambientes onde se pode explorar a

computa¸c˜ao paralela. No estudo da computa¸c˜ao paralela, a Taxonomia de Flynn

tem sido frequentemente utilizada ao longo das ´ultimas d´ecadas para classiﬁcar as

arquiteturas dos computadores, as quais s˜ao divididas em quatro grupos de acordo

com o n´umero ﬂuxos de instru¸c˜oes (instruction streams) e de dados (data streams)

que podem ser operados simultaneamente pela m´aquina. Nas se¸c˜oes seguintes

exploramos cada um dos grupos: SISD, SIMD, MISD e MIMD.

2.2.1.1

Single Instruction, Single Data (SISD)

Em um sistema SISD, como mostrado na ﬁgura 2.1, um ´unico ﬂuxo de

instru¸c˜oes opera em um ´unico ﬂuxo de dados. Essa ´e a arquitetura de um sistema

cl´assico de Von Neumman. A m´aquina executa apenas uma instru¸c˜ao por vez e

pode buscar ou armazenar um dado por vez.

2.2.1.2

Single Instruction, Multiple Data (SIMD)

Nos sistemas com arquitetura SIMD, como mostrado na ﬁgura 2.2, um ´unico

ﬂuxo de instru¸c˜oes ´e distribu´ıdo para v´arios processadores, cada qual com seu

ﬂuxo de dados. As mesmas instru¸c˜oes s˜ao, portanto, aplicadas, em paralelo,

a conjuntos de dados diferentes. De acordo com Pacheco (2011), um sistema

SIMD ´e caracterizado pela presen¸ca de uma Unidade de Controle (UC) e m´ultiplas

38

Figura 2.1: A arquitetura SISD

Unidades Logico-Aritm´eticas (ULAs). Cada instru¸c˜ao ´e distribu´ıda pela UC para

todas as ULAs que as aplicam a seus dados de entrada de forma s´ıncrona. Sistemas

SIMD s˜ao ideais para a paraliza¸c˜ao de loops simples que operam em longos arrays

de dados. Este tipo de paralelismo obtido dividindo os dados pelos processadores e

estes aplicando as mesmas instru¸c˜oes sobre seus subconjuntos dos dados ´e chamado

de paralelismo de dados (data parallelism).

No in´ıcio dos anos 1990, uma fabricante de sistemas SIMD (Thinking

Machines) era a maior fabricante de m´aquinas paralelas do planeta. No ﬁnal dos

anos 1990, os ´unicos sistemas SIMD produzidos passaram a ser os processadores

vetoriais (vector processors), ainda presentes em arquiteturas modernas de CPUs,

sob a forma de unidades especializadas em opera¸c˜oes vetoriais: as chamadas

Vector Processing Units (VPUs), munidas de registradores e instru¸c˜oes

vetoriais, al´em de unidades funcionais vetorizadas para o pipelining.

Mais recentemente as unidades de processamento gr´aﬁco (GPUs)

tamb´em fazem uso da computa¸c˜ao SIMD. Segundo Pacheco (2011), aplica¸c˜oes

gr´aﬁcas frequentemente utilizam fun¸c˜oes de shader que muitas vezes resultam em

um ´unico ﬂuxo de controle aplicado a diferentes elementos e isso faz com que o

desempenho de GPUs seja otimizado com a computa¸c˜ao SIMD. Isso ´e obtido com

39

Unidade de Controle instruções Processador dados a inclus˜ao de um grande n´umeros de ULAs em um ´unico core das GPUs.

Figura 2.2: A arquitetura SIMD

2.2.1.3

Multiple Instruction, Single Data (MISD)

N˜ao existem sistemas de computa¸c˜ao bem conhecidos que se enquadrem no

modelo MISD, o qual ´e citado na taxonomia de Flynn apenas por motivos de

completude.

2.2.1.4

Multiple Instruction, Multiple Data (MIMD)

Em um sistema do modelo MIMD, mostrado na ﬁgura 2.3, cada processador

tem seu pr´oprio ﬂuxo de instru¸c˜oes e opera sobre seus pr´oprios dados individuais.

40

Unidade de Controle instruções Processador dados dados Processador dados dados Processador dados dados Processador dados dados Figura 2.3: A arquitetura MIMD

Para Mattson et al. (2004) o modelo MIMD ´e muito geral para ser ´util

no entendimento pr´atico de m´aquinas reais. Essa categoria ´e normalmente

reclassiﬁcada de acordo com a organiza¸c˜ao de mem´oria. O modelo pode

ser decomposto em m´aquinas de mem´oria compartilhada e de mem´oria

distribu´ıda.

2.2.2

Mem´oria compartilhada

Em um sistema de mem´oria compartilhada,

todos os processadores

compartilham um ´unico espa¸co de endere¸camento de mem´oria e se comunicam

entre si pela escrita e leitura em vari´aveis compartilhadas. Os sistemas de mem´oria

compartilhada se subdividem em sistemas SMP (Symmetric Multiprocessors) e

sistemas NUMA (Non Uniform Memory Acess).

Nos sistemas SMP, todos os processadores acessam todas as regi˜oes da

mem´oria de maneira uniforme, ou seja, na mesma velocidade. S˜ao os sistemas

mais f´aceis para se programar, pois os programadores n˜ao s˜ao respons´aveis pela

distribui¸c˜ao das estruturas de dados entre os processadores. Um n´umero elevado

de unidades de processamento aumenta a disputa pelo acesso `a mem´oria. Por essa

raz˜ao, sistemas desse tipo costumam possuir um n´umero limitado de processadores,

j´a que a largura de banda para o acesso `a mem´oria ´e um fator limitante para o

41

Unidade de Controle instruções Processador dados dados Unidade de Controle instruções Processador dados dados Unidade de Controle instruções Processador dados dados Unidade de Controle instruções Processador dados dados Rede de Interconexão desempenho.

Nos sistemas NUMA, toda a mem´oria ´e ﬁsicamente acess´ıvel a todos as

unidade de processamento, entretanto, alguns blocos de mem´oria podem estar

mais diretamente associados a certos processadores do que a outros. Dessa forma

o acesso n˜ao ´e completamente uniforme em todas as regi˜oes da mem´oria. Isso pode

reduzir a disputa pelo acesso a mem´oria e diminuir os efeitos de um gargalo de

desempenho devido `a largura de banda da mem´oria. Por´em, os tempos de acesso

de um dado processador a diferentes regi˜oes de mem´oria podem sofrer varia¸c˜oes,

sendo sens´ıveis ao qu˜ao pr´oxima cada regi˜ao est´a do processador.

Para diminuir os efeitos do acesso n˜ao uniforme, cada processador possui

uma mem´oria cache e um protocolo de coerˆencia entre as caches dos v´arios

processadores, o que faz com que o modelo receba frequentemente o nome de

ccNUMA (Cache Coherent Non Uniform Memory Acess). Programar para tais

sistemas ´e equivalente a programar par sistemas SMP, mas para que se extraia o

melhor desempenho ´e necess´ario maior aten¸c˜ao `a localidade dos dados e aos efeitos

da cache.

2.2.3

Mem´oria Distribu´ıda

Nos sistemas de mem´oria distribu´ıda, cada processador possui seu pr´oprio

espa¸co de endere¸camento individual.

Isso implica que a comunica¸c˜ao entre os

processadores n˜ao se possa fazer por meio de vari´aveis compartilhadas, como no

caso anterior. Nos sistemas com mem´oria distribu´ıda a comunica¸c˜ao se d´a por

troca de mensagens, as quais devem ser feitas explicitamente pelo programador.

Al´em disso, os programadores precisam se responsabilizar pela divis˜ao dos dados

entre as mem´orias dos processadores. Apesar de representar uma responsabilidade,

isso tamb´em representa uma oportunidade. Em aplica¸c˜oes com estruturas de dados

muito grandes, incapazes de residir inteiramente na mem´oria de uma m´aquina de

mem´oria compartilhada, ´e poss´ıvel utilizar ambientes de mem´oria distribu´ıda para

particion´a-las.

42

Dependendo da topologia e das tecnologias empregadas para a interconex˜ao

entre os processadores dos sistemas de mem´oria distribu´ıda, a velocidade de

comunica¸c˜ao pode variar drasticamente, desde praticamente t˜ao r´apidas quanto

mem´oria uniﬁcada at´e v´arias ordens de magnitude mais lenta, como no caso

de clusters de PCs conectados via redes ethernet. Clusters s˜ao sistemas de

computa¸c˜ao de mem´oria distribu´ıda compostos por computadores conectador por

alguma infraestrutura de rede. Com a tecnologia de redes em cont´ınuo avan¸co e

a comunica¸c˜ao cada vez mais r´apida, os clusters tˆem se tornado cada vez mais

comuns e mais poderosos, sendo a principal representa¸c˜ao pr´atica dessa classe de

sistemas.

2.2.4

Ambientes de Computa¸c˜ao Paralela

Um ambiente de computa¸c˜ao paralela conﬁgura um conjunto de tecnologias,

ferramentas e recursos de linguagens de programa¸c˜ao, necess´arios para o

desenvolvimento de aplica¸c˜oes paralelas. O produto ﬁnal provido pelo conjunto

de todos os componentes do ambiente de programa¸c˜ao ´e um modelo de

programa¸c˜ao, o qual

fornece uma abstra¸c˜ao do hardware sobre a qual os

programadores atuam.

Nos computadores paralelos, existe uma grande variedade de modelos de

programa¸c˜ao, dependendo das caracter´ısticas particulares de hardware que deﬁnem

como os processadores s˜ao integrados para formar um sistema ´unico. Os modelos

de programa¸c˜ao mais utilizados, por´em, se baseiam em uma das classiﬁca¸c˜oes vistas

nas se¸c˜oes anteriores: mem´oria compartilhada, mem´oria distribu´ıda com troca de

mensagens ou uma combina¸c˜ao de ambas.

Nesta se¸c˜ao faremos uma apresenta¸c˜ao dos dois mais difundidos modelos

de programa¸c˜ao paralela existentes: O OpenMP (Open Multi-Processing), para

mem´oria compartilhada e o MPI (Message Passing Interface), para mem´oria

distribu´ıda.

43

2.2.4.1

O modelo OpenMP

O OpenMP ´e um conjunto de diretivas e fun¸c˜oes de biblioteca para o

desenvolvimento de programas paralelos em ambientes de mem´oria compartilhada.

´E combinado com C, C++ ou Fortran para a cria¸c˜ao de uma linguagem multithread,

ou seja, as unidades de execu¸c˜ao de programas em OpenMP s˜ao as threads, as quais

compartilham um ´unico espa¸co de endere¸camento. Dessa forma, a comunica¸c˜ao se

d´a atrav´es da manipula¸c˜ao de vari´aveis compartilhadas.

A deﬁni¸c˜ao formal do OpenMP cont´em duas especiﬁca¸c˜oes: uma para

Fortran e outra para C e C++, embora sejam ambas bastante similares. Baseados

no modelo fork/join, os programas em OpenMP come¸cam sua execu¸c˜ao com uma

´unica thread (master) e, em pontos espec´ıﬁcos do programa, criam-se threads

adicionais (fork ). As m´ultiplas threads

executam em paralelo em trechos de

c´odigo denominados regi˜oes paralelas. Ao ﬁm das regi˜oes paralelas, cada thread

aguarda que todas as demais tenham conclu´ıdo a execu¸c˜ao do trecho e voltam a

se unir (join) em uma ´unica thread master.

O modelo OpenMP foi criado com o objetivo de ser simples para os

desenvolvedores de aplica¸c˜oes. Apesar de o melhor desempenho ser sempre

desej´avel, ´e decis˜ao frequente abrir-se m˜ao do desempenho m´aximo, caso isso venha

a tornar dif´ıcil e custoso o desenvolvimento e a manuten¸c˜ao das aplica¸c˜oes. Por

isso, o OpenMP foi desenvolvido com dois princ´ıpios b´asicos: a equivalˆencia

sequencial e o paralelismo incremental.

Equivalˆencia sequencial signiﬁca que o programa paralelo deve gerar os

mesmos resultados utilizando-se uma ou mais threads, ou seja, o resultado da vers˜ao

paralela deve ser igual ao resultado da execu¸c˜ao serial. Segundo Mattson et al.

(2004), um programa com equivalˆencia sequencial ´e mais f´acil de manter e muito

mais f´acil de desenvolver e compreender.

O paralelismo incremental ´e um estilo de programa¸c˜ao paralela onde

o programa evolui gradualmente de sua vers˜ao serial para a vers˜ao paralela.

O programador inicia seus trabalhos com uma vers˜ao serial da aplica¸c˜ao em

44

funcionamento. Em seguida,

identiﬁca regi˜oes no c´odigo onde vale a pena

explorar o paralelismo (usualmente chamadas de hotspots). Assim, o paralelismo

´e adicionado de forma incremental em cada uma dessas regi˜oes. Com essa

abordagem, a cada fase do processo tem-se uma vers˜ao completamente funcional,

que pode ser testada, aumentando a chance de sucesso.

Infelizmente, nem sempre ser´a poss´ıvel que o paralelismo incremental

conduza `a equivalˆencia sequencial. Muitas vezes um algoritmo paralelo precisa

refatorar completamente o seu an´alogo serial.

Existem tamb´em algoritmos

paralelos que simplesmente n˜ao funcionam com uma ´unica thread, de maneira serial.

Apesar das diﬁculdades, os dois conceitos, equivalˆencia sequencial e paralelismo

incremental, guiaram o desenvolvimento do modelo OpenMP e s˜ao considerados

boas pr´aticas de programa¸c˜ao.

2.2.4.2

O modelo MPI

O MPI (Message Passing Interface), criado no in´ıcio da d´ecada de 1990,

´e o modelo de programa¸c˜ao padr˜ao para sistemas de computa¸c˜ao de mem´oria

distribu´ıda com troca de mensagens. A unidade de execu¸c˜ao do MPI s˜ao processos

e, naturalmente, cada um possui seu espa¸co de endere¸camento pr´oprio. O conceito

central do MPI ´e a troca de mensagens. Cada processo precisa agrupar informa¸c˜oes

em uma mensagem e envi´a-la a outros processos que devem estar preparados para

recebˆe-las. A comunica¸c˜ao ´e responsabilidade do programador.

O MPI ´e distribu´ıdo na forma de uma biblioteca, originalmente com vers˜oes

para C e Fortran, apesar de outras linguagens tamb´em terem sido contempladas.

Existem muitas implementa¸c˜oes do MPI com uso difundido, por´em as duas mais

comuns s˜ao LAM/MPI e MPICH, ambas disponibilizadas gratuitamente pelos

mantenedores. Mais do que um simples mecanismo para a troca de mensagens, o

MPI oferece rotinas para sincroniza¸c˜ao de processos, distribui¸c˜ao dos dados para os

diferentes processos e muito mais funcionalidades que suportam o desenvolvimento

de programas paralelos.

45

A ideia b´asica de troca de mensagens levanta questionamentos sobre os

detalhes de como isso ´e feito na pr´atica. O que os processos podem fazer enquanto

enviam mensagens, como as mensagens podem ser identiﬁcadas de forma que cada

envio seja pareado com um respectivo recebimento s˜ao questionamentos comuns.

O MPI deﬁne solu¸c˜oes para essas e outras quest˜oes com os conceitos de grupos

de processos e contextos de comunica¸c˜ao.

Um grupo de processos engloba todos os processos envolvidos em uma

computa¸c˜ao. No in´ıcio da execu¸c˜ao de um programa, todos os processos est˜ao

agrupados em um ´unico grupo. Posteriormente, o programador pode subdividi-

los, agrupando-os em grupos menores envolvidos em uma determinada atividade e

pode controlar como os grupos interagem.

J´a os contextos de comunica¸c˜ao fornecem um meio para que sejam

agrupados conjuntos de comunica¸c˜oes relacionadas.

Em qualquer troca de

mensagens, ´e necess´ario que as mesmas sejam identiﬁcadas de forma que se saiba

quem deve recebˆe-las e quem as enviou. Em MPI, as mensagens s˜ao identiﬁcadas

com as IDs dos processos que as enviam e daqueles que devem recebˆe-las. Apesar

de intuitivo, o conceito de identiﬁcar mensagens com IDs de processos pode n˜ao

funcionar em certas situa¸c˜oes, especialmente em aplica¸c˜oes complexas que incluem

bibliotecas reutilizadas de outros programas.

Se as bibliotecas incluem chamadas ao MPI, existe o risco de que a

aplica¸c˜ao cliente e as bibliotecas compartilhem IDs de processos de origem

e destino acidentalmente. Ainda mais porque o programador da aplica¸c˜ao

normalmente desconhece os detalhes das implementa¸c˜oes das bibliotecas utilizadas.

Os contextos de comunica¸c˜ao aparecem como solu¸c˜ao para esse problema. Cada

envio e cada recebimento pertence a um ´unico contexto de comunica¸c˜ao. A cria¸c˜ao

de v´arios contextos pode evitar os problemas citados.

Os contextos de comunica¸c˜ao e os grupos de processos s˜ao encapsulados pelo

MPI em uma ´unica entidade denominada comunicador (communicator ). Apesar

de n˜ao ser necess´ario lidar diretamente com comunicadores em todos os programas,

46

a maioria das fun¸c˜oes do MPI faz referˆencia a um comunicador e ´e essencial para

programadores interessados em desenvolver componentes de software reutiliz´aveis

que manipulem comunicadores.

47

Cap´ıtulo 3

Refatora¸c˜ao de um m´odulo do simulador

com inclus˜ao do paradigma orientado a

objetos

Neste cap´ıtulo, s˜ao abordadas evolu¸c˜oes no c´odigo-fonte de um simulador

da ciˆencia de escoamentos em reservat´orios de g´as em folhelhos, desenvolvido e

utilizado por pesquisadores e alunos do LNCC.

Com foco na organiza¸c˜ao est´atica e estrutural do c´odigo-fonte, o cap´ıtulo

apresenta e analisa, primeiramente, a evolu¸c˜ao anterior a este trabalho pela qual

o simulador passou desde sua vers˜ao inicial. Em seguida, o cap´ıtulo apresenta e

descreve com maiores detalhes a contribui¸c˜ao espec´ıﬁca do presente trabalho nesse

quesito, a qual consiste em uma reestrutura¸c˜ao de um dos m´odulos do simulador

com incorpora¸c˜ao do paradigma de orienta¸c˜ao a objetos.

Por ser um produto sem um projeto inicial de longo prazo e por ter

prop´osito cient´ıﬁco, o simulador tem passado por um processo evolutivo continuado

que atende a demandas pontuais de seus usu´arios, especialistas do dom´ınio de

aplica¸c˜ao que, frequentemente, desejam incorporar novas funcionalidades e realizar

experimentos espec´ıﬁcos, dando origem a diferentes vers˜oes que passam a levar em

conta diferentes fenˆomenos f´ısicos.

`A medida em que tais demandas espec´ıﬁcas foram atendidas, aspectos

modulares acabaram sendo gradativamente incorporadas ao c´odigo do simulador,

48

o que deixa clara a tendˆencia de que os programas cient´ıﬁcos em geral evoluam

no sentido tornarem-se produtos cada vez mais ﬂex´ıveis, adapt´aveis e com

componentes reaproveit´aveis. O m´odulo reestruturado neste trabalho corresponde

`a por¸c˜ao do c´odigo-fonte do simulador respons´avel pela montagem e solu¸c˜ao dos

sistemas de equa¸c˜oes lineares do m´etodo de elementos ﬁnitos e possui componentes

com alto potencial de re´uso neste ou em outros simuladores cient´ıﬁcos similares.

3.1

Evolu¸c˜ao do simulador de escoamentos em meios porosos

Como visto no cap´ıtulo 1,

o simulador utilizado neste

trabalho,

bem como outros desenvolvidos pelo no LNCC,

tomam como ponto de

partida a implementa¸c˜ao do m´etodo de

elementos ﬁnitos proposta em

Hughes (1987):

o programa DLEARN, que pode ser encontrado no link :

http://www.zsoil.com/dlearn/.

Escrito originalmente no padr˜ao Fortran 77, durante a d´ecada de 1980,

quando tamb´em foram desenvolvidos outros programas semelhantes, como o

ADINA software (Bathe, 1982), o programa de elementos ﬁnitos DLEARN, de

Hughes (1987), se destacou em sua ´epoca, sendo um bom programa para o per´ıodo

no qual foi criado. O programa precisou lidar ou contornar limita¸c˜oes/restri¸c˜oes

impostas pelo pr´oprio padr˜ao da linguagem, como, por exemplo, a limita¸c˜ao do

n´umero de caracteres nos nomes de vari´aveis e subprogramas, que diﬁcultava a

legibilidade e o entendimento humano; ou a ausˆencia de aloca¸c˜ao dinˆamica de

mem´oria, que, por sua vez, tornava dif´ıcil a tarefa de adequar o programa a

diferentes tamanhos de problemas.

Devido ao tamanho vari´avel de diferentes estruturas de dados em diferentes

problemas, ´e algo ineﬁciente deﬁnir valores ﬁxos para estes tamanhos, o que pode

ocasionar erros ou, sen˜ao, desperd´ıcio de mem´oria. O programa original de Hughes

(1987) contornava esse problema prevendo uma esp´ecie de emula¸c˜ao da aloca¸c˜ao

dinˆamica com o uso de um grande array est´atico, dentro do qual s˜ao virtualmente

“alocados” m´ultiplos vetores menores em tempo de execu¸c˜ao com o aux´ılio de

49

vari´aveis que indicam o ponto de in´ıcio de cada um deles.

A ﬁgura 3.1 ilustra tal estrat´egia. Um grande vetor est´atico A, presente na

implementa¸c˜ao original de Hughes (1987), simula um espa¸co de aloca¸c˜ao dinˆamica

em mem´oria, no qual residem v´arios vetores menores, dentre eles alhs, brhs,

lm, id, idiag, os quais podem ser localizados em seu interior por meio de

vari´aveis inteiras (mpalhs, mpbrhs, mplm, mpid, mpidiag) com os ´ındices dos

elementos de A nos quais iniciavam cada uma das ´areas reservadas aos vetores

correspondentes.

Figura 3.1: Vetor est´atico ´unico comportando m´ultiplos vetores menores de
diferentes tamanhos deﬁnidos em tempo de execu¸c˜ao.

Do ponto de vista de sua organiza¸c˜ao estrutural, embora seja concentrada

em um ´unico arquivo de fonte, tal implementa¸c˜ao base j´a possui certo n´ıvel de

modulariza¸c˜ao, gra¸cas `a sua organiza¸c˜ao em uma estrutura de subprogramas.

Hughes (1987) apresenta a organiza¸c˜ao do programa fornecendo um ´ındice com

pouco mais de uma centena de subprogramas, os quais s˜ao, em sua maioria, do

tipo procedimento (SUBROUTINE) e, em sua minoria, do tipo fun¸c˜ao (FUNCTION).

Sabemos que, dentre os subprogramas, a forma padr˜ao para a troca de

informa¸c˜oes ´e a passagem de uma lista de parˆametros. Listas de parˆametros

frequentemente se tornam grandes quando programas modulares crescem. Al´em

desta forma para a troca de informa¸c˜oes, o programa em quest˜ao utiliza outro

conceito fornecido pela linguagem Fortran para esse mesmo ﬁm: Os blocos

COMMON, que s˜ao declara¸c˜oes de regi˜oes de mem´oria compartilhadas, acess´ıveis

a qualquer unidade do programa que contenha o bloco COMMON de mesmo nome.

Eles representam uma forma de troca de informa¸c˜oes por meio da partilha de

50

brhs alhs lm idiag id mpbrhs mpalhs mplm mpid mpidiag A (…) dados.

Os blocos COMMON permitem que supbrogramas partilhem os dados por meio

do compartilhamento de uma regi˜ao de mem´oria comum. N˜ao ´e necess´ario que

todos os blocos COMMON de mesmo nome possuam o mesmo n´umero de vari´aveis

ou nomes iguais para as mesmas. Com base na ordena¸c˜ao e nos tipos das

vari´aveis o compilador estabelece as correspondˆencias entre as m´ultiplas vari´aveis

que representam formas alternativas de referenciar uma mesma regi˜ao de mem´oria.

Na implementa¸c˜ao de Hughes (1987), os blocos COMMON s˜ao usados para

a implementa¸c˜ao da estrat´egia da simula¸c˜ao da aloca¸c˜ao dinˆamica mostrada

anteriormente na ﬁgura 3.1. O grande vetor est´atico a e os os apontadores de in´ıcio

dos subvetores s˜ao compartilhados entre as diversas unidades do programa por meio

dos blocos COMMON. A listagem 3.1 mostra o trecho de c´odigo correspondente na

implementa¸c˜ao original. Na linha 6 aparece o vetor A em um bloco COMMON sem

nome (Blank COMMON) e na linha 4 aparece o COMMON /Spoint/, que armazena

em posi¸c˜oes cont´ıguas de mem´oria as vari´aveis com os ´ındices de in´ıcio de cada

subvetor.

Listagem 3.1: Rotina driver, da implementa¸c˜ao original de Hughes (1987) e o

uso de um bloco COMMON importante.

1 SUBROUTINE driver(ntstep,neq,nalhs)

2

3

4

5

6

7

!c.... solution driver program

(...)

COMMON /spoint/ mpd,mpx,mpid,mpf,mpg,mpg1,mpdiag,mpngrp,mpalhs,

mpbrhs

include ’memory_size.inc’

COMMON A(MAX_SIZE)

(...)

8 END SUBROUTINE driver

A partir do padr˜ao Fortran 90, os blocos COMMON deixaram de ser a principal

forma para o compartilhamento de dados entre unidades de programa¸c˜ao, dando

51

lugar a uma forma alternativa, chamada MODULE, sobre a qual voltaremos a falar

mais adiante.

Tendo como ponto de partida a referida implementa¸c˜ao do m´etodo de

elementos ﬁnitos, muitas aplica¸c˜oes cient´ıﬁcas foram desenvolvidas no LNCC. No

ﬁnal da d´ecada de 1980, o programa Axis (Toledo et al., 1988), da ´area de an´alise de

tens˜oes em s´olidos sob rota¸c˜ao, desenvolvido no LNCC em projeto de colabora¸c˜ao

com o COPESP, foi a primeira de tais aplica¸c˜oes a utilizar a separa¸c˜ao em arquivos

com compila¸c˜ao separada. O compartilhamento de dados com os blocos COMMON

continuava a ser explorado, mesmo com m´ultiplos arquivos.

No grupo de pesquisas em reservat´orios petrol´ıferos, seguiu-se o mesmo

caminho e o simuladores do grupo passaram a ser divididos em m´ultiplos arquivos,

principalmente devido ao alto custo de compila¸c˜ao. O simulador deste trabalho

tamb´em est´a dividido dessa forma e, mais adiante, ser˜ao dados mais detalhes sobre

sua organiza¸c˜ao. No ﬁnal da d´ecada de 2000, este simulador passou por um avan¸co

signiﬁcativo: a transi¸c˜ao para o padr˜ao Fortran 90, incluindo a aloca¸c˜ao dinˆamica

de suas estruturas de dados em mem´oria.

Com as estruturas de dados dinˆamicas, surge um problema: a forma de

compartilhamento de dados com blocos COMMON, usada at´e ent˜ao, n˜ao permite o

compartilhamento de vari´aveis alocadas dinamicamente.

Conforme j´a adiantado, a partir do Fortran 90, existe, alternativamente

ao COMMON, um outro elemento de programa¸c˜ao, denominado MODULE. Os

MODULES s˜ao unidades independentemente compil´aveis que, al´em de promover a

modulariza¸c˜ao e o encapsulamento de dados e processos, possuem importante papel

no compartilhamento de dados, permitindo que sejam tamb´em compartilhadas as

vari´aveis alocadas dinamicamente.

Em Fortran, um MODULE ´e uma entidade que cont´em uma s´erie de

deﬁni¸c˜oes e valores iniciais de dados e representa uma forma alternativa para o

compartilhamento de dados entre diferente unidades de programa¸c˜ao (Chapman,

2004). Da mesma forma que ocorre nos blocos COMMON, as diferentes unidades

52

utilizam os mesmos dados e valores presentes em uma ´unica regi˜ao de mem´oria.

A motiva¸c˜ao para a inclus˜ao dos MODULES no simulador, entretanto,

foi a

possibilidade do compartilhamento das novas vari´aveis alocadas dinamicamente,

o que era imposs´ıvel com os blocos COMMON.

As listagens 3.2 e 3.3 mostram trechos de dois m´odulos do simulador,

mFratura e mBloco, os quais, como veremos mais adiante, s˜ao respons´aveis pela

formula¸c˜ao variacional relacionada aos problemas f´ısicos modelados no simulador.

Nos trechos, pode-se notar a existˆencia de vari´aveis para aloca¸c˜ao dinˆamica, com

a anota¸c˜ao ALLOCATABLE, as quais s˜ao compartilhadas com outras unidades de

programa¸c˜ao por fazerem parte de um MODULE.

Listagem 3.2: Uso de MODULES para o Compartilhamento de vari´aveis alocadas

dinamicamente: M´odulo mFratura.

1 MODULE mFratura

2

3

4

5

6

7

(...)

implicit none

(...)

REAL*8, ALLOCATABLE :: solucao_F(:,:), solucaoTmpAnt_F(:,:),

solucaoNaoLinearAnt_F(:,:)

REAL*8, ALLOCATABLE :: f_F(:,:), flux_F(:,:)

(...)

8 END MODULE mFratura

Listagem 3.3: Uso de MODULES para o Compartilhamento de vari´aveis alocadas

dinamicamente: m´odulo mBloco.

1 MODULE mBLoco

2

3

4

5

6

(...)

implicit none

(...)

REAL*8, ALLOCATABLE :: solucao_B(:,:), solucaoTmpAnt_B(:,:),

solucaoNaoLinearAnt_B(:,:)

REAL*8, ALLOCATABLE :: mSolucao_B(:,:,:), mSolucaoTmpAnt_B(:,:,:)

53

7

8

REAL*8, ALLOCATABLE :: f_B(:,:), flux_B(:,:)

(...)

9 END MODULE mBLoco

Nas unidades clientes, que usam os dados e processos encapsulados pelo

m´odulo, deve existir uma declara¸c˜ao USE seguida do nome do m´odulo Fortran.

Dessa forma, elas podem acessar os mesmos dados e valores presentes no

m´odulo. Por isso, diz-se que os m´odulos em Fortran constituem uma forma de

compartilhamento de dados alternativa `a lista de parˆametros de subprogramas.

A listagem 3.4 mostra uma subrotina processador2Escalas, do

programa principal, que faz uso de certas vari´aveis presentes nos m´odulos

mFratura e mBloco mostrados anteriormente. Dentre as vari´aveis, est˜ao algumas

alocadas dinamicamente como mostrado nas listagens 3.2 e 3.3.

Listagem 3.4: Uso de MODULES para o Compartilhamento de vari´aveis alocadas

dinamicamente no simulador: rotina processador2Escalas (cliente).

1 SUBROUTINE processador2Escalas()

2

3

4

5

6

USE mFratura, only : NITER_F, NDOF_F, NLVECT_F, flux_F

USE mFratura, only : DTEMPO_F, solucao_F, f_F, SUM_NUMITER_F;

(...)

USE mBloco,

only : mSolucao_B, mSolucaoTmpAnt_B, NDOF_B,

DTEMPO_B, NITER_B, SUM_NUMITER_B

(...)

7 END SUBROUTINE processador2Escalas

Al´em de

encapsular dados

e promover

seu compartilhamento,

as

unidades MODULE podem ainda conter procedimentos e fun¸c˜oes integralmente

implementadas em seu interior. Esses procedimentos contidos em m´odulos s˜ao

chamados de Module Procedures em Fortran. Vimos na se¸c˜ao 2.1.4.4 a pr´atica

comum de dividir-se os m´odulos-fonte em m´odulo de interface e de implementa¸c˜ao.

Em Fortran, com o uso da unidade MODULE, n˜ao ´e necess´ario que se crie

separadamente os m´odulos de deﬁni¸c˜ao e implementa¸c˜ao. O MODULE engloba

54

simultaneamente os conceitos de m´odulo de implementa¸c˜ao e de deﬁni¸c˜ao.

Os module procedures se diferenciam dos procedimentos e fun¸c˜oes escritos

fora de um MODULE pelo fato de que os primeiros tˆem sua interface sempre

dispon´ıvel para seus clientes. Quando sub-rotinas s˜ao escritas dentro de um

MODULE e outra unidade de compila¸c˜ao faz uso desse MODULE com a declara¸c˜ao

USE, automaticamente a interface de tais sub-rotinas torna-se dispon´ıvel para as

unidades clientes.

Em Chapman (2004) evidencia-se a diferen¸ca entre procedimentos fora

de um MODULE e os module procedures no que diz respeito `a classiﬁca¸c˜ao

de suas interfaces. Um module procedure, acessado pela declara¸c˜ao USE, ´e

dito possuir interface expl´ıcita, uma vez que todos os detalhes sobre seus

parˆametros formais s˜ao explicitamente conhecidos pelo compilador Fortran. Por

outro lado, procedimentos fora de um MODULE possuem interface impl´ıcita,

dado que o compilador Fortran n˜ao possui informa¸c˜oes sobre esses procedimentos

quando est´a compilando qualquer uma das suas unidades clientes.

Nesse

momento, o compilador simplesmente assume que o programador est´a utilizando os

procedimentos da maneira correta, quanto ao n´umero e aos tipos dos argumentos

passados (Chapman, 2004).

Outra vantagem oferecida pelos MODULES est´a relacionada a uma exigˆencia

da pr´opria linguagem: os procedimentos que possuem parˆametros do tipo ponteiro

(POINTER) ou arrays alocados dinamicamente (ALLOCATABLE) precisam,

obrigatoriamente, ter sua interface expl´ıcita e, portanto, vis´ıvel a seus clientes.

Sendo assim, os MODULES facilitam essa tarefa, visto que, por padr˜ao, explicitam

a interface de todos os seus procedimentos internos.

A listagem 3.5 mostra trechos do m´odulo mFratura, onde pode-se ver

a declara¸c˜ao da vari´avel solucao_F e sua aloca¸c˜ao dinˆamica em mem´oria na

linha 5. Pode-se ver ainda a rotina printsol_F. A vari´avel solucao_F ´e

posteriormente utilizada como argumento na chamada desta subrotina, como

veremos mais adiante.

55

Listagem 3.5: M´odulo contendo subrotina com parˆametro alocado dinamicamente.

1 MODULE mFratura

2

3

4

5

6

7

8

9

(...)

REAL*8, ALLOCATABLE :: solucao_F(:,:)

(...)

ALLOCATE(solucao_F (ndof_F, numnp_F));

(...)

SUBROUTINE printsol_F(solucao,X,NUMNP,TEMPO)

!Imprime a solução na fratura

(...)

10

END SUBROUTINE printsol_F

11 END MODULE mFratura

A listagem 3.6 mostra a rotina processador2Escalas, pertencente ao

programa principal. A rotina faz uso da vari´avel solucao_F e a utiliza como

primeiro argumento na chamada da rotina printsol_F. Isso s´o ´e poss´ıvel pois,

como visto na listagem 3.5, printsol_F ´e um module procedure, j´a que faz parte

do MODULE mFratura, e, como tal, tem sua interface vis´ıvel ao seu cliente.

Listagem 3.6: Chamada a uma subrotina com parametro alocado dinamicamente.

1 SUBROUTINE processador2Escalas()

2

3

4

5

6

7

(...)

use mFratura, only : solucao_F

use mFratura, only: printsol_F

(...)

CALL printsol_F(solucao_F, x_F ,NUMNP_F, TEMPO)

(...)

8 END SUBROUTINE

56

3.2

Organiza¸c˜ao do simulador em arquivos para compila¸c˜ao

separada

Como j´a adiantado, a vers˜ao do simulador tomada como base neste trabalho

possui modulariza¸c˜ao em arquivos para compila¸c˜ao separada. Com alguma

simpliﬁca¸c˜ao, podemos enxergar tais arquivos organizados em uma estrutura de

3 n´ıveis que deﬁnem diferentes potenciais de re´uso, como mostrado na ﬁgura 3.2.

Figura 3.2: Organiza¸c˜ao em 3 n´ıveis dos m´ultiplos arquivos de fonte

Exce¸c˜ao ´unica feita ao arquivo-fonte que cont´em a rotina principal

(driver2Escalas.F90), cada arquivo compreende uma unidade MODULE do

Fortran. Tomamos a liberdade de nos referir a “arquivos” e “m´odulos” de forma

indistinta, embora no caso do arquivo/m´odulo principal n˜ao exista de fato uma

unidade MODULE.

No primeiro n´ıvel mostrado na ﬁgura 3.2, com baixo ou nenhum n´ıvel de

re´uso, est´a o m´odulo principal, o qual desempenha apenas papel de cliente dos

demais m´odulos e descreve o ﬂuxo b´asico de execu¸c˜ao do simulador. As rotinas

57

deste m´odulo s˜ao usadas apenas em seu interior. O re´uso de c´odigo deste m´odulo

pode dar-se apenas na forma de adapta¸c˜ao, visto que outras vers˜oes do simulador

de reservat´orios podem ser desenvolvidas adaptando-se este m´odulo. Entretanto,

devido `a sua natureza enquanto um m´odulo principal que guia a execu¸c˜ao da

aplica¸c˜ao, sua reutiliza¸c˜ao ou de seus componentes internos na forma de re´uso

verbatim ´e inexistente.

No segundo n´ıvel, est˜ao os m´odulos que desempenham papel de provedores

de funcionalidades, mas ao mesmo tempo s˜ao clientes de outros m´odulos. Nessa

categoria, enquadram-se os m´odulos referentes aos problemas f´ısicos do simulador

(fratura.F90 e bloco.F90). Tais m´odulos e seus componentes internos s˜ao

reutilizados com alguma frequˆencia dentro do simulador.

No terceiro n´ıvel, encontram-se os m´odulos de base, provedores de

funcionalidades com alto n´ıvel de re´uso e n˜ao relacionadas ao dom´ınio da aplica¸c˜ao.

Isso signiﬁca que tais m´odulos tˆem grande potencial de re´uso verbatim, n˜ao

somente dentro do simulador, mas tamb´em em outras aplica¸c˜oes baseadas na

mesma implementa¸c˜ao do m´etodo de elementos ﬁnitos proposta em Hughes (1987),

caso de v´arios outros simuladores cient´ıﬁcos desenvolvidos no LNCC.

58

Figura 3.3: A organiza¸c˜ao do simulador em arquivos-fonte

A ﬁgura 3.3 mostra os arquivos-fonte divididos nos 3 n´ıveis descritos

anteriormente.

No n´ıvel 1,

est´a o m´odulo do programa principal,

(driver2escalas.F90). Nele est´a deﬁnido o ﬂuxo de execu¸c˜ao do programa

composto por:

(i) leitura de dados de entrada, (ii) processamento e (iii) p´os-

processamento, onde s˜ao apresentados os resultados. Para tanto, este m´odulo

utiliza funcionalidades providas pelos outros dois n´ıveis.

No n´ıvel 2, os m´odulos s˜ao fratura.F90 e bloco.F90, respons´aveis pela

formula¸c˜ao variacional e sua implementa¸c˜ao num´erica para os problemas f´ısicos de

escoamento na fratura hidr´aulica e no bloco da rocha matriz, respectivamente.

No n´ıvel 3, os m´odulos s˜ao: leituraEscrita.F90, respons´avel pela

leitura dos arquivos de entrada com informa¸c˜oes de malha e coordenadas, al´em de

escrita dos resultados; malha.F90, repons´avel pela gera¸c˜ao de coordenadas nodais

e conectividades e busca de vizinhos; funcoesDeForma, que inclui fun¸c˜oes de

interpola¸c˜ao lagrangeanas e informa¸c˜oes para integra¸c˜ao num´erica e, ﬁnalmente, o

59

m´odulo algMatricial.F90, respons´avel pela constru¸c˜ao e solu¸c˜ao dos sistemas

de equa¸c˜oes lineares do m´etodo de elementos ﬁnitos, incluindo a montagem das

estruturas de dados e a implementa¸c˜ao de solver interno.

3.3

Orienta¸c˜ao a Objetos no M´odulo dos Sistemas de Equa¸c˜oes

A contribui¸c˜ao deste trabalho no c´odigo do simulador ´e concentrada no

m´odulo algMatricial.F90 e consiste em uma reformula¸c˜ao deste m´odulo,

que passa a dar lugar a outros menores, incluindo o paradigma de orienta¸c˜ao a

objetos e alguns de seus conceitos como heran¸ca e polimorﬁsmo. Considerando

que o m´odulo em quest˜ao est´a no terceiro n´ıvel mostrado na ﬁgura 3.2 e possui

alto potencial de re´uso, percebe-se que os benef´ıcios decorrentes da reestrutura¸c˜ao

podem se estender a outras aplica¸c˜oes semelhantes dentro do LNCC.

Com essa reestrutura¸c˜ao, objetiva-se conferir ao simulador caracter´ısticas

mais modulares, contribuindo para um melhor entendimento humano e capacidade

de evolu¸c˜ao. Conforme visto no cap´ıtulo 2, c´odigos que implementam conceitos

de modularidade e orienta¸c˜ao a objetos beneﬁciam-se de abstra¸c˜oes por meio

do encapsulamento de dados e opera¸c˜oes, ganhando em usabilidade. Entidades

encapsuladas encorajam e facilitam o uso de aplica¸c˜oes com c´odigo-fonte extenso.

3.3.1

Identiﬁca¸c˜ao das Entidades de Interesse

O m´odulo refatorado compreende dados e opera¸c˜oes relativos `a constru¸c˜ao e

`a solu¸c˜ao dos sistemas de equa¸c˜oes e tamb´em `as estruturas de dados capazes

de tratar a esparsidade das matrizes relacionadas a tais sistemas. O m´odulo inclui

originalmente apenas uma op¸c˜ao de solver interno implementando elimina¸c˜ao de

Gauss.

Muitas aplica¸c˜oes cient´ıﬁcas fazem uso de solvers de bibliotecas de ´algebra

linear, dentre as quais podemos citar para ﬁns de exemplo: LAPACK, PETSC,

umfPACK ou Intel MKL. Alguns simuladores cient´ıﬁcos num´ericos desenvolvidos

no LNCC, em determinado momento de sua evolu¸c˜ao, por for¸ca de iniciativas

60

individuais e de demandas espec´ıﬁcas, passaram a utilizar solvers externos como

o Pardiso (Intel MKL) ou HYPRE (Los Alamos).

Tais iniciativas demandaram que o m´odulo algMatricial.F90 fosse

consideravelmente modiﬁcado, passando a englobar n˜ao apenas novas op¸c˜oes de

solvers, mas tamb´em diferentes estruturas de dados adequando-se aos diferentes

solvers. Al´em de trabalhosos, processos de adapta¸c˜ao desse tipo frequentemente

resultam em altera¸c˜oes feitas de maneira esparsa no c´odigo-fonte, o que impacta

negativamente sua usabilidade e evolu¸c˜ao.

A reestrutura¸c˜ao feita neste trabalho funciona como uma camada de

software com orienta¸c˜ao a objetos, que substitui o m´odulo citado e encapsula em

diferentes classes os dados e opera¸c˜oes relativos aos sistemas de equa¸c˜oes, suas

estruturas de dados e solvers. Os novos m´odulos com classes permitem maior

organiza¸c˜ao do c´odigo e facilitam a troca e a inclus˜ao de novos solvers e estruturas

de dados para sistemas de equa¸c˜oes neste ou em outros simuladores baseados na

mesma implementa¸c˜ao original.

O m´odulo algMatricial.F90, cuja implementa¸c˜ao original simpliﬁcada

´e mostrada na listagem 3.7, re´une vari´aveis e subrotinas cuja an´alise cuidadosa

permite a identiﬁca¸c˜ao de grupos de dados e opera¸c˜oes com certo n´ıvel

de similaridade e que podem ser melhor distribu´ıdos em outras unidades

encapsuladoras para melhor modulariza¸c˜ao. O paradigma de orienta¸c˜ao a objetos

permitir´a a realiza¸c˜ao desta tarefa com naturalidade e trar´a benef´ıcios n˜ao somente

`a organiza¸c˜ao do c´odigo-fonte e `a sua evolu¸c˜ao futura, mas tamb´em facilitar´a a

implementa¸c˜ao de uma estrat´egia de paraleliza¸c˜ao para o c´odigo do simulador

como ser´a visto no cap´ıtulo 4.

Listagem 3.7: O m´odulo mAlgmatricial.

1 MODULE mAlgmatricial

2

3

4

integer

integer

real*8,

:: neq_F, nalhs_F, ned_F

:: neq_B, nalhs_B, ned_B

allocatable :: alhs_F(:), brhs_F(:)

61

5

6

7

8

9

10

11

12

allocatable :: alhs_B(:), brhs_B(:)

real*8,
integer, allocatable :: id_F(:,:), idiag_F(:), lm_F(:,:,:)

integer, allocatable :: id_B(:,:), idiag_B(:), lm_B(:,:,:)

!Subrotinas

public :: back, factor

public :: diag, load, addnsl, addlhs, addrhs

public :: btod, kdbc, ftod, colht

(...)

13 END MODULE mAlgmatricial

Algumas vari´aveis e rotinas mostradas listagem 3.7 est˜ao relacionadas com o

sistema propriamente dito e outras com as estruturas de dados utilizadas. Assim, a

abordagem adotada neste trabalho estabelece a existˆencia de uma estrutura de duas

classes base que representam as entidades de interesse: Sistemas de Equa¸c˜oes

e suas Estruturas de Dados. Solvers podem ser vistos como conjuntos de

opera¸c˜oes que atuam sobre os dados de um sistema. Um solver ser´a, portanto,

entendido como um procedimento que faz parte de todo Sistema de Equa¸c˜oes e

cuja fun¸c˜ao ´e resolvˆe-lo.

O trabalho inclui o provimento de duas op¸c˜oes de solver, sendo um deles

interno, implementando a elimina¸c˜ao de Gauss j´a presente no m´odulo original,

e outro externo (Intel MKL Pardiso), ambos encapsulados pelo arcabou¸co da

Orienta¸c˜ao a Objetos. Os tipos de solvers deﬁnem tipos de sistemas de equa¸c˜oes,

de forma que temos sistemas do tipo Gauss e do tipo Pardiso, uma vez que os

sistemas precisam incluir rotinas e vari´aveis espec´ıﬁcas para lidar com os tipos

espec´ıﬁcos de solver.

Cada um dos solvers requer um tipo de estrutura de dados para tratar a

esparsidade das matrizes de forma espec´ıﬁca. Tais estruturas de dados s˜ao do tipo

Skyline para os sistemas do tipo Gauss, por´em do tipo CRS para sistemas do

tipo Pardiso. A ﬁgura 3.4 mostra um diagrama UML simpliﬁcado da estrutura de

classes adotada, onde se observa o conceito de heran¸ca tanto nos tipos de sistemas

de equa¸c˜oes como nos tipos de estruturas de dados.

62

Figura 3.4: A estrutura b´asica de classes: Sistemas e Estruturas de Dados

Sistemas do tipo Gauss e do tipo Pardiso possuem semelhan¸cas e diferen¸cas.

Os pontos em comum est˜ao encapsulados na classe m˜ae SistemaEquacoes.

Por isso as classes SistemaGauss e SistemaPardiso s˜ao ligadas `a primeira

por uma seta que indica o relacionamento de heran¸ca. As diferen¸cas s˜ao

implementadas nas classes ﬁlhas. Suas estruturas de dados, EstruturaSkyline

e EstruturaCRS, respectivamente, tamb´em possuem pontos em comum e pontos

de diferen¸ca, herdando os pontos em comum da classe m˜ae EstruturaDados e

sendo as diferen¸cas implementadas nas classes ﬁlhas.

3.3.2

Introduzindo os Atributos e M´etodos das Classes

A an´alise das vari´aveis e rotinas do m´odulo algMatricial.F90, bem como

das semelhan¸cas entre sistemas do tipo Gauss e Pardiso, permite observar que todos

sistemas em sua forma matricial A.x = B possuem como atributos em comum: uma

matriz A (vari´avel ALHS), o vetor B (vari´avel BRHS), al´em uma estrutura de dados,

que poder´a ser do tipo Skyline ou CRS.

Quanto `as opera¸c˜oes, ou m´etodos da classes, podemos observar que todos os

sistemas possuem em comum uma opera¸c˜ao de solver, representada pelo m´etodo

solver, al´em de outras opera¸c˜oes adicionais do m´etodo de elementos ﬁnitos,

relacionadas, por exemplo, `a montagem das matrizes globais, como ´e o caso de

addlhs e addrhs, ou `a coloca¸c˜ao de condi¸c˜oes de contorno, como load e ftod.

A ﬁgura 3.5 mostra o diagrama em UML das classes de sistemas de equa¸c˜oes com

63

EstruturaDados{abstract}EstruturaSkylineEstruturaCRSSistemaEquacoes{abstract}SistemaGaussSistemaPardisoum n´ıvel maiores detalhes.

Figura 3.5: A estrutura de classes de Sistemas de Equacoes com maiores detalhes.

O entendimento de todas as rotinas do c´odigo ´e considerado irrelevante nesse

momento em que estamos interessados em explorar a reestrutura¸c˜ao do m´odulo

original com conceitos de orienta¸c˜ao a objetos como heran¸ca, polimorﬁsmo e

encapsulamento. Dessa forma, concentrando-nos na explora¸c˜ao de tais conceitos,

chamamos aten¸c˜ao para dois m´etodos em espec´ıﬁco: solver e addlhs.

Todos os sistemas possuem um m´etodo solver capaz de resolvˆe-lo.

Entretanto,

temos tipos diferentes de sistemas com diferentes solvers que,

naturalmente, devem se comportar de maneira diferente. Este cen´ario evoca

automaticamente o conceito de polimorﬁsmo, visto no cap´ıtulo 2. Algo

semelhante ocorre com o m´etodo addlhs, respons´avel por adicionar a contribui¸c˜ao

64

SistemaEquacoes{abstract}+ alhs(:) : real*8, allocatable+ brhs(:) : real*8, allocatable+ estDados : class(EstruturaDados), pointer+ solverM {abstract}+ addlhsM {abstract}+ addrhsM+ ftodM+ loadM+ kdbcM+ btodM+ colocarCondContornoSistemaGauss+ construtorSistemaGauss+ solverM+ addlhsM+ backM+ factorMSistemaPardiso+ pt(64) : integer+ iparm(64) : integer+ dparm(64) : real*8+ construtorSistemaPardiso+ solverM+ addlhsMdas matrizes de elemento na matriz global do m´etodo de elementos ﬁnitos.

No caso das classes de estruturas de dados, ocorre um cen´ario similar e este

´e mostrado no diagrama da ﬁgura 3.6. Podemos identiﬁcar que tanto estruturas de

dados do tipo Skyline quanto as do tipo CRS possuem atributos em comum, como

o trio de vetores lm, id e idiag, que aparecem portanto na classe m˜ae. Toda

estrutura de dados tamb´em possui o m´etodo montarEstruturaDados, embora ele

se comporte de maneira diversa nas classes ﬁlhas. Isso signiﬁca que tamb´em ´e um

m´etodo onde aparece o conceito de polimorﬁsmo.

Figura 3.6: A estrutura de classes de Estruturas de Dados com maiores detalhes.

Nas se¸c˜oes seguintes, abordaremos a implementa¸c˜ao pr´atica, no c´odigo do

simulador, dos conceitos discutidos at´e aqui. Ser˜ao apresentados trechos de c´odigo

em linguagem Fortran mostrando como foram implementadas as classes e os

conceitos de heran¸ca e polimorﬁsmo discutidos at´e este ponto.

65

EstruturaDados{abstract}+ lm(:,:,:) : integer*4, allocatable + id(:,:) : integer*4, allocatable+ idiag(:) : integer*4, allocatable+ ai(:) : integer*4, allocatable+ neq : integer+ nalhs : integer+ ned : integer+ blocoOuFratura : character + montarEstruturaDados {abstract}EstruturaSkyline+ montarEstruturaDados+ colhtM+ diagMEstruturaCRS+ lmStencilEq(:,:) : integer*4, allocatable+ posPonteiro : integer*4+ contPonteiro : integer*4+ nonzeros : integer*4+ montarEstruturaDados+ criarPonteirosMatEsparsa3.3.3

Implementa¸c˜ao de Orienta¸c˜ao a Objetos em Fortran: Heran¸ca

De acordo com Chapman (2004), na linguagem Fortran, utilizada neste

trabalho, a orienta¸c˜ao a objetos ´e implementada com o uso de MODULES e de

tipos de dados derivados, deﬁnidos com a palavra TYPE, os quais podem

conter opera¸c˜oes em seu interior e s˜ao implementados integralmente dentro de uma

unidade MODULE. Na listagem 3.8, vemos como a classe m˜ae SistemaEquacoes

foi implementada. A classe ´e representada pelo tipo derivado SistemaEquacoes,

deﬁnido na linha 3.

Listagem 3.8: Implementa¸c˜ao de uma classe em Fortran.

1 MODULE mSistemaEquacoes

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

(...)

TYPE, public, abstract :: SistemaEquacoes

real*8, public, allocatable :: alhs(:), brhs(:)
class(EstruturaDados), pointer :: estDados

CONTAINS

!Métodos da classe SistemaEquacoes:

procedure(solverM_interface), public, deferred :: solverM

procedure(addlhsM_interface), public, deferred :: addlhsM

procedure, public :: addrhsM

procedure, public :: ftodM

(...)

END TYPE SistemaEquacoes

(...)

CONTAINS

!Implementação dos métodos da classe:

SUBROUTINE addrhsM (this, p_elresf, p_nel, p_nee, p_ndof, p_nen)

(...)

END SUBROUTINE

(...)

21 END MODULE

Ainda na listagem 3.8, vemos que, ap´os a palavra CONTAINS da linha 6, os

66

m´etodos da classe s˜ao listados como procedimentos do tipo (type bound procedures)

e, em seguida, s˜ao implementados, ap´os a palava CONTAINS da linha 15, ainda

dentro do m´odulo mSistemaEquacoes.

A listagem 3.9 mostra como a classe ﬁlha SistemaPardiso foi

implementada. A deﬁni¸c˜ao ´e similar ao que foi feito para a classe m˜ae. Chamamos

aten¸c˜ao para a anota¸c˜ao extends(SistemaEquacoes) feita na deﬁni¸c˜ao do

tipo derivado na linha 3.

Isso indica que a classe SistemaPardiso ´e ﬁlha

da classe SistemaEquacoes e, portanto, herda seus atributos e m´etodos,

funcionando como uma especializa¸c˜ao.

Listagem 3.9: Implementa¸c˜ao de uma classe herdeira em Fortran.

1 MODULE mSistemaPardiso

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

(...)

TYPE, public, extends(SistemaEquacoes) :: SistemaPardiso

!Atributos do Sistema Pardiso:

INTEGER pt(64), iparm(64)

REAL*8 dparm(64)
CONTAINS

!Métodos do Sistema Pardiso

procedure, public ::

solverM

procedure, public ::

addlhsM

procedure, public ::

construtorSistemaPardiso

END TYPE SistemaPardiso

CONTAINS

!Implementação dos métodos da classe:

SUBROUTINE solverM(this, p_solucao, p_label)

(...)

END SUBROUTINE solverM

(...)

19 END MODULE

67

3.3.4

Implementa¸c˜ao do Polimorﬁsmo em Fortran:

Se repararmos no diagrama UML da ﬁgura 3.5, veremos que, na classe

SistemaEquacoes, ao lado dos m´etodos solver e addlhs, existe a anota¸c˜ao

abstract.

Isso quer dizer que estes s˜ao dois m´etodos abstratos, ou seja, sem

implementa¸c˜ao deﬁnida na classe m˜ae, onde existe apenas a declara¸c˜ao de

sua interface. A implementa¸c˜ao completa dos m´etodos ´e feita nas classes

ﬁlhas (sistemaGauss e sistemaPardiso) que especializam a classe m˜ae e

implementam tais m´etodos de diferentes formas. A listagem 3.10 mostra a deﬁni¸c˜ao

de tais interfaces na classe SistemaEquacoes.

Listagem 3.10: Deﬁni¸c˜ao das interfaces dos m´etodos abstratos na classe

SistemaEquacoes.

1 MODULE mSistemaEquacoes

2

3

4

5

6

7

8

9

10

11

12

(...)

ABSTRACT INTERFACE

SUBROUTINE solverM_interface(this, p_solucao, p_label)

import :: SistemaEquacoes

class (SistemaEquacoes) :: this

character(len=*) :: p_label
REAL*8, ALLOCATABLE :: p_solucao(:,:)

END SOUBROUTINE solverM_interface

(...)

END INTERFACE

(...)

13 END MODULE mSistemaEquacoes

A seguir vemos na listagem 3.11 a implementa¸c˜ao do m´etodo solver

na subclasse SistemaGauss. Vemos que o comportamento do m´etodo nesta

subclasse ´e chamar os m´etodos factor e back nas linhas 7 e 8, respons´aveis

pela fatora¸c˜ao e substitui¸c˜ao retrocedida, etapas j´a existentes na implementa¸c˜ao

original que possu´ıa apenas o solver baseado em elimina¸c˜ao de Gauss. Veremos a

seguir que o comportamento do m´etodo ´e diferente na outra subclasse.

68

Listagem 3.11: Polimorﬁsmo: A implementa¸c˜ao do m´etodo solver na classe

SistemaGauss.

1 MODULE mSistemaGauss

2

3

4

5

6

7

8

9

10

11

12

(...)

SOUBROUTINE solverM(this, p_solucao, p_label)

implicit none

class(sistemaGauss) :: this

(...)

CALL this%factorM()

CALL this%backM()

(...)

CALL this%btodM(p_solucao,ndof,numnp)

END SOUBROUTINE solverM

(...)

13 END MODULE mSistemaGauss

Como adiantado, observa-se na listagem 3.12 o comportamento polim´orﬁco

do m´etodo solver, cuja implementa¸c˜ao interna ´e completamente diferente na

subclasse SistemaPardiso, chamando a rotina solverPardisoPPD_Nodal,

que realiza opera¸c˜oes espec´ıﬁcas do solver Pardiso.

Listagem 3.12: Polimorﬁsmo: A implementa¸c˜ao do m´etodo solver na classe

SistemaPardiso.

1 MODULE msistemaPardiso

2

3

4

5

6

7

8

9

(...)

SUBROUTINE solverM(this, p_solucao, p_label)

implicit none

class(sistemaPardiso) :: this

(...)

CALL solverPardisoPPD_Nodal(this, simetria, p_label, etapa);

(...)

CALL this%btodM(p_solucao,ndof,numnp)

10

END SUBROUTINE solverM

69

11

(...)

12 END MODULE msistemaPardiso

3.3.5

A nova organiza¸c˜ao do simulador com os novos m´odulos

contendo classes

Como visto ao longo da se¸c˜ao 3.3, o m´odulo original algMatricial.F90

deu lugar a outros seis m´odulos com classes, mostrados do n´ıvel 3 da ﬁgura 3.7.

Nesta se¸c˜ao, abordamos a forma como os seis m´odulos integram-se ao restante do

simulador e como s˜ao feitas as instˆancias dos objetos das classes citadas nas se¸c˜oes

anteriores em c´odigo.

Figura 3.7: Organiza¸c˜ao do simulador com os novos m´odulos fonte.

Lembramos que, como visto na se¸c˜ao 3.2, os m´odulos fratura.F90 e

bloco.F90 englobam aspectos relacionados aos problemas f´ısicos de escoamento

na fratura hidr´aulica e no bloco matriz. Por essa raz˜ao, decidiu-se que os objetos

das classes de sistemas de equa¸c˜oes e de estruturas de dados devem existir dentro

70

desses m´odulos. Faz sentido pensar que fratura e bloco “possuem”sistemas de

equa¸c˜oes e estruturas de dados para tais sistemas. Haveria, portanto, uma instˆancia

de sistemaEquacoes para a fratura hidr´aulica e outra para o bloco. O mesmo

sendo v´alido para as estruturas de dados.

As listagens 3.13 e 3.14 mostram trechos dos m´odulos fratura.F90 e

bloco.F90 onde s˜ao declaradas vari´aveis que na verdade s˜ao ponteiros, os quais,

no momento apropriado, far˜ao referˆencia `as regi˜oes de mem´oria em que ser˜ao

alocados os objetos das classes criadas. ´E interessante observar que os ponteiros

s˜ao para objetos do tipos SistemaEquacoes e EstruturaDados, que s˜ao as

superclasses mostradas nas se¸c˜oes anteriores. Ponteiros para um classe m˜ae podem

apontar para objetos de suas classes ﬁlhas e isso ser´a feito em momento oportuno

e de acordo com a escolha do usu´ario.

Listagem 3.13: Objetos das classes SistemaEquacoes e EstruturaDados no

m´odulo mFratura

1 MODULE mFratura

2

3

4

5

(...)

class(SistemaEquacoes), pointer :: umSistEqFratura

class(estruturaDados), pointer :: umaEstDadosFratura

(...)

6 END MODULE mFratura

Listagem 3.14: Objetos das classes SistemaEquacoes e EstruturaDados no

m´odulo mBloco

1 MODULE mBloco

2

3

4

5

(...)

class(SistemaEquacoes), pointer :: umSistEqBloco

class(estruturaDados), pointer :: umaEstDadosBloco

(...)

6 END MODULE mBloco

71

No caso do bloco, ´e interessante lembrar que os dados referentes ao sistema

seriam sobrescritos a cada passo de um processo iterativo que resolve m´ultiplos

problemas unidimensionais de elementos ﬁnitos, como descrito no cap´ıtulo 1 e

mostrado na ﬁgura 1.3. No caso da fratura, existe apenas uma malha e, portanto,

o sistema n˜ao ´e sobrescrito em momento algum.

No programa principal, reservoirSimulator,

localizado no arquivo

driver2Escalas.F90, ´e feita a decis˜ao sobre os tipos de sistema e de estruturas

de dados a serem utilizados. A seguir ser˜ao mostrados alguns trechos deste arquivo

fonte e destacados alguns pontos relevantes.

Primeiramente destacamos que o programa principal faz uso dos ponteiros

presentes em fratura.F90 e bloco.F90, mostrados anteriormente nas

listagens 3.13 e 3.14. Isso pode ser visto na listagem 3.15, nas declara¸c˜oes USE

das linhas 4, 5, 7 e 8.

Listagem 3.15: Programa principal fazendo uso dos ponteiros declarados nos

m´odulos fratura.F90 e bloco.F90

1 PROGRAM reservoirSimulator

2

3

4

5

6

7

8

9

(...)

!Sistemas de Equações presentes em mFratura e mBloco:

USE mFratura,

only : umSistEqFratura

USE mBloco,

only : umSistEqBloco,

!Estruturas de dados presentes em mFratura e mBloco:

USE mFratura,

only : umaEstDadosFratura

USE mBloco,

only : umaEstDadosBloco

(...)

10 END PROGRAM reservoirSimulator

Em seguida, ´e feita a decis˜ao sobre os tipos de sistemas estruturas de dados

utilizados. Isso ´e feito por meio das diretivas de compila¸c˜ao encontradas nas linhas

4 e 12 da listagem 3.16. Se a ﬂag withPardiso n˜ao for deﬁnida devemos ter

sistemas do tipo Gauss e estruturas Skyline. Caso a ﬂag esteja deﬁnida, isso

signiﬁca que a op¸c˜ao do usu´ario ´e pelo solver Pardiso e, ent˜ao, devemos ter sistemas

72

desse tipo e estruturas do tipo CRS.

O conceito utilizado aqui ´e o de Typed Allocation, aloca¸c˜ao tipada. Assim,

podemos alocar os objetos das classes SistemaEquacoes e EstruturaDados

j´a deﬁnindo neste momento que desejamos alocar objetos de uma subclasse

espec´ıﬁca. Isso ´e feito para sistemas do tipo Gauss e estruturas Skyline nas linhas

6, 7, 9 e 10. Para os sistemas do tipo Pardiso e estruturas CRS isso ´e feito nas

linhas 14, 15, 17 e 18.

Listagem 3.16: Aloca¸c˜ao dos objetos das classes de sistemas de equa¸c˜oes e

estruturas de dados.

1 PROGRAM reservoirSimulator

2

3

(...)

implicit none

4 #ifndef withPardiso !SOLVER GAUSS:

5

6

7

8

9

10

!Fratura:

ALLOCATE (

sistemaGauss :: umSistEqFratura)

ALLOCATE (estruturaSkyline :: umaEstDadosFratura)

!Bloco:

ALLOCATE (

sistemaGauss :: umSistEqBloco)

ALLOCATE (estruturaSkyline :: umaEstDadosBloco)

11 #endif

12 #ifdef withPardiso !SOLVER PARDISO:

13

14

15

16

17

18

!Fratura:

ALLOCATE (sistemaPardiso :: umSistEqFratura)

ALLOCATE ( estruturaCRS :: umaEstDadosFratura)

!Bloco:

ALLOCATE (sistemaPardiso :: umSistEqBloco)

ALLOCATE (estruturaCRS::umaEstDadosBloco)

19 #endif

20

21

22

23

!Ligando os ponteiros:

umSistEqFratura%estDados => umaEstDadosFratura

umSistEqBloco%estDados

=> umaEstDadosBloco

(...)

73

24

25

26

CALL preprocessadorFratura()

CALL preprocessadorBloco()

(...)

27 END PROGRAM reservoirSimulator

Chamamos aten¸c˜ao para as opera¸c˜oes feitas nas linhas 21 e 22, tamb´em na

listagem 3.16, depois da aloca¸c˜ao dos objetos, onde estabelecemos a liga¸c˜ao entre o

atributo estDados dos objetos das classes de sistema com as estruturas de dados

rec´em alocadas. ´E conveniente atentar para o diagrama da ﬁgura 3.5, onde vemos

que todo sistema tem um atributo que ´e a sua estrutura de dados.

Ap´os esta etapa da cria¸c˜ao dos sistemas, aparecem, ainda na listagem

3.16, nas linhas 24 e 25, chamadas `as rotinas preprocessadorFratura

preprocessadorBloco, repons´aveis por m´ultiplas tarefas de inicializa¸c˜ao

referentes aos modelos do bloco e da fratura. A listagem 3.17 mostra a rotina

preprocessadorFratura. A rotina preprocessadorBloco ´e similar e faz

opera¸c˜oes an´alogas para o caso do bloco.

Listagem 3.17: A rotina preprocessadorFratura.

1 MODULE mFratura

2

3

4

5

6

7

8

9

10

11

12

13

(...)

SUBROUTINE preprocessadorFratura()

(...)

ALLOCATE(umSistEqFratura%estDados%id(ndof_F,numnp_F))

umSistEqFratura%estDados%id = 0

call leituraCodigosCondContorno(umSistEqFratura%estDados%id,

ndof_F,numnp_F,n,iin,iecho,iprtin)

umSistEqFratura%estDados%NEQ = n

ALLOCATE(umsistEqFratura%estDados%idiag(umSistEqFratura%estDados%

neq))

umSistEqFratura%estDados%idiag=0

ALLOCATE(umSistEqFratura%estDados%lm(ndof_F,nen_F,numel_F))

(...)

SELECT TYPE (umSistEqFratura)

74

14

15

16

17

18

19

20

21

22

type is (sistemaGauss)

optSolver_F=’Gauss’

call umSistEqFratura%construtorSistemaGauss

type is (sistemaPardiso)

optSolver_F=’PardisoEsparso’

call umSistEqFratura%construtorSistemaPardiso(nsd_F,nen_F,

numConexoesPorElem)

END SELECT

(...)

END SUBROUTINE preprocessadorFratura()

23 END MODULE mFratura

Nas linhas 5, 9 e 11 vemos, respectivamente a aloca¸c˜ao dinˆamica dos vetores

id, idiag e lm, pertencentes `as estruturas de dados para armazenamento de

matrizes esparsas e mostrados no diagrama da ﬁgura 3.6. ´E interessante lembrar

que estes eram alguns dos vetores da ﬁgura 3.1, os quais, na implementa¸c˜ao original

de Hughes (1987), estavam contidos dentro do grande vetor est´atico A.

Dentre as tarefas realizadas pelas rotinas preprocessadorFratura

e

preprocessadorBloco,

destacamos,

ainda,

as

chamadas

a

duas

novas

rotinas

importantes:

construtorSistemaGauss

ou

construtorSistemaPardiso, de acordo com o tipo de sistemas escolhido.

As chamadas a tais rotinas, as quais s˜ao m´etodos das subclasses de sistemas,

podem ser vistas nas linhas 16 e 19 da listagem 3.17. Estes m´etodos cumprem

aproximadamente a fun¸c˜ao de m´etodos construtores, j´a que s˜ao respons´aveis por

algumas tarefas de inicializa¸c˜ao dos sistemas que devem ser feitas imediatamente

ap´os a aloca¸c˜ao de um objeto.

A importˆancia de tais m´etodos est´a no fato de que neles ´e feita a aloca¸c˜ao

dinˆamica das matrizes e do vetor carga dos sistemas de equa¸c˜oes. Dessa forma,

assim que um novo sistema ´e alocado, o m´etodo construtor correspondente ´e

chamado e faz a aloca¸c˜ao dinˆamica da matriz ALHS e do vetor BRHS, pertencentes

aos sistemas de equa¸c˜oes. As listagens 3.18 e 3.19 mostram as rotinas em quest˜ao.

Destacamos a aloca¸c˜ao das vari´aveis ALHS e BRHS.

75

Listagem 3.18: O m´etodo construtorSistemaGauss.

1 MODULE mSistemaGauss

2

3

4

5

6

7

8

(...)

SUBROUTINE construtorSistemaGauss(this)

implicit none

class(sistemaGauss) :: this

ALLOCATE(this%ALHS(this%estDados%nalhs))

ALLOCATE(this%BRHS(this%estDados%NEQ))

END SUBROUTINE construtorSistemaGauss

9 END MODULE mSistemaGauss

Listagem 3.19: O m´etodo construtorSistemaPardiso.

1 MODULE mSistemaPardiso

2

3

4

5

6

7

8

9

10

11

12

13

(...)

SUBROUTINE construtorSistemaPardiso(this, p_nsd_F, p_nen_F,

p_numConexoesPorElem)

use mMalha, only: numConexoesPorElem

implicit none

class(sistemaPardiso) :: this

integer, intent(in) :: p_nsd_F, p_nen_F

integer :: p_numConexoesPorElem

ALLOCATE(this%ALHS(this%estDados%nalhs))

ALLOCATE(this%BRHS(this%estDados%NEQ))

p_numConexoesPorElem=p_nen_F

END SUBROUTINE construtorSistemaPardiso

(...)

14 END MODULE mSistemaPardiso

Finalmente, ao ﬁm do programa principal, todos os sistemas e estrtuturas de

dados s˜ao desalocados, como mostra a listagem 3.20.

76

Listagem 3.20: Desaloca¸c˜ao dos sistemas e estruturas de dados

1 PROGRAM reservoirSimulator

2

3

4

5

6

(...)

DEALLOCATE(umSistEqFratura)

DEALLOCATE(umaEstDadosFratura)

DEALLOCATE(umaEstDadosBloco)

DEALLOCATE(umSistEqBloco)

7 END PROGRAM reservoirSimulator

No pr´oximo cap´ıtulo, veremos que a reestrutura¸c˜ao com orienta¸c˜ao a objetos

desenvolvida nesse trabalho facilitou a implementa¸c˜ao de uma estrat´egia de

paraleliza¸c˜ao para o simulador. O desenvolvimento da solu¸c˜ao paralela produziu

uma altera¸c˜ao na aloca¸c˜ao do sistema de equa¸c˜oes do bloco, o qual deixa de ser

´unico e passa a dar lugar a v´arios sistemas alocados dinamicamente convivendo

simultaneamente em mem´oria, deixando de ser sobrescrito a cada itera¸c˜ao do

la¸co que resolve os m´ultiplos problemas relativos ao bloco. Tal abordagem ser´a

detalhada no cap´ıtulo 4.

77

Cap´ıtulo 4

Paraleliza¸c˜ao do simulador 2 escalas de

Shale Gas

Neste cap´ıtulo, abordamos o desenvolvimento de estrat´egias de paraleliza¸c˜ao

com os padr˜oes OpenMP e MPI para o c´odigo do simulador de escoamentos em

reservat´orios de g´as em folhelhos.

Inicialmente ´e feita uma an´alise do perﬁl de

desempenho serial da aplica¸c˜ao com uso de ferramenta especializada. Em seguida,

s˜ao apresentadas evolu¸c˜oes no c´odigo necess´arias `a implementa¸c˜ao do paralelismo.

Por ﬁm s˜ao apresentados testes de desempenho e criticados os seus resultados.

4.1

An´alise inicial do perﬁl serial de desempenho da aplica¸c˜ao com

intel VTUNE

O primeiro passo para a formula¸c˜ao das estrat´egias de paraleliza¸c˜ao propostas

neste trabalho envolveu uma fase de an´alise de desempenho para a identiﬁca¸c˜ao,

no c´odigo do simulador, das regi˜oes que consomem mais tempo de execu¸c˜ao. A

avalia¸c˜ao correta do perﬁl de desempenho do c´odigo suporta o desenvolvimento

de estrat´egias de paraleliza¸c˜ao, na medida em que os esfor¸cos de otimiza¸c˜ao

s˜ao direcionados `as regi˜oes mais demandantes do c´odigo, onde um aumento no

desempenho ter´a o maior impacto no desempenho geral da aplica¸c˜ao.

Para a obten¸c˜ao de um perﬁl de desempenho do c´odigo, foi utilizada a

ferramenta de perﬁlamento de c´odigo Intel VTune Ampliﬁer XE, dispon´ıvel no

cluster Altix-xe do LNCC. De acordo com Jeﬀers e Reinders (2013), o VTune

78

´e uma ferramenta capaz de fornecer diversas m´etricas a respeito da execu¸c˜ao de

um programa como, por exemplo, uso de mem´oria e de cache, al´em de ajudar

na identiﬁca¸c˜ao das regi˜oes com maior custo de execu¸c˜ao, conhecidas como de

hotspots.

O arcabou¸co de orienta¸c˜ao a objetos desenvolvido neste trabalho e

incorporado ao simulador permite a utiliza¸c˜ao de dois diferentes solvers: o solver

interno, implementando elimina¸c˜ao de Gauss, al´em de um solver externo, intel

Pardiso, da biblioteca intel MKL.

Neste momento inicial, estamos interessados apenas em analisar o perﬁl de

desempenho da aplica¸c˜ao e confrontar dois solvers neste simulador. Para tanto,

foram realizadas an´alises com Intel VTune, com execu¸c˜oes seriais do c´odigo do

simulador, com ambos os referidos solvers.

Nesta an´alise inicial comparativa de solvers, utilizamos, no caso da fratura,

uma malha unidimensional de 400 elementos. No bloco, cada malha possui 135

elementos e o tempo de simula¸c˜ao total foi de 20 meses. Apesar de tratar-se de

um primeiro caso de teste experimental e ainda sem pretens˜oes de interpreta¸c˜ao de

resultados f´ısicos, os tamanhos de malhas utilizados nesta etapa s˜ao compat´ıveis

aos adotados em simula¸c˜oes semelhantes como em Costa (2015).

O tipo de an´alise feito com a ferramenta de perﬁlamento, denominado “Basic

Hotspot Analysis” ´e, segundo Jeﬀers e Reinders (2013), o mais recomendado para

uma vis˜ao geral do perﬁl de desempenho, com identiﬁca¸c˜ao das subrotinas mais

custosas em tempo de execu¸c˜ao.

As ﬁguras 4.1 e 4.2 mostram, na aba “Caller/Callee”, a lista de todas as

rotinas do c´odigo do simulador, ordenadas pelo seu tempo de execu¸c˜ao total,

considerando a soma de todas as vezes em que foram chamadas e executadas.

Os n´umeros s˜ao percentuais em rela¸c˜ao ao tempo total de execu¸c˜ao da aplica¸c˜ao.

79

Figura 4.1: Percentual do tempo total de execu¸c˜ao por rotina com solver Gauss

Na ﬁgura 4.1 ´e mostrada a execu¸c˜ao com o solver interno (Elimina¸c˜ao

de Gauss), ao passo que na ﬁgura 4.2 a execu¸c˜ao foi

feita com o solver

exerno, intel Pardiso. Em ambos os casos, nota-se uma semelhan¸ca: a rotina

processadorBloco, cuja linha est´a em destaque nas ﬁguras, corresponde a um

grande percentual do tempo de execu¸c˜ao total da aplica¸c˜ao (99,5% e 93,3%).

Figura 4.2: Percentual do tempo total de execu¸c˜ao por rotina com solver intel
Pardiso

A rotina em destaque, processadorBloco, ´e respons´avel, em linhas

gerais, por resolver um problema unidimensional dentre os v´arios relativos a um

bloco, como exposto no cap´ıtulo 1. Tal rotina, cuja implementa¸c˜ao, antes de

qualquer interven¸c˜ao para paraleliza¸c˜ao ´e mostrada na listagem 4.1, ´e respons´avel

por: (i) montar o problema matricial relacionado a um sistema de equa¸c˜oes lineares

com a chamada `a rotina montarSistema_B e (ii) resolver o sistema com a

chamada ao m´etodo polimorfo solver, visto no cap´ıtulo 3, se¸c˜ao 3.3.

80

Listagem 4.1:

A rotina processadorBloco do m´odulo principal,

driver2Escalas.F90

1 SUBROUTINE processadorBloco(...)

2

3

4

5

6

7

8

9

10

11

(...)

USE mBloco, only : umSistEqBloco

USE mBloco, only : umaEstDadosBloco

(...)

umSistEqBloco%ALHS=0.d0

umSistEqBloco%BRHS=0.d0

(...)

CALL montarSistema_B(umSistEqBloco, DT, solucaoTmpAnt_B_aux,

solucao_B_aux)

CALL umSistEqBloco%solverM(solucao_B_aux, "Bloco")

(...)

12 END SUBROUTINE processadorBloco

Como visto no cap´ıtulo 1, se¸c˜ao 1.2, para cada ponto da malha de elementos

ﬁnitos do problema da fratura, existe um problema relativo ao bloco. Dessa forma,

a rotina processadorBloco ´e executada tantas vezes quantos forem os pontos

na malha da fratura. O grande n´umero de chamadas a essa rotina, junto a seu

custo por chamada, fazem com que ela corresponda a um alt´ıssimo percentual do

tempo total de execu¸c˜ao da aplica¸c˜ao, como visto nas ﬁguras 4.1 e 4.2. O fato de

que cada chamada a tal rotina resolve um sistema de equa¸c˜oes independente nos

permite a considerar sua execu¸c˜ao em paralelo. Esta ´e a ideia b´asica da estrat´egia

de paralelismo desenvolvida e que ser´a melhor detalhada em se¸c˜oes seguintes.

Voltemo-nos agora `as diferen¸cas entre os dois solvers. A experiˆencia anterior

de colegas dentro do LNCC em implementa¸c˜ao de simuladores similares ao deste

trabalho mostra que, especiﬁcamente para o caso de problemas unidimensionais

de elementos ﬁnitos, a utiliza¸c˜ao do solver baseado em elimina¸c˜ao de Gauss ´e

mais vantajosa em tempo de execu¸c˜ao do que a utiliza¸c˜ao de alguns outros solvers

externos, dentre eles o Pardiso.

A ﬁm de conﬁrmar tal hip´otese neste simulador em espec´ıﬁco, apresentamos

dados, tamb´em obtidos na mesma an´alise com a ferramenta Intel VTune Ampliﬁer

81

XE, capazes de clariﬁcar a diferen¸ca pr´atica entre os dois solvers dispon´ıveis,

especiﬁcamente nesta aplica¸c˜ao, que lida unicamente com malhas unidimensionais,

de acordo com o modelo apresentado na se¸c˜ao 1.2.

Na ﬁgura 4.3, pode-se observar a aba “Summary” do VTune que mostra

o tempo total de execu¸c˜ao da aplica¸c˜ao. Com a utiliza¸c˜ao do solver interno

(elimina¸c˜ao de Gauss), o tempo total de execu¸c˜ao do simulador para o caso de

teste foi de pouco mais de 30 segundos.

Figura 4.3: Tempo total de execu¸c˜ao do caso experimental com solver Gauss

Com o uso do solver Pardiso, entretanto, o tempo total se aproximou de 1

minuto e 24 segundos segundos, o que refor¸ca a hip´otese de que a utiliza¸c˜ao do

solver Gauss, para problemas com malhas unidimensionais, ou seja, aqueles que

estamos interessados em resolver neste momento com a vers˜ao atual do simulador,

´e mais vantajosa em termos de dempo de execu¸c˜ao.

Figura 4.4: Tempo total de execu¸c˜ao do caso experimental com solver intel Pardiso

Examinando os resultados das ﬁguras 4.1 e 4.2 um pouco mais a fundo, nos

concentraremos na rotina processadorBloco, que aparece em destaque em nas

ﬁguras e ´e respons´avel por alt´ıssimo percentual do tempo de execu¸c˜ao da aplica¸c˜ao.

As ﬁguras 4.5 e 4.6 s˜ao detalhamentos das ﬁguras 4.1 e 4.2, respectivamente.

Analisam especiﬁcamente a rotina processadorBloco, mostrando quais s˜ao

suas rotinas “ﬁlhas”, ou seja, chamadas em seu interior, desmembrando o tempo

82

de execu¸c˜ao gasto em cada uma delas. Nas duas ﬁguras, destacamos a linha da

rotina solver, pois estamos interessados em confrontar os dois solvers.

Na ﬁgura 4.5 obervam-se dados da execu¸c˜ao com o solver interno (Gauss).

Nota-se que, dos 29,85 segundos gastos na rotina processadorBloco, apenas

3,571 segundos foram gastos na rotina do solver propriamente dito, na linha

destacada na ﬁgura.

Figura 4.5: An´alise da rotina processadorBloco com solver Gauss

Em contraste, na ﬁgura 4.6, onde s˜ao mostrados resultados da execu¸c˜ao

com solver Pardiso, percebe-se que, do total de 82,012 segundos gastos na rotina

processadorBloco, mais de 50 segundos foram gastos na rotina referente ao

solver, sendo esta, neste caso, mais impactante do que a montagem do sistema.

Figura 4.6: An´alise da rotina processadorBloco com solver Pardiso

Esta ´ultima an´alise, portanto, conﬁrma a hip´otese de a execu¸c˜ao tornou-se

mais lenta com o solver Pardiso, especiﬁcamente em raz˜ao do custo de execu¸c˜ao

rotina do solver propriamente dito, e n˜ao por qualquer outro tipo de processamento

a ele relacionado. Dessa forma, consideramos o solver interno mais adequado ao

modelo f´ısico e matem´atico atual utilizado por este simulador. O provimento de

um solver externo, entretanto, n˜ao deixa de ser justiﬁc´avel e de representar um

avan¸co para o simulador.

83

Embora atualmente o modelo desenvolvido em Costa (2015) e utilizado no

simulador fa¸ca uso apenas de malhas unidimensionais, qualquer itera¸c˜ao futura que

contemple a utiliza¸c˜ao de malhas bidimensionais ou tridimensionais poder´a fazer

com que o usu´ario deste simulador necessite lan¸car m˜ao de um solver externo mais

adequado a tais situa¸c˜oes. Seja este o intel Pardiso, j´a disponibilizado, ou mesmo

um outro que decida implementar, o que seria facilitado pelo uso do arcabou¸co de

orienta¸c˜ao a objetos desenvolvido neste trabalho. Implementar uma nova classe de

sistemas de equa¸c˜oes e integr´a-la ao simulador ´e uma tarefa mais convidativa do

que integrar um novo solver ao antigo m´odulo mAlgMatricial, visto no cap´ıtulo

3.

4.2

A estrat´egia de paraleliza¸c˜ao com OpenMP

Neste momento, ´e conveniente retomar uma caracter´ıstica do modelo f´ısico

e matem´atico proposto em Costa (2015), adotado no simulador. Conforme visto

no cap´ıtulo 1 e na ﬁgura 1.3, o modelo para a simula¸c˜ao de escoamento do g´as no

reservat´orio n˜ao convencional (composto por bloco e fratura induzida) ´e baseado

em m´ultiplos problemas unidimensionais de elementos ﬁnitos referentes ao bloco,

os quais est˜ao relacionados aos pontos da malha unidimensional do problema na

fratura, fornecendo termo de fonte de massa a este ´ultimo.

O modelo confere ao c´odigo a caracter´ıstica de ser naturalmente paraleliz´avel,

visto que as solu¸c˜oes dos m´ultiplos problemas do bloco, originalmente feitas

de forma serial, n˜ao apesentam qualquer tipo de dependˆencia entre si. Esta

caracter´ıstica ´e conhecida, em inglˆes, como embarassing parallelism, o que sugere

ser constrangedor que n˜ao se explore oportunidade t˜ao convidativa `a execu¸c˜ao

paralela de tais atividades. A estrat´egia de paralelismo desenvolvida neste trabalho

explora, portanto, a solu¸c˜ao em paralelo de m´ultiplos problemas unidimensionais

de elementos ﬁnitos referentes ao bloco.

N˜ao ´e proposta aqui a paraleliza¸c˜ao de um solver, mas sim o desenvolvimento

de uma estrat´egia para tornar paralela a execu¸c˜ao de um modelo, o qual inclui

84

a solu¸c˜ao de m´ultiplos problemas independentes para o bloco. Trata-se de uma

estrat´egia de paraleliza¸c˜ao em um n´ıvel mais alto de abstra¸c˜ao e com granularidade

grossa, no sentido de que cada uma das unidades de execu¸c˜ao do OpenMP, as

threads, resolvem problemas completos.

Considerando que o m´odulo refatorado neste trabalho visa disponibilizar

solvers diferentes e,

inclusive,

facilitar a inclus˜ao de outros, ele confere ao

c´odigo certo grau de heterogeneidade no seu ﬂuxo de execu¸c˜ao. A estrat´egia de

paraleliza¸c˜ao desenvolvida ´e adequada a essa caracter´ıstica, na medida em que

permite que o simulador se beneﬁcie do paralelismo e tenha escalabilidade de

execu¸c˜ao independente de qual seja o solver em uso.

A ideia ´e que cada thread do OpenMP resolva um subconjunto do n´umero

total de problemas unidimensionais do bloco.

Cada um destes problemas

corresponde basicamente a uma chamada `a rotina processadorBloco, a qual,

como visto na se¸c˜ao 4.1, monta o problema matricial relacionado a um sistema e

o resolve.

No c´odigo do simulador,

em sua vers˜ao serial,

existe uma rotina

respons´avel por

resolver, um a um,

todos os problemas relacionados ao

bloco: resolverProbVariosBlocos, mostrada na listagem 4.2. Na linha

6, ´e poss´ıvel observar o loop que resolve cada problema, um ap´os outro,

chamando a rotina processadorBloco v´arias vezes, al´em de armazenar,

em cada itera¸c˜ao, a contribui¸c˜ao de um problema em uma posi¸c˜ao do vetor

fluxoMassicoDeBlocoParaFratura a ser utilizado no problema da fratura.

Listagem 4.2: Rotina serial que resolve os m´ultiplos problemas do Bloco.

1 SUBROUTINE resolverProbVariosBlocos(...)

2

3

4

5

6

(...)

INTEGER :: numBlocos

REAL*8 :: fluxoMassicoDeBlocoParaFratura(numBlocos), fluxoMolM2S
INTEGER :: iBlocos

DO iBlocos=1, numBlocos

85

7

8

9

CALL processadorBloco(...)

fluxoMassicoDeBlocoParaFratura(iBlocos) = fluxoMolM2S;

END DO

! fim do loop do problema micro (blocos)

10 END SUBROUTINE resolverProbVariosBlocos

´E nesta rotina, resolverProbVariosBlocos, que ser˜ao inseridas as

diretivas de compila¸c˜ao do padr˜ao OpenMP, para a pareliza¸c˜ao do loop que que

resolve os v´arios sistemas de equa¸c˜oes. Entretanto, ´e muito importante observar

que, na vers˜ao serial do c´odigo, o objeto da classe SistemaEqua¸c˜oes possui seus

dados sobrescritos a cada itera¸c˜ao do loop mostrado na linha 6 da listagem 4.2.

A listagem 4.1 mostra que a rotina processadorBloco, a cada vez que ´e

executada, reconstr´oi as matrizes dos sistemas que resolve. As linhas 6 e 7 mostram

as vari´aveis ALHS e BRHS sendo reinicializadas com valor zero pela rotina.

Neste momento percebe-se uma grande utilidade para os novos m´odulos

baseados no paradigma orientado a objetos.

´E preciso que tenhamos v´arios

sistemas coexistindo simultaneamente em mem´oria para que a estrat´egia de

paraleliza¸c˜ao funcione e cada thread OpenMP seja capaz de operar em um sistema

diferente. Com a classe SistemaEquacoes,

isso pode ser alcan¸cado com

naturalidade, uma vez que podemos instanciar m´ultiplos objetos da classe. Um

novo objeto dever´a ser criado a cada chamada da rotina processadorBloco.

Uma reformula¸c˜ao da rotina processadorBloco foi feita para que os

objetos da classe SistemaEquacoes referentes ao bloco fossem criados em seu

interior. A listagem 4.3 mostra parte da nova vers˜ao da rotina. Na linha 6, vemos

o ponteiro umSistEqBloco para a classe m˜ae SistemaEquacoes, enquanto

nas linhas 8 ou 11 o objeto propriamente dito ´e criado (alocado em mem´oria) e

relacionado ao ponteiro da linha 6. Na linha 14, o atributo do estDados do

sistema ´e ralacionado `a estrutura de dados presente no m´odulo mBloco.

Listagem 4.3: Reformula¸c˜ao da rotina processadorBloco

1 SUBROUTINE processadorBloco(...)

86

2

3

4

5

6

(...)

use mBloco, only : umaEstDadosBloco

(...)

!Ponteiro para sistema:

class(SistemaEquacoes),

pointer

:: umSistEqBloco

7 #ifndef withPardiso

8

ALLOCATE (sistemaGauss :: umSistEqBloco)

9 #endif

10 #ifdef withPardiso

11

ALLOCATE(sistemaPardiso::umSistEqBloco)

12 #endif

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

!Ligando os ponteiros:

umSistEqBloco%estDados => umaEstDadosBloco

(...)

SELECT TYPE (umSistEqBloco)

type is (sistemaGauss)

optSolver_B_aux=’Gauss’

call umSistEqBloco%construtorSistemaGauss

type is (sistemaPardiso)

optSolver_B_aux=’PardisoEsparso’

call umSistEqBloco%construtorSistemaPardiso(nsd_B,nen_B,

numConexoesPorElem_aux)

END SELECT

(...)

umSistEqBloco%ALHS=0.d0

umSistEqBloco%BRHS=0.d0

(...)

CALL montarSistema_B(umSistEqBloco, DT, solucaoTmpAnt_B_aux,

solucao_B_aux)

CALL umSistEqBloco%solverM(solucao_B_aux, "Bloco")

(...)

DEALLOCATE(umSistEqBloco)

32 END SUBROUTINE processadorBloco

87

O processo descrito anteriormente ´e exatamente o mesmo que antes era feito

no m´odulo principal driver2Escalas.F90 mas, naquela situa¸c˜ao, utilizando

o ponteiro umSistEqBloco presente no m´odulo mBloco. O ponteiro apontava

para o objeto do tipo sistema de equa¸c˜oes alocado no programa principal. Isso foi

mostrado em detalhes nos trechos de c´odigo exibidos cap´ıtulo 3, se¸c˜ao 3.3.5. Na

nova vers˜ao, tanto o ponteiro para sistema como a aloca¸c˜ao do objeto em mem´oria

est˜ao dentro da rotina processadorBloco.

Chamamos aten¸c˜ao para o fato de que, ap´os a aloca¸c˜ao do sistema,

ainda dentro da rotina processadorBloco, aparece agora a chamada aos

m´etodos construtorSistemaGauss ou construtorSistemaPardiso, os

quais, conforme discutido no cap´ıtulo 3, se¸c˜ao 3.3.5, fazem a aloca¸c˜ao dinˆamica

das matrizes dos sistemas de equa¸c˜oes.

Finalmente, ao ﬁnal da rotina

processadorBloco, na linha 31, o sistema ´e desalocado.

A compara¸c˜ao entre as listagens 4.1 e 4.3 permite, portanto, a identiﬁca¸c˜ao

de evolu¸c˜oes feitas na rotina processadorBloco que ser˜ao necess´arias ao

desenvolvimento da estrat´egia de paraleliza¸c˜ao desenvolvida. Resumidamente,

duas diferen¸cas mais importantes s˜ao:

(i) O ponteiro para objetos da classe SistemaEquacoes referentes ao bloco

deixa de estar no m´odulo mBloco e, portanto, deixa de ser ´unico, passando a

ser uma vari´avel de fun¸c˜ao, interna `a rotina processadorBloco, residindo

na pilha (stack ). A cada vez que ´e chamada, essa essa rotina cria um novo

ponteiro para sistema de equa¸c˜oes.

(ii) A aloca¸c˜ao dinˆamica dos objetos a serem apontados por tais ponteiros

tamb´em ´e feita no interior da rotina.

Isso signiﬁca que, a cada chamada

`a mesma, um novo objeto ´e alocado e referenciado pelo ponteiro local.

De (i) e (ii) conslui-se que, caso a rotina processadorBloco seja chamada

v´arias vezes em paralelo, teremos m´ultiplos ponteiros para sistemas e m´ultiplos

objetos da classe SistemaEquacoes coexistindo em mem´oria e apontados por

88

tais ponteiros.

Isso nos pertimitir´a a paralelizar o loop que resolve os v´arios

problemas do bloco.

As altera¸c˜oes reﬂetiram-se no programa principal, anteriormente mostrado

na listagem3.15, onde havia quatro decla¸c˜oes USE para ponteiros nos m´odulos

mFratura e mBloco (linhas 4, 5, 7 e 8). A listagem 4.4 mostra altera¸c˜oes

relevantes no programa principal. Das referidas quatro declara¸c˜oes USE, restaram

apenas trˆes, sendo duas delas para ponteiros no m´odulo mFratura (linhas 4 e

5), referentes ao sistema e `a estrutura de dados da fratura; para o caso do bloco

restou apenas uma, na linha 7, fazendo uso do ponteiro para estrutura de dados

do m´odulo mBloco.

Listagem 4.4: Altera¸c˜oes no programa principal.

1 PROGRAM reservoirSimulator

2

3

4

5

6

7

8

(...)

!Sistema de Equações e Estrutura de dados em mFratura:

use mFratura,

only : umSistEqFratura

use mFratura,

only : umaEstDadosFratura

!Apenas Estrutura de dados presente em mBloco:

use mBloco,

only : umaEstDadosBloco

(...)

9 END PROGRAM reservoirSimulator

Como visto, para o caso da fratura, n˜ao houve altera¸c˜ao, uma vez que o

paralelismo tratar´a unicamente dos problemas do bloco. Continuaremos tendo

apenas um sistema de equa¸c˜oes para a fratura, al´em de uma estrutura de dados.

No caso do bloco, teremos m´ultiplos sistemas, por´em apenas uma ´unica estrutura

de dados para todos eles. Esta ser´a apontada por um ´unico ponteiro em mBloco.

Lembramos que estruturas de dados tratam de deﬁnir a forma espec´ıﬁca como

os dados referentes `as matrizes dos sistemas ser˜ao armazenados, considerando

sua esparsidade; por´em, as estruturas n˜ao contˆem as matrizes dos sistemas

propriamente ditas, as quais s˜ao atributos da classe sistemaEquacoes.

89

As altera¸c˜oes descritas at´e

este ponto, nos permitem, ﬁnalmente,

aplicar

as

diretivas

de

compila¸c˜ao

do

padr˜ao OpenMP `a

rotina

resolverProbVariosBlocos, cuja vers˜ao serial foi apresentada na listagem

4.2. No trecho de c´odigo mostrado na listagem 4.5, pode-se ver uma nova vers˜ao

da rotina resolverProbVariosBlocos com as diretivas OpenMP para a

paraleliza¸c˜ao do loop da linha 9 que resolve os v´arios problemas do bloco.

Listagem 4.5: Vers˜ao da rotina resolverProbVariosBlocos paralelizada com

OpenMP

1 SUBROUTINE resolverProbVariosBlocos(...)

2

3

4

5

6

7

8

9

10

11

12

13

implicit none

INTEGER :: numBlocos, NUSTEP

REAL*8 :: fluxoMassicoDeBlocoParaFratura(numBlocos), fluxoMolM2S

, DT

INTEGER :: iBlocos

integer :: omp_get_thread_num

!Loop paralelizado com OpenMP:

!$omp parallel do firstprivate(NUSTEP, TEMPO, fluxoMolM2S,

condContorno, DT)

DO iBlocos=1, numBlocos

CALL processadorBloco(...)

fluxoMassicoDeBlocoParaFratura(iBlocos) = fluxoMolM2S;

END DO

! fim do loop do problema micro (blocos)

!$omp end parallel do

14 END SUBROUTINE resolverProbVariosBlocos

Nas linhas 8 e 13 ´e poss´ıvel observar as diretivas $omp parallel do e

$omp end parallel do utilizadas para a divis˜ao das itera¸c˜oes do loop entre

as threads. Tamb´em na linha 8, observa-se a cl´ausula firstprivate seguida de

uma s´erie de vari´aveis.

Isto foi necess´ario para corrigir e erros de execu¸c˜ao em

decorrˆencia de condi¸c˜oes de corrida causadas pelo paralelismo. Alguns recursos

(vari´aveis) eram compartilhados pelas threads quando n˜ao deveriam sˆe-lo. As

escritas simultˆaneas causavam erros nos resultados obtidos com a vers˜ao paralela.

90

Os defeitos puderam ser corrigidos com a utiliza¸c˜ao de uma ferrmenta

especialida em thread debugging. Trata-se do Intel Inspector, dispon´ıvel nos

recursos computacionais do LNCC. A ferramenta ´e capaz de apontar vari´aveis que

est˜ao causando condi¸c˜ao de corrida. Sua utiliza¸c˜ao guiou a remo¸c˜ao de erros nesta

aplica¸c˜ao e a tela de uma das an´alises feitas com ferramenta ´e mostrada na ﬁgura

4.7, a t´ıtulo de ilustra¸c˜ao. De posse das informa¸c˜oes obtidas com a ferramenta, foi

poss´ıvel utilizar com assertividade a cl´ausula firstprivate, que faz com que as

vari´aveis listadas possuam c´opias privadas em cada thread, evitando as condi¸c˜oes

de corrida.

Figura 4.7: An´alise de condi¸c˜oes de corrida feita com a ferramente Intel Inspector.

Em linhas gerais, a ideia da solu¸c˜ao paralela ´e que cada thread do OpenMP

instancie um sistema de equa¸c˜oes para um dos problemas do bloco, o resolva e

o desaloque ao ﬁnal de sua solu¸c˜ao. O processo segue se repetindo at´e que os

problemas se esgotem, uma vez que o n´umero de problemas ´e, invariavelmente,

ordens de grandeza maior do que o n´umero de threads. As threads podem

instanciar novos sistemas de equa¸c˜oes simplesmente chamando a nova vers˜ao da

rotina processadorBloco, como feito na linha 10.

´E importante lembrar neste momento, que a rotina processadorBloco

n˜ao corresponde apenas `a solu¸c˜ao dos sistemas lineares propriamente ditos. Inclui,

al´em do solver, a chamada `a rotina montarSistema_B, como visto nas ﬁguras

4.5 e 4.6. Isso signiﬁca que a estrat´egia de paraleliza¸c˜ao engloba tanto a montagem

das matrizes dos sistemas quanto a solu¸c˜ao dos mesmos e nos permite ter ganhos

nas duas frentes, o que n˜ao ocorreria com a paraleliza¸c˜ao de um solver.

91

A organiza¸c˜ao ﬁnal dos ponteiros e objetos de sistemas e estruturas ap´os

a implementa¸c˜ao da estrat´egia de paraleliza¸c˜ao descrita funcionasse corretamente

tem a forma como ilustrado na ﬁgura 4.8.

Figura 4.8: A organiza¸c˜ao de objetos de sistemas e estruturas de dados na vers˜ao
paralelizada com OpenMP

S˜ao mostrados na ﬁgura 4.8: (i) o m´odulo mFratura, com dois ponteiros

para dois objetos referentes `a fratura: um sistema e uma estrutura de dados, (ii)

o m´odulo mBloco, com apenas um ponteiro para uma estrutura de dados a ser

utilizada em todos os problemas do bloco e (iii) uma s´erie de ponteiros na pilha

(stack ) referentes `as chamadas da fun¸c˜ao processadorBloco que ocorrem em

paralelo.

4.3

Testes de desempenho I: Paraleliza¸c˜ao com OpenMP

Nesta se¸c˜ao ser˜ao abordados testes de desempenho realizados com a aplica¸c˜ao

paralela. Nas simula¸c˜oes realizadas foram adotados dados f´ısicos realistas, baseados

92

Heap Estrutura Dados Fratura Sistema Fratura Módulo mFratura.F90 POINTER umaEstDadosFratura  POINTER umSistEqFratura (…)  Sistema Bloco #N Estrutura Dados Bloco Módulo mBloco.F90 POINTER umaEstDadosBloco Stack umSistEqBloco umSistEqBloco umSistEqBloco      (…) umSistEqBloco em um campo real. A tabela 4.1, adaptada de (Costa, 2015), mostra os parˆametros

f´ısicos utilizados nas simula¸c˜oes realizadas.

Tabela 4.1: Parˆametros f´ısicos relativos ao campo realista utilizado nos testes.
Adaptado de [Costa (2015)]

Permeabilidade (K∗)(m2/nD)
Porosidade na matriz (φP )
Press˜ao inicial no reserv. (M P a/psi)
Press˜ao no po¸co (M P a/psi)
Temperatura no reserv. (K/F )
Satura¸c˜ao do g´as
PL(M P a/psi)
VL(scf /ton)/((kg/m3)

1 × 10−19/100
0.06
26.2/3800
6.89/1000
355/180
70%
4.48/350
96/5.18

Foram adotadas nos experimentos duas conﬁgura¸c˜oes de tamanhos de malhas

para fratura e bloco:

• Experimento (A): Malha de 400 elementos para a fratura e 135 elementos

para o bloco, algo compat´ıvel com os tamanhos utilizados em Costa (2015),

ambas com reﬁnamento em sua por¸c˜ao inicial. No caso da fratura, na

regi˜ao mais pr´oxima ao po¸co e, no caso do bloco, na regi˜ao mais pr´oxima

`a fratura.

• Experimento (B): Neste experimento a malhas do bloco s˜ao idˆenticas ao

anterior. A ﬁm de aumentar o n´umero de sistemas de equa¸c˜oes referentes

ao bloco para observar o comportamento da vers˜ao paralelizada e comparar

resultados f´ısicos e de desempenho computacional, foi adotada para a

fratura uma malha maior, desta vez uniforme. Apesar de tratar-se de

uma malha uniforme, o espa¸camento entre os pontos da malha existente

no experimento (A) para a regi˜ao reﬁnada foi mantido, o que nos faz chegar

ao n´umero de 2200 elementos para uma malha uniforme a ser adotada na

fratura. Este experimento nos permitir´a avaliar se a solu¸c˜ao paralela ´e

sens´ıvel ao reﬁnamento das malhas.

Para ambas as conﬁgura¸c˜oes de tamanhos de malha,

foram realizadas

93

simula¸c˜oes com tempo total de 30 anos. No experimento (A), com 400 elementos

na malha da fratura foi obtida a seguinte sa´ıda:

--> Pressão no reservatório:
--> Pressão no poço:
--> Quantidade de gás total:
--> Quantidade de gás recuperável:
--> Quantidade de gás produzido:
--> Quantidade de gás produzido:
--> Valor da pressão no último nó:
--> Tempo de simulação
--> Fator de recuperação (sobre gás total):
--> Fator de recuperação (sobre gás recuperável):

2.62000E+07(Pa)
6.89000E+06(Pa)
6.43831E+03(kg)
4.26160E+03(kg)
3.53262E+03(kg)
2.35508E+06(kg/m^2)
6.91931E+06(Pa)
3.00000E+01(anos)

resultado para todo reservatorio

--> Quantidade de gás total:
--> Quantidade de gás recuperável:
--> Quantidade de gás produzido:

2.71087E+01(MMscf)
1.79436E+01(MMscf)
1.48742E+01(MMscf)

5.48688E-01
8.28942E-01

Para o experimento (B), com 2200 elementos na malha uniforme da fratura,

foi obtida a sa´ıda a seguir, sem diferen¸cas relevantes nos resultados, como esperado:

--> Pressão no reservatório:
--> Pressão no poço:
--> Quantidade de gás total:
--> Quantidade de gás recuperável:
--> Quantidade de gás produzido:
--> Quantidade de gás produzido:
--> Valor da pressão no último nó:
--> Tempo de simulação
--> Fator de recuperação (sobre gás total):
--> Fator de recuperação (sobre gás recuperável):

2.62000E+07(Pa)
6.89000E+06(Pa)
6.43831E+03(kg)
4.26160E+03(kg)
3.53270E+03(kg)
2.35514E+06(kg/m^2)
6.91931E+06(Pa)
3.00000E+01(anos)

resultado para todo reservatorio

--> Quantidade de gás total:
--> Quantidade de gás recuperável:
--> Quantidade de gás produzido:

2.71087E+01(MMscf)
1.79436E+01(MMscf)
1.48745E+01(MMscf)

5.48701E-01
8.28961E-01

A valida¸c˜ao dos resultados obtidos tamb´em foi feita por compara¸c˜ao com as

sa´ıdas da vers˜ao do simulador utilizada em Costa (2015). As ﬁguras 4.9 e 4.10

mostram gr´aﬁcos relativos ao exeperimento (A) com 400 elementos na malha da

fratura. Tais gr´aﬁcos n˜ao apresentam diferen¸cas vis´ıveis em rela¸c˜ao aos obtidos

para o experimento (B), dada a semelhan¸ca dos resultados. Na ﬁgura 4.9, vemos

os perﬁs de press˜ao ao longo dos 30,5 metros de dimens˜ao da fratura em 7 tempos

espec´ıﬁcos: 1 mˆes, 6 meses, 1, 5, 10, 20 e 30 anos.

94

Figura 4.9: Perﬁs de press˜ao ao longo da fratura (30,5 metros) para: 1 mˆes, 6
meses, 1, 5, 10, 20 e 30 anos.

A ﬁgura 4.10 mostra a produ¸c˜ao acumulada em Kg para o reservat´orio.

Figura 4.10: Produ¸c˜ao acumulada em Kg

4.3.1

Ambiente de Execu¸c˜ao e Metodologia dos testes

Os testes da aplica¸c˜ao paralelizada com OpenMP foram realizados e uma

m´aquina com duas CPUs multi-core com as seguintes especiﬁca¸c˜oes:

• 2 Processadores Quad-Core Intel(R) Xeon(R) CPU X5550 @ 2.67GHz

(Total de 8 cores)

95

051015202530356.877.27.47.67.888.28.48.68.8x 106Pressão na Fratura (MPa) x (metros)05101520253001000200030004000Tempo(anos)Producao(Kg)• 12GB de mem´oria RAM DDR3
• Sistema Operacional Ubuntu 12.04.5 LTS

O compilador utilizado foi o gfortran vers˜ao 4.6.3 e todos os experimentos

paralelos para a tomada de tempo foram repetidos 6 vezes para cada conﬁgura¸c˜ao

de n´umero de unidades de execu¸c˜ao, ou threads. Ao ﬁm de cada experimento,

foram consideradas as m´edias das 6 execu¸c˜oes de cada caso: 1, 2, 4, 6 e 8 threads.

Nas se¸c˜oes seguintes apresentamos os resultados das tomadas de tempo para a

aplica¸c˜ao paralelizada com OpenMP.

4.3.2

Resultados do experimento (A) com OpenMP

Nesta se¸c˜ao apresentamos os resultados das tomadas de tempo com a vers˜ao

paralelizada com OpenMP. A tabela 4.2 mostra os tempos totais de execu¸c˜ao da

aplica¸c˜ao (em minutos) para 1, 3, 4, 6 e 8 threads. Para cada n´umero de threads a

aplica¸c˜ao foi executada 6 vezes. Na linha em destaque na tabela s˜ao apresentadas

as m´edias dos tempos de execu¸c˜ao. A ´ultima linha apresenta o speedup obtido em

cada caso onde:

SpeedU p =

T empo serial
T empo paralelo

Tabela 4.2: Tempos de execu¸c˜ao (em minutos) da vers˜ao com OpenMP para o
experimento (A).

Execu¸c˜ao #1
Execu¸c˜ao #2
Execu¸c˜ao #3
Execu¸c˜ao #4
Execu¸c˜ao #5
Execu¸c˜ao #6
M´edia (minutos)
SpeedUp Obtido

1 Thread 2 Threads
4.48
4.48
4.48
4.5
4.58
4.57
4.52
1x

2.55
2.53
2.53
2.52
2.65
2.53
2.55
1.77x

4 Threads
1.37
1.37
1.37
1.37
1.37
1.4
1.37
3.29x

6 Threads
0.98
0.95
0.92
0.92
0.93
0.92
0.94
4.82x

8 Threads
0.75
0.72
0.73
0.75
0.72
0.72
0.73
6.18x

Na ﬁgura 4.11 ´e mostrado o gr´aﬁco dos speedups obtidos com OpenMP (em

verde) em compara¸c˜ao ao caso ideal, speedup linear (em vermelho).

96

Figura 4.11: Speedups da vers˜ao com OpenMP para o experimento (A).

4.3.3

Resultados do experimento (B) com OpenMP

Os testes realizados para os experimento (A) foram repetidos para o

experimento (B), com malha uniforme de 2200 elementos na fratura. A seguir s˜ao

exibidos: a tabela de tempos obtidos e o gr´aﬁco de speedups, os mesmos exibidos

anteriormente para o experimento (A).

Tabela 4.3: Tempos de execu¸c˜ao (em minutos) da vers˜ao com OpenMP para o
experimento (B).

Execu¸c˜ao #1
Execu¸c˜ao #2
Execu¸c˜ao #3
Execu¸c˜ao #4
Execu¸c˜ao #5
Execu¸c˜ao #6
M´edia (minutos)
SpeedUp Obtido

1 Thread 2 Threads
25.47
24.93
24.40
24.97
24.95
25.52
25.04
1x

14.33
14.52
14.32
14.32
14.33
14.30
14.35
1.74x

4 Threads
7.22
7.35
7.32
7.23
7.18
7.20
7.25
3.45x

6 Threads
5.27
5.30
5.53
5.27
5.25
5.32
5.32
4.70x

8 Threads
4.12
4.37
3.95
4.33
4.15
4.07
4.16
6.01x

97

1.00 2.00 4.00 6.00 8.00 4.82 6.18 0.001.002.003.004.005.006.007.008.009.0002468Speedup Número de Threads Speedup LinearSpeedup Obtido: OpenMPFigura 4.12: Speedups da vers˜ao com OpenMP para o experimento (B).

4.3.4

Discuss˜ao dos resultados e otimiza¸c˜ao com OpenMP

Ao ﬁm dos testes anteriores, foi poss´ıvel observar que a paraleliza¸c˜ao com

OpenMP apresentou signiﬁcativos ganhos de desempenho, atingindo, para 8

threads, speedups de 6,18x para o experimento (A) e de 6,01x para o experimento

(B).

Speedups s˜ao m´etricas afetadas pelo balanceamento de carga, ou seja,

pela forma como ´e feita a distribui¸c˜ao da carga de processamento entre as

diferentes threads ou unidades de execu¸c˜ao. Se o balanceamento ´e insuﬁciente,

algumas threads terminam seu trabalho muito antes das demais e precisam esperar

ociosamente pelas outras, quando poderiam estar realizando parte do trabalho

extra que est´a destinado `as demais. Isso impacta negativamente o speedup obtido.

Sabe-se que, em nosso modelo, os problemas relativos ao bloco em sua parte

inferior (mais pr´oximos ao po¸co) s˜ao problemas mais demorados que os localizados

na parte superior. Se isso for um fato, signiﬁcaria que as threads respons´aveis pelos

problemas superiores terminariam seu trabalho primeiro e ter´ıamos uma situa¸c˜ao

desbalanceamento, ainda n˜ao mensurada, que poderia estar afetando o speedup

obtido.

A ﬁm de conﬁrmar a hip´otese de que os problemas inferiores s˜ao mais

98

1.00 2.00 4.00 6.00 8.00 4.70 6.01 0.001.002.003.004.005.006.007.008.009.0002468Speedup Número de Threads Speedup LinearSpeedup Obtido: OpenMPcustosos, avaliamos, separadamente, os tempos totais de execu¸c˜ao gastos em cada

um dos problemas do bloco, para que o desbalanceamento de carga pudesse

ser visualizado em termos pr´aticos. As ﬁgura 4.13 e 4.14 mostram, para os

experimentos (A) e (B), os tempos de execu¸c˜ao, acumulados em todos os passos de

tempo da simula¸c˜ao, de cada um dos problemas do bloco, sendo os valores mais `a

esquerda aqueles relativos aos problemas superiores (afastados do po¸co) e os mais

`a direita, relativos aos problemas mais pr´oximos do po¸co.

Figura 4.13: Tempos de execu¸c˜ao por cada problema do bloco para o experimento
(A).

Figura 4.14: Tempos de execu¸c˜ao por cada problema do bloco para o experimento
(B).

Ambos os resultados s˜ao parecidos. As ﬁguras 4.13 e 4.14. Mostram uma

oscila¸c˜ao consider´avel dos custos dos problemas do bloco ao longo de toda a

externs˜ao da fratura, por´em ambos os casos apresentam uma linha de tendˆencia

com uma inclina¸c˜ao a qual, embora pequena, ´e capaz de demonstrar que, de fato,

99

y = 4E-05x + 0.6386 05010015020025030035040000.10.20.30.40.50.60.70.80.9Tempo de execução (s)Identificador do problema (400 = Problema mais próximo ao poço)y = 6E-06x + 0.6678 0200400600800100012001400160018002000220000.10.20.30.40.50.60.70.80.9Tempo de execução (s)Identificador do problema (2200 = Problema mais próximo ao poço)os problemas mais pr´oximos ao po¸co tendem a ser mais lentos, gerando algum

desbalanceamento de carga.

O padr˜ao OpenMP oferece recursos para o melhor balanceamento da carga

entre as threads. A cl´ausula schedule, quando usada na paraleliza¸c˜ao de um loop,

pode deﬁnir, de diversas formas, como as itera¸c˜oes do loop ser˜ao divididas entre as

threads para promover melhor balanceamento de carga. A forma padr˜ao utilizada

com a omiss˜ao da cl´ausula schedule ´e chamada de escalonamento est´atico e

divide todo o espa¸co de itera¸c˜oes do loop em blocos de itera¸c˜oes de tamanho

aproximadamente igual ao quociente entre o n´umero total de itera¸c˜oes do loop

e o n´umero de threads. Tais blocos de itera¸c˜oes, ou chunks, s˜ao ent˜ao designados

para as threads, ainda em tempo de compila¸c˜ao.

Os resultados expostos at´e aqui utilizam o escalonamento est´atico. Os testes

das se¸c˜oes 4.3.2 e 4.3.3 foram repetidos com a cl´ausula schedule deﬁnindo

uma nova maneira de dividir o espa¸co de itera¸c˜oes entre as threads capaz de

diminuir os efeitos do desbalanceamento: o escalonamento dinˆamico. No loop

paralelizado em nosso simulador, mostrado na listagem 4.5,

foi adicionada a

cl´ausula schedule(dynamic).

Com a utiliza¸c˜ao do escalonamento dinˆamico, recomendado para situa¸c˜oes

onde o custo das itera¸c˜oes do loop n˜ao ´e constante, todas as itera¸c˜oes s˜ao

organizadas em uma esp´ecie de ﬁla de trabalho interna, por meio da qual s˜ao

distribu´ıdas para as threads `a medida em que estas terminam seu trabalho. Dessa

forma, evita-se que tenhamos threads ociosas por muito tempo.

A ﬁguras 4.15 e 4.16 mostram os novos speedups obtidos para o experimento

(A) e para o experimento (B), por´em utilizando a cl´ausula schedule(dynamic).

100

Figura 4.15: Speedups com a cl´ausula schedule(dynamic) para o experimento
(A).

Figura 4.16: Speedups com a cl´ausula schedule(dynamic) para o experimento
(B).

Com esses

resultados,

´e

poss´ıvel

perceber

que

a

cl´ausula

schedule(dynamic) elevou consideravelmente os speedups obtidos, apesar de

as inclina¸c˜oes da linhas de tendˆencia das ﬁguras 4.13 e 4.14 serem pequenas.

Neste momento ´e interessante saber o que ocorreria em situa¸c˜oes onde as

inclina¸c˜oes s˜ao mais pronunciadas. Para o mesmo n´umero de tarefas, mantendo

a malha da fratura com 400 elementos, foram aumentadas as malhas do bloco de

135 para 600 elementos. A ﬁgura 4.17 mostra os resultados das tomadas de tempo

101

1.00 2.00 4.00 6.00 8.00 4.82 6.18 1.00 5.21 6.80 0.001.002.003.004.005.006.007.008.009.0002468Speedup Número de Threads Speedup LinearSpeedup Obtido: Schedule STATICSpeedup Obtido: Schedule DYNAMIC1.00 2.00 4.00 6.00 8.00 4.70 6.01 1.00 5.19 6.87 0.001.002.003.004.005.006.007.008.009.0002468Speedup Número de Threads Speedup LinearSpeedup Obtido: Schedule STATICSpeedup Obtido: Schedule DYNAMICsimilares `as da ﬁgura 4.13, por´em com 600 elementos nas malhas do bloco.

Figura 4.17: Tempos de execu¸c˜ao por cada problema do bloco (Fratura com 400
elementos e malhas de 600 elementos no bloco)

Pode-se observar que as varia¸c˜oes diminu´ıram consideralvemnte. A linha de

tendˆencia continua apontando um maior custo nos problemas pr´oximos ao po¸co e,

dessa vez com uma inclina¸c˜ao maior. A diferen¸ca percebida nas inclina¸c˜oes indica

que no caso com 600 elementos nas malhas do bloco, h´a uma maior diferen¸ca entre

os custos dos problemas pr´oximos ao po¸co e aqueles mais distantes. Os valores de

tais inclina¸c˜oes s˜ao mostrados na ﬁgura 4.18 como fun¸c˜ao do n´umero de elementos

nas malhas do bloco. Foi inclu´ıdo um experimento adicional e intermedi´ario com

300 elementos nas malhas do bloco.

Figura 4.18: Inclina¸c˜oes das linhas de tendˆencia das ﬁguras 4.13 e 4.17 em fun¸c˜ao
do tamanho das malhas do bloco.

Esses resultados indicam que, com malhas maiores no bloco, maiores ser˜ao

102

y = 0.0002x + 2.8567 05010015020025030035040000.511.522.533.5Identificador do problema (400 = Problema mais próximo ao poço)Tempo de execução (s)0.00E+005.00E-051.00E-041.50E-042.00E-042.50E-040100200300400500600700Número de elementos em cada malha do blocoInclinação das linhas de tendência das figuras 4.13 e 4.17(Figura 4.17)(Figura 4.13)(Caso intermediário)os problemas de desbalanceamento caso seja omitida a cl´ausula schedule e,

portanto, utilizado o escalonamento est´atico. Nesses casos, espera-se que a

eﬁc´acia da cl´ausula schedule(dynamic) seja maior, sendo maiores tamb´em

os speedups com sua utiliza¸c˜ao. Apresentamos a seguir os resultados de speedup

dos experimentos (A) e (B) por´em com as malhas do bloco aumentadas para 600

elementos e uso da cl´ausula schedule(dynamic).

Figura 4.19: Speedups com a cl´ausula schedule(dynamic) para o experimento
(A) com malhas do bloco aumentadas para 600 elementos..

Figura 4.20: Speedups com a cl´ausula schedule(dynamic) para o experimento
(B) com malhas do bloco aumentadas para 600 elementos.

Os

resultados mostram que, na verdade, os benef´ıcios da cl´ausula

103

1.00 2.00 4.00 6.00 8.00 4.87 6.54 1.00 5.13 6.79 0.001.002.003.004.005.006.007.008.009.0002468Speedup Número de Threads Speedup LinearSpeedup Obtido: Schedule STATICSpeedup Obtido: Schedule DYNAMIC1.00 2.00 4.00 6.00 8.00 4.79 6.29 1.00 5.14 6.71 0.001.002.003.004.005.006.007.008.009.0002468Speedup Número de Threads Speedup LinearSpeedup Obtido: Schedule STATICSpeedup Obtido: Schedule DYNAMICschedule(dynamic) tiveram menor impacto nesses casos. Mesmo com uma

maior inclina¸c˜ao na linha de tendˆencia da ﬁgura 4.17, que indica maior diferen¸ca

entre os custos dos problemas da parte inferior do bloco e os da parte superior,

os ganhos em speedup atribu´ıdos ao uso do escalonamento dinˆamico foram menos

importantes do que nos casos dos experimentos (A) e (B), mostrados nas ﬁguras

4.15 e 4.16, onde nota-se uma maior distˆancia entre static e dynamic.

Estas observa¸c˜oes nos permitem concluir que a eﬁc´acia da cl´ausula

schedule(dynamic) esteve muito mais relacionada com uma grande oscila¸c˜ao

nos custos de cada problema do que com a conhecida tendˆencia de que eles sejam

mais custosos pr´oximo ao po¸co.

Testes adicionais com todos os outros tipos de escalonamento oferecidos pelo

padr˜ao OpenMP foram realizados para os experimentos padr˜ao (A) e (B), sendo os

melhores resultados obtidos com a cl´ausula schedule(dynamic). Levantamos

a hip´otese de que, nos casos com malhas maiores no bloco, como os ´ultimos

casos testados, outros tipos de scheduling possam levar a melhores resultados.

Em raz˜ao da menor oscila¸c˜ao dos tempos de cada problema, nesses casos, os

custos inerentes ao dynamic scheduling, relacionados `a constru¸c˜ao da ﬁla que

controla a distribui¸c˜ao da itera¸c˜oes para as threads, provavelmente n˜ao est˜ao sendo

suﬁcientemente sobrepujados pela pouca melhoria de balanceamento oferecida.

4.4

A estrat´egia de paraleliza¸c˜ao com MPI

Nas se¸c˜oes anteriores j´a foi poss´ıvel observar ganhos signiﬁcativos no

desempenho do simulador com a estret´egia de paraleliza¸c˜ao desenvolvida com

uso do padr˜ao OpenMP. Como visto no cap´ıtulo 2, o padr˜ao OpenMP ´e

voltado aos sistemas de mem´oria compartilhada, sendo aplicado frequentemente na

paraleliza¸c˜ao de aplica¸c˜oes executadas em ambientes com CPUs multicore como os

existentes em cada um dos n´os do cluster Altix-xe, descrito mais adiante e utilizado

em testes com MPI neste trabalho.

A aplica¸c˜ao de uma estrat´egia utilizando a padr˜ao MPI, permitiria a

104

utiliza¸c˜ao de v´arios n´os do cluster, uma vez que tal padr˜ao ´e destinado a sistemas

de mem´oria distribu´ıda. Com objetivo de expandir os resultados obtidos com

OpenMP em um ´unica m´aquina, ou n´o, foi desenvolvida, com uso do MPI, uma

segunda estrat´egia de paraleliza¸c˜ao para funcionar em conjunto com a primeira.

Para a implementa¸c˜ao da estrat´egia com MPI, nos concentramos novamente

na rotina resolverProbVariosBlocos, j´a mostrada na se¸c˜ao 4.2 em suas

vers˜oes serial (listagem 4.2) e com OpenMP (listagem 4.5) A seguir apresentaremos

por partes, em trˆes trechos de c´odigo as altera¸c˜oes feitas na rotina para a inclus˜ao

da paraleliza¸c˜ao com MPI.

Primeiramente, na listagem 4.6 observamos uma s´erie novas vari´aveis, a

partir da linha 7, a serem usadas neste e nos pr´oximos dois trechos mostrados.

Destacamos as vari´aveis numProcs e meuId, na linha 8, que servir˜ao para

armazenar, respectivamente, o tamanho do comunicador MPI (quantos processos

estar˜ao executando em paralelo) e a identiﬁca¸c˜ao de cada processo. A essas duas

vari´aveis s˜ao atribu´ıdos valores com uso de duas rotinas do MPI: MPI_Comm_size

e MPI_Comm_rank, cujas chamadas podem ser vistas nas linhas 13 e 14.

Listagem 4.6: Vers˜ao da rotina resolverProbVariosBlocos paralelizada com

OpenMP+MPI.

1 SUBROUTINE resolverProbVariosBlocos(...)

2

3

4

5

6

7

8

9

10

11

12

USE mpi

implicit none

INTEGER :: numBlocos, NUSTEP

REAL*8 :: fluxoMassicoDeBlocoParaFratura(numBlocos), fluxoMolM2S

, DT

INTEGER :: iBlocos

INTEGER :: error

INTEGER :: numProcs, meuId

INTEGER :: meuComeco, meuFim, meuTamanho, resto, parteInteira

INTEGER :: tamanho, cont !Tamanho do comunicador MPI

REAL*8 :: receiveBuffer(numBlocos)
ALLOCATABLE :: sendArray(:)
REAL*8,

105

13

14

15

call MPI_Comm_size (mpi_comm_world, numProcs, error )

call MPI_Comm_rank (mpi_comm_world, meuId, error )

(...)

16 END SUBROUTINE resolverProbVariosBlocos

A id´eia b´asica da paraleliza¸c˜ao com MPI, que ir´a conviver com aquilo que

foi feito com OpenMP, ´e ilustrada na ﬁgura 4.21 e consiste em dividir inicialmente

a fratura em um n´umero N de partes de tamanho igual e destinar, a cada n´o de

execu¸c˜ao, todos os problemas do bloco relacionados a uma por¸c˜ao da fratura.

Cada processo MPI, tratar´a, portanto, de resolver um n´umero predeﬁnido de

problemas do bloco, relativos `a sua parcela da fratura.

Isso assemelha-se ao

que era feito automaticamente pelo padr˜ao OpenMP na paraleliza¸c˜ao no loop

em quest˜ao com escalonamento est´atico. Internamente, cada processo continuar´a

utilizando o paralelismo multithread com OpenMP para resolver o seu subconjunto

de problemas, visto que cada n´o ´e dotado de CPUs multicore.

Figura 4.21: A estrat´egia de paraleliza¸c˜ao em processos com MPI.

Mais adiante no c´odigo da fun¸c˜ao resolverProbVariosBlocos, ap´os

o trecho exibido na listagem 4.6, existe o loop que resolve os v´arios problemas

106

Poço Bloco Processo #N Processo #1(…) (…) (...)(...)do bloco. Esta parte ´e mostrada na listagem 4.7. Chamamos aten¸c˜ao para

a linha 14, onde percebe-se que cada processo s´o realizar´a uma parte do loop

original, resolvendo apenas uma parcela dos problemas do bloco, como ilustrado

na ﬁgura 4.21 e portanto, atribuindo valores a apenas uma parte do vetor

fluxoMassicoDeBlocoParaFratura na linha 16.

Listagem 4.7: Vers˜ao da rotina resolverProbVariosBlocos paralelizada com

OpenMP+MPI.

1 SUBROUTINE resolverProbVariosBlocos(..)

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

(...)

resto = MOD(numBlocos, numProcs)

parteInteira=(numBlocos / numProcs)

meuComeco = parteInteira * meuId + 1;

if (resto .gt. meuId) then

meuComeco = meuComeco + meuId

meuFim = meuComeco + parteInteira

else

meuComeco = meuComeco + resto

meuFim = meuComeco + parteInteira - 1

endif

!$omp parallel do firstprivate(NUSTEP, TEMPO, fluxoMolM2S,

condContorno, DT)

DO iBlocos=meuComeco, meuFim

CALL processadorBloco(iBlocos, NUSTEP, TEMPO, fluxoMolM2S

, condContorno(iBlocos), DT)

fluxoMassicoDeBlocoParaFratura(iBlocos) = fluxoMolM2S;

END DO

! fim do loop do problema micro (blocos)

!$omp end parallel do

(...)

20 END SUBROUTINE resolverProbVariosBlocos

Finalmente, ap´os o trecho apresentado na listagem 4.7 cada processo possui

apenas uma parte do vetor fluxoMassicoDeBlocoParaFratura devidamente

calculada. ´E necess´ario que, nesse momento, exista uma comunica¸c˜ao entre os

processos para que cada um possa enviar aos demais a sua parcela do vetor. Para

107

isso, ´e feito uso de mais uma rotina do MPI: MPI_AllGather, cuja chamada pode

ser vista na listagem 4.8, linha 9. Ao ﬁm da rotina, todos os processos possuem o

vetor fluxoMassicoDeBlocoParaFratura completo, com a contribui¸c˜ao dos

demais.

Listagem 4.8: Vers˜ao da rotina resolverProbVariosBlocos paralelizada com

OpenMP+MPI

1 SUBROUTINE resolverProbVariosBlocos (...)

2

3

4

5

6

7

8

9

(...)

!Comunicação entre os processos:

meuTamanho = (meuFim - meuComeco) + 1

if (.not. ALLOCATED(sendArray)) then

allocate(sendArray(meuTamanho))

endif

sendArray(1:meuTamanho) = fluxoMassicoDeBlocoParaFratura(

meuComeco:meufim)

call MPI_AllGather(sendArray, meuTamanho, MPI_REAL8,

receiveBuffer,meuTamanho, MPI_REAL8, MPI_COMM_WORLD, error)

10

fluxoMassicoDeBlocoParaFratura(1:numBlocos) = receiveBuffer(1:

numBlocos)

11 END SUBROUTINE resolverProbVariosBlocos

4.5

Testes de desempenho II: Paraleliza¸c˜ao com OpenMP + MPI

Nesta se¸c˜ao abordamos os testes de desempenho feitos com a segunda

estrat´egia de paraleliza¸c˜ao desenvolvida, utilizando OpenMP e MPI em conjunto.

Os casos de teste s˜ao os mesmos experimentos (A) e (B) utilizados para os testes

com OpenMP e descritos na se¸c˜ao 4.3.

4.5.1

Ambiente de Execu¸c˜ao e Metodologia dos testes

Todos os testes realizados para a paraleliza¸c˜ao com OpenMP+MPI foram

executados no cluster Altix-xe, do LNCC. O cluster ´e composto por 30 n´os de

108

execu¸c˜ao, cada um com a seguinte especiﬁca¸c˜ao:

• Modelo Altix-XE 340
• 2 Processadores Quad Core Intel(R) Xeon(R) CPU E5520 @ 2.27GHz

(Total de 8 cores)

• 24GB de mem´oria DDR3

O compilador utilizado foi o gfortran vers˜ao 4.7.2. A implementa¸c˜ao do MPI

utilizada foi o OpenMPI 1.8.5 e a metodologia de testes utilizada foi a mesma da

se¸c˜ao 4.3.

Para os testes com MPI, foram submetidos jobs ao gerenciador de ﬁlas Sun

Grid Engine (SGE) e a aplica¸c˜ao foi testada em uma ﬁla que d´a acesso a 8 n´os com

conﬁgura¸c˜ao descrita anteriormente. Por esta raz˜ao, os testes foram feitos com o

m´aximo de 8 processos, cada um utilizando 8 threads OpenMP internamente ao

n´o.

4.5.2

Resultados do experimento (A) com OpenMP+MPI

Nesta se¸c˜ao apresentamos os resultados das tomadas de tempo com a vers˜ao

paralelizada via OpenMP e MPI, para o experimento (A). A tabela 4.4 exibe, na

parte esquerda, os tempos totais de execu¸c˜ao da aplica¸c˜ao (em minutos) para

1, 3, 4, 6 e 8 threads (somente OpenMP) e, na parte direita, os tempos de

execu¸c˜ao utilizando a vers˜ao h´ıbrida (OpenMP e MPI), sempre com 8 threads,

por´em variando o n´umero de processos. Para cada caso, a aplica¸c˜ao foi executada

6 vezes. Na linha em destaque na tabelaa s˜ao apresentadas as m´edias dos tempos

de execu¸c˜ao. A ´ultima linha apresenta o speedup obtido em cada caso.

109

Tabela 4.4: Tempos de execu¸c˜ao (em minutos) da vers˜ao com OpenMP+MPI para
o experimento (A).

(T) Threads OpenMP

(P) Processos MPI

(T) Threads OpenMP

Execu¸c˜ao #1
Execu¸c˜ao #2
Execu¸c˜ao #3
Execu¸c˜ao #4
Execu¸c˜ao #5
Execu¸c˜ao #6
M´edia (minutos)
SpeedUp Obtido

2T
3.50
3.47
3.37
3.52
3.43
3.43
3.45

1T
6.40
6.42
6.43
6.28
6.28
6.03
6.31
1.00x 1.83x 3.38x 4.87x 6.07x 6.04x

4T
1.85
1.88
1.88
1.88
1.83
1.87
1.87

6T
1.32
1.30
1.28
1.32
1.28
1.27
1.29

8T
1.02
1.05
1.03
1.05
1.05
1.03
1.04

1P 8T 2P 8T 4P 8T 8P 8T
1.03
1.05
1.05
1.03
1.05
1.05
1.04

0.30
0.30
0.30
0.30
0.30
0.30
0.30
21.03x 30.69x

0.62
0.65
0.65
0.65
0.65
0.63
0.64
9.83x

0.20
0.23
0.20
0.20
0.20
0.20
0.21

O gr´aﬁco da ﬁgura 4.22 mostra as m´edias dos tempos de execu¸c˜ao destacadas

na tabela 4.4.

Figura 4.22: M´edias dos tempos de execu¸c˜ao (em minutos) da vers˜ao com
OpenMP+MPI para o experimento (A).

Na ﬁgura 4.23 ´e mostrado o gr´aﬁco de speedups.

S˜ao exibidos, para

visualiza¸c˜ao comparativa, os speedups obtidos com a vers˜ao que inclui somente

OpenMP (em verde), aos quais juntam-se os speedups da vers˜ao h´ıbrida (em azul),

ambos em compara¸c˜ao ao caso ideal, speedup linear, (em vermelho).

110

6.31 3.45 1.87 1.29 1.04 1.04 0.64 0.30 0.21 0.001.002.003.004.005.006.007.001 Thread2 Threads4 Threads6 Threads8 Threads8 Threads1 Processo8 Threads2 Processos8 Threads4 Processos8 Threads8 ProcessosTempo de Execução (min) Unidades de Execução Figura 4.23: Speedups da vers˜ao com OpenMP+MPI para o experimento (A).

4.5.3

Resultados do experimento (B) com OpenMP+MPI

Os testes realizados para os experimento (A) foram repetidos para o

experimento (B), com malha uniforme de 2200 elementos na fratura. A seguir

s˜ao exibidos: a tabela de tempos obtidos, o gr´aﬁco com as m´edias de tempo e o

gr´aﬁco de speedup, os mesmos exibidos anteriormente para o experimento (A).

Tabela 4.5: Tempos de execu¸c˜ao (em minutos) da vers˜ao com OpenMP+MPI para
o experimento (B).

(T) Threads OpenMP

(P) Processos MPI

(T) Threads OpenMP

Execu¸c˜ao #1
Execu¸c˜ao #2
Execu¸c˜ao #3
Execu¸c˜ao #4
Execu¸c˜ao #5
Execu¸c˜ao #6
M´edia (minutos)
SpeedUp Obtido

1T

2T

4T

19.28
19.65
19.62
19.68
19.25
19.10
19.43

37.67
36.93
36.48
37.22
37.70
37.18
37.20
1.00x 1.91x 3.74x 5.15x 6.49x 6.47x

10.07
9.92
9.95
9.75
10.20
9.87
9.96

6T
7.10
7.43
7.27
7.23
7.02
7.25
7.22

8T
5.68
5.68
5.77
5.80
5.75
5.70
5.73

1P 8T 2P 8T 4P 8T 8P 8T
5.77
5.77
5.68
5.78
5.75
5.75
5.75

2.95
2.95
2.93
2.93
2.95
2.97
2.95
12.62x 20.20x 37.62x

1.90
1.75
1.82
1.87
1.85
1.87
1.84

1.00
1.02
1.00
1.02
0.98
0.92
0.99

111

6.04 9.83 21.03 30.69 0.0010.0020.0030.0040.0050.0060.0070.000816243240485664Speedup Número de Unidades de Execução (Processos x Threads) Speedup LinearSpeedup Obtido: OpenMPSpeedup Obtido: OpenMP + MPIFigura 4.24: M´edias dos tempos de execu¸c˜ao (em minutos) da vers˜ao com
OpenMP+MPI para o experimento (B).

Figura 4.25: Speedups da vers˜ao com OpenMP+MPI para o experimento (B).

4.5.4

Discuss˜ao dos resultados com MPI

Quanto `a estrat´egia de paraleliza¸c˜ao h´ıbrida com OpenMP e MPI, obtivemos

resultados de speedup de at´e 30,69x para o experimento (A) e 37,62x para o

experimento (B). Embora tais n´umeros estejam mais distantes do speedup linear,

consideramos esta uma boa aproxima¸c˜ao inicial para uma paraleliza¸c˜ao do modelo

em m´aquinas de mem´oria distribu´ıda que ainda pode ser aperfei¸coada. Sem a

paraleliza¸c˜ao com MPI, estar´ıamos limitados a apenas um n´o de execu¸c˜ao.

112

37.20 19.43 9.96 7.22 5.73 5.75 2.95 1.84 0.99 0.005.0010.0015.0020.0025.0030.0035.0040.001 Thread2 Threads4 Threads6 Threads8 Threads8 Threads1 Processo8 Threads2 Processos8 Threads4 Processos8 Threads8 ProcessosTempo de Execução (min) Unidades de Execução 6.47 12.62 20.20 37.62 0.0010.0020.0030.0040.0050.0060.0070.000816243240485664Speedup Número de Unidades de Execução (Processos x Threads) Speedup LinearSpeedup Obtido: OpenMPSpeedup Obtido: OpenMP + MPIOs problemas quanto ao balanceamento de carga vistos para o OpenMP

tamb´em ocorrem na paraleliza¸c˜ao com MPI, neste caso com um agravante: a

divis˜ao inicial da fratura em por¸c˜oes de tamanho pr´e-deﬁnido foi a ideia central

da estrat´egia de paraleliza¸c˜ao para a divis˜ao de trabalho, o que se assemelha ao

escalonamento est´atico do OpenMP. Espera-se que o desbalanceamento n˜ao possa

ser corrigido com a mesma facilidade encontrada no caso do OpenMP, que oferece

mais ﬂexibilidade nesse sentido, com pouca altera¸c˜ao de c´odigo, por meio do uso

da cl´ausula schedule.

O balanceamento de carga com MPI dever´a envolver t´ecnicas desenvolvidas

pelo programador a ﬁm de que se divida os problemas do bloco entre os processos

de forma mais adequada. Uma solu¸c˜ao paralela mais avan¸cada com MPI dever´a

dividir o espa¸co de itera¸c˜oes do loop mostrado na listagem 4.7 entre os processos

de forma mais elaborada do que aquela mostrada na ﬁgura 4.21.

Um ponto de melhoria aqui identiﬁcado ´e a utiliza¸c˜ao de uma alternativa

`a rotina MPI_AllGather,

cuja chamada foi mostrada na listagem 4.8.

A rotina promove

comunica¸c˜oes desnecess´arias

entre

todos os processos

para que cada um envie a todos os demais a sua parcela do vetor

fluxoMassicoDeBlocoParaFratura. ´E necess´ario que apenas um processo

contenha o vetor completo para que prossiga com a execu¸c˜ao do programa. A

economia de comunica¸c˜oes entre os processos poder´a beneﬁciar consider´avelmente

o desempenho geral da aplica¸c˜ao.

113

Cap´ıtulo 5

Conclus˜oes e perspectivas

Nesta disserta¸c˜ao descrevemos um trabalho centrado em aspectos

computacionais, realizado com um simulador relacionado `a ciˆencia de escoamentos

em meios porosos. Baseado em uma implementa¸c˜ao original da d´ecada de 1980

que se destacou em sua ´epoca como um poderoso programa de elementos ﬁnitos,

o simulador pode, atualmente, beneﬁciar-se de algumas t´ecnicas de programa¸c˜ao

mais modernas. Este trabalho representou um passo evolutivo deste simulador que

se manifestou de duas formas: (i) o emprego da programa¸c˜ao orientada a objetos

em um de seus m´odulos e (ii) o desenvolvimento de estrat´egias de computa¸c˜ao

paralela para reduzir o seu tempo de execu¸c˜ao.

Quanto ao primeiro aspecto, desenvolvemos uma reformula¸c˜ao do m´odulo

de sistemas de equa¸c˜oes deste simulador, conferindo ao mesmo caracter´ısticas mais

modernas com a inclus˜ao do paradigma de orienta¸c˜ao a objetos, visando contribuir

para sua melhor organiza¸c˜ao, entendimento humano e capacidade de evolu¸c˜ao. Os

novos m´odulos com classes facilitam e encorajam a troca ou incorpora¸c˜ao de novos

solvers neste ou em outros simuladores semelhantes. Al´em disso, a reformula¸c˜ao

funcionou como facilitadora para o desenvolvimento de estrat´egias de paraleliza¸c˜ao

para este simulador.

Quanto ao desempenho de execu¸c˜ao, desenvolvemos estrat´egias com os

padr˜oes OpenMP e MPI para a solu¸c˜ao em paralelo de m´ultiplos problemas de

elementos ﬁnitos. A refatora¸c˜ao do m´odulo referente aos sistemas de equa¸c˜oes,

114

descrita no cap´ıtulo 3, facilitou essa tarefa. A inclus˜ao do paradigma de orienta¸c˜ao

a objetos tornou mais natural a tarefa de criar m´ultiplas instˆancias em mem´oria

de uma mesma entidade denominada sistema de equa¸c˜oes, para sua solu¸c˜ao em

paralelo.

Nesse momento foi poss´ıvel observar como a escalabilidade do design

pˆode contribuir para a escalabilidade de execu¸c˜ao. A organiza¸c˜ao est´atica do

c´odigo-fonte de forma modular, a qual promoveu o agrupamento de elementos

relacionados, o encapsulamento e as abstra¸c˜oes, acabou, tamb´em, por contribuir

para a facilita¸c˜ao do desenvolvimento de solu¸c˜oes paralelas, que melhoraram o

desempenho do simulador.

Beneﬁciando-se dos resultados obtidos neste trabalho, s˜ao algumas as

perspectivas de trabalhos futuros.

Em primeiro lugar, ´e importante reconhecer que, embora os novos m´odulos

com orienta¸c˜ao objetos tenham trazido benef´ıcios ao simulador, contribuindo

para sua melhor organiza¸c˜ao e capacidade de evolu¸c˜ao, estes s˜ao resultado de

uma interven¸c˜ao localizada, restrita ao m´odulo de sistemas de equa¸c˜oes. Assim,

ressaltamos que o simulador n˜ao apresenta, neste momento, caracter´ısticas de

um produto ﬁnal desenvolvido com orienta¸c˜ao a objetos e n˜ao explora todos os

recursos deste paradigma, como, por exemplo, a prote¸c˜ao de dados. Dessa forma, a

expans˜ao do emprego da orienta¸c˜ao a objetos para outros m´odulos deste simulador,

bem como a maior e mais profunda explora¸c˜ao dos recursos oferecidos por tal

paradigma, surgem como perspectivas para a continua¸c˜ao dos trabalhos.

Quanto `as estrat´egias de paraleliza¸c˜ao, especialmente `aquela desenvolvida

com o padr˜ao MPI, pode-se dizer que a continua¸c˜ao dos trabalhos envolve ciclos

de otimiza¸c˜ao que podem melhorar o desempenho obtido at´e o presente momento.

Ainda que os testes de desempenho aqui realizados tenham utilizado o solver

baseado em elimina¸c˜ao de Gauss, mais adequado ao nosso modelo em espec´ıﬁco,

a solu¸c˜ao paralela desenvolvida ´e independente de solver. Isso signiﬁca que, em

outras situa¸c˜oes, nas quais o uso de um solver mais robusto como o Intel Pardiso

115

se fa¸ca necess´ario, como, por exemplo, simula¸c˜oes com malhas bidimensionais ou

tridimensionais, a mesma estrat´egia de paraleliza¸c˜ao poder´a ser utilizada, ainda

que sejam demandados, provavelmente, esfor¸cos de otimiza¸c˜ao.

Tamb´em merece lembran¸ca o fato de que a refatora¸c˜ao do m´odulo de sistemas

de equa¸c˜oes com inclus˜ao de orienta¸c˜ao a objetos possui potencial de re´uso em

outros simuladores similares que utilizam a mesma implementa¸c˜ao base do MEF.

Portanto, estender os benef´ıcios das classes a outros programas tamb´em ´e uma

perspectiva de trabalho e pode representar novas possibilidades para os mesmos.

Alguns simuladores utilizados no LNCC contam apenas com uma op¸c˜ao de solver.

A inclus˜ao dos novos m´odulos com classes poder´a oferecer, com certa facilidade,

nova op¸c˜oes em tal quesito.

116

Referˆencias Bibliogr´aﬁcas

K.J. Bathe. Finite Element Procedures in Engineering Analysis. Prentice-

Hall, 1982.

Stephen J. Chapman. Fortran 95/2003 for Scientists and Engineers.

McGraw-Hill, 3 edi¸c˜ao, 2004.

Patr´ıcia A. Pereira Costa. Modelagem Computacional Multiescala de

Reservat´orios n˜ao Convencionais de G´as em Folhelhos. Tese de

Doutorado, LNCC, 2015.

Thomas J. R. Hughes. The ﬁnite element method :

linear static and

dynamic ﬁnite element analysis. Englewood Cliﬀs, N.J. Prentice-Hall

International, 1 edi¸c˜ao, 1987.

James Jeﬀers e James Reinders.

Intel Xeon Phi Coprocessor High-

Performance Programming. Morgan Kaufmann, 1 edi¸c˜ao, 2013.

ISBN

9780124104143.

David Kirk e Wen-Mei. Hwu. Programming Massively Parallel Processors:

A Hands-on Approach. Morgan-Kaufmann, 2013.

T. G. Mattson, B. A. SANDERS, e B. L. MASSINGILL. Patterns for Parallel

Programming. Addison Wesley, 2004. ISBN 0321228111.

Peter S. Pacheco. An Introduction to Parallel Programming. Morgan

Kaumann, 2011.

117

Damian Rouson, Jim Xia, e Xiaofeng Xu. Scientiﬁc software design : the

object-oriented way. 2011.

E. M. Toledo, Eduardo L´ucio Mendes Garcia, R. S. SILVA, e R. C. ALMEIDA.

Axis - um programa para an´alise de s´olidos axisim´etricos com carregamento

qualquer. V SIBRAT- Simp´osio Brasileiro sobre Tubula¸c˜aoes e Vasos

de Press˜ao, 1988.

F. M. Varej˜ao. Linguagens de Programa¸c˜ao: conceitos e t´ecnicas. Elsevier,

1 edi¸c˜ao, 2004.

A. von Staa. Programa¸c˜ao Modular: desenvolvendo programas complexos

de forma organizada e segura. Elsevier, 1 edi¸c˜ao, 2000.

118

