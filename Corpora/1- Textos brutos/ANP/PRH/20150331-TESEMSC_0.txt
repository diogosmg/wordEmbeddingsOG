Felipe Codevilla Moraes

VisÃ£o computacional em meio subaquÃ¡tico:

Um estudo sobre detecÃ§Ã£o de pontos de interesse

e classificaÃ§Ã£o utilizando contexto.

Brasil

5 de marÃ§o, 2015

Felipe Codevilla Moraes

VisÃ£o computacional em meio subaquÃ¡tico:

Um estudo sobre detecÃ§Ã£o de pontos de interesse

e classificaÃ§Ã£o utilizando contexto.

Universidade Federal de Rio Grande

Programa de PÃ³s graduaÃ§Ã£o em ComputaÃ§Ã£o

Mestrado em Engenharia de ComputaÃ§Ã£o

Orientador: Nelson Duarte Lopez Filho

Coorientador: Silvia Silva da Costa Botelho

Brasil

5 de marÃ§o, 2015

Um estudo sobre detecÃ§Ã£o de pontos de interesse
e classificaÃ§Ã£o utilizando contexto./ Felipe Codevilla Moraes. â€“ Brasil, 5 de marÃ§o,
2015-

Felipe Codevilla Moraes

VisÃ£o computacional em meio subaquÃ¡tico:

Orientador: Nelson Duarte Lopez Filho

DissertaÃ§Ã£o de Mestrado â€“ Universidade Federal de Rio Grande
Programa de PÃ³s graduaÃ§Ã£o em ComputaÃ§Ã£o
Mestrado em Engenharia de ComputaÃ§Ã£o, 5 de marÃ§o, 2015.
1. VisÃ£o Computacional; 2. VisÃ£o SubaquÃ¡tica.

CDU 02:141:005.7

Felipe Codevilla Moraes

VisÃ£o computacional em meio subaquÃ¡tico:

Um estudo sobre detecÃ§Ã£o de pontos de interesse

e classificaÃ§Ã£o utilizando contexto.

Nelson Duarte Lopez Filho

Orientador

Silvia Silva da Costa Botelho

Co-orientadora

Nuno Estrela Gracias

Convidado

Glauber Acunha GonÃ§alvez

Convidado

Rafael Garcia

Convidado

Brasil

5 de marÃ§o, 2015

Agradecimentos

Entre tantas dÃºvidas , resolvi apostar permanecer onde estava e, ao contrÃ¡rio do

que se espera, nÃ£o poderia ter acertado mais.

Primeiramente gostaria de agradecer a minha orientadora Silvia Botelho por todas
as brilhantes ideias e oportunidades oferecidas, obviamente nada disso seria possÃ­vel sem
ela.

AgradeÃ§o a todos os colegas e amigos onde destaco Luan Silveira, Joel Gaya, Felipe

Guth e, em especial, a ajuda fornecida pelo Pedro Ballester na Ãºltima hora.

AgradeÃ§o a banca por comparecer e avaliar este trabalho.
I want to thank all the people from the ViCOROB Lab in Girona for receiving me
so well during the 10 months I spent in Catalonia. Without this experience, the majority
of this thesis would not be possible.

I also want to thank all my classmates and friends from Vibot (and Patryk) for all

the fun and professional adventures we had toguether.

AgradeÃ§o o apoio financeiro da AgÃªncia Nacional do PetrÃ³leo, GÃ¡s Natural e Bio-
combustÃ­veis â€“ ANP â€“ , da Financiadora de Estudos e Projetos â€“ FINEP â€“ e do MinistÃ©rio
da CiÃªncia e Tecnologia â€“ MCT por meio do Programa de Recursos Humanos da ANP para
o Setor PetrÃ³leo e GÃ¡s â€“ PRH-ANP/MCT. Um agradecimento especial aos professores
responsÃ¡veis pelo PRH - 27, Maria Isabel e Gilberto Griep.

Por fim, agradeÃ§o a minha famÃ­lia pelo apoio e carinho que sempre me deram, em
especial ao meu pai, por sempre servir como um formidÃ¡vel exemplo Ã©tico em minha vida.

Resumo

A exploraÃ§Ã£o e o monitoramento do bentos no ambiente marinho possuem
importÃ¢ncia econÃ´mica e ambiental crescente na sociedade atual. A qualidade da
tecnologia de obtenÃ§Ã£o de imagens Ã³ticas subaquÃ¡ticas tem melhorado consideravel-
mente devido ao advento dos Remotely Operated Vehicles (ROV) e dos Autonomous
Underwater Vehicles (AUVs), o que tem possibilitado a coleta de milhares de dados
visuais do fundo do oceano.

TÃ©cnicas de visÃ£o computacional, atualmente em franca utilizaÃ§Ã£o em am-
bientes terrestres, podem auxiliar a interpretaÃ§Ã£o automÃ¡tica destas imagens, seja
para minimizar o trabalho de identificaÃ§Ã£o e monitoramento de feiÃ§Ãµes e espÃ©cies,
seja para fornecer subsÃ­dios a realizaÃ§Ã£o autÃ´noma de missÃµes.

PorÃ©m devido a presenÃ§a do meio lÃ­quido, a propagaÃ§Ã£o da luz no meio su-
baquÃ¡tico apresenta efeitos fotomÃ©tricos que causam degradaÃ§Ã£o na imagem, emer-
gindo diversas questÃµes a serem tratadas na classificaÃ§Ã£o de imagens subaquÃ¡ticas,
as quais nÃ£o estÃ£o presentes em outros ambientes.

Assim, o objetivo geral deste trabalho Ã© estudar tÃ©cnicas de visÃ£o computa-
cional, e sua sensibilidade a presenÃ§a do meio lÃ­quido. De forma mais precisa, duas
tÃ©cnicas de visÃ£o computacional sÃ£o principalmente tratadas: a detecÃ§Ã£o de pontos
de interesse e a adiÃ§Ã£o das informaÃ§Ãµes de contexto para classificaÃ§Ã£o de objetos em
ambientes subsea.

SÃ£o aplicados e analisados diferentes algoritmos de detecÃ§Ã£o de pontos de
interesse frente a imagens com diferentes nÃ­veis de turbidez. Um novo dataset foi
proposto capaz de fornecer cenÃ¡rios com diferentes nÃ­veis de turbidez e objetos em
cena, permitindo o testes mÃºltiplos dos detectores mais usados na literatura e seu
comportamento frente os fenÃ´menos de degradaÃ§Ã£o causados na imagem no meio
subaquÃ¡tico. Foi encontrado que o algoritmo DoG se mostrou como uma melhor
alternativa para resolver tal problema de forma invariante a escala.

TambÃ©m foi estudada a questÃ£o da adiÃ§Ã£o de contexto como forma de me-
lhorar a taxa de acerto da classificaÃ§Ã£o de imagens subaquÃ¡ticas. Foi proposto um
novo mÃ©todo para incluir contexto na classificaÃ§Ã£o baseado em GeoestatÃ­stica e
comparou-se com outras formas tradicionais de adiÃ§Ã£o de contexto como os Condi-
tional Random Fields (CRF).

Palavras-chaves: VisÃ£o Computacional, GeostatÃ­stica, VisÃ£o SubaquÃ¡tica.

Abstract

The exploration and monitoring of the benthic sea zone has an important
economic and environmental role in the nowadays society. The quality of the optical
image acquiring technologies has become considerably better. This happened mainly
due to the advent of the Remotely Operated Vehicles (ROV) and the Autonomous
Underwater Vehicles (AUVs) and has opened the possibility to collect thousands of
visual data from the seabed environment.

Computer vision techniques are today being largely used in over-land envi-
ronments and can help the autonomous interpretation of images. These techniques
can help to minimize the work of identifying and monitoring species and objects.
Either having vision as a data acquiring source or to assist the automation of the
operations.

However, due to the presence of the liquid media, the light propagation
in underwater environments has photometric effects that cause degradation of the
image. This degradation develops a lot of issues to be treated on underwater images
that do not exist in other environments.

Thus, the objective of this work is to study computer vision tecniques consid-
ering their sensibility to phenomenas of the underwater environments media. More
precisely, mainly two computer vision techniques are considered: feature point detec-
tion and the adition context information for image classification, both on underwater
images.

Different algorithms for feature point detection are applied for feature point
detection under different turbidity levels. We provide a new dataset capable of
providing different scenarios with different levels of turbidity. This dataset allowed
the test of multiple feature detectors regarding their behavior with respect to the
degradation effects of water turbidity. We found that, in this scenario, the DoG
algorithm is the best alternative to solve scale invariant feature detection problems.
Finally, we studied the issue concerning the addition of context as a way to
improve the accuracy of underwater image classification. We proposed a new method
to include the context information on classification that is based on Geostatistics.
This method was compared with an other traditional form of context addition that
is the Conditional Random Fields (CRF).

Key-words: Computer Vision, Geostatistics, Underwater Vision.

Lista de ilustraÃ§Ãµes

Figura 1 â€“ Comportamento da aplicaÃ§Ã£o dos kernels Hessian e Harris para uma
imagem teste (1a). (1b) mostra a saÃ­da da medida de Harris (Eq. 1.4).
(1c) mostra a saÃ­da do determinante da matriz Hessian ( Eq. 1.5 )
para a imagem teste. Tanto o Hessian como o Harris tem como saÃ­da
as regiÃµes de alta curvatura ( Figura por Sojka (2003)). . . . . . . . . . 31

Figura 2 â€“ O processo para geraÃ§Ã£o do espaÃ§o de escala pelo DoG. Ao invÃ©s de
computar o Laplacian para cada escala, o mesmo Ã© estimado pela dife-
renÃ§a entre escalas consecutivas. Figura adaptada de (LOWE, 2004).

. 34

Figura 3 â€“ Exemplo de um filtro caixa de tamanho 9x9 aplicado para geraÃ§Ã£o de
um espaÃ§o de escala equivalente a ğœ = 1.2. Outros espaÃ§os podem ser
gerados usando caixas maiores.

. . . . . . . . . . . . . . . . . . . . . . 34

Figura 4 â€“ Alguns tipos de filtros utilizados para geraÃ§Ã£o do espaÃ§o de escala pelo
CenSurE. O filtro estrela, o filtro hexagonal e o filtro por diferenÃ§a de
caixas. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

Figura 5 â€“ EspaÃ§os de escala gerados. Primeira coluna mostra o espaÃ§o Gaussiano.
Segunda coluna mostra o filtro mÃ©dia de caixas usado pelo FastHes-
sian. Terceira coluna mostra um filtro poligonal estrelar de seis lados.
A quarta Coluna mostra o espaÃ§o de escala anisotrÃ³pico usado pelo
KAZE.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

Figura 6 â€“ Janela considerada para a classificaÃ§Ã£o usando contexto. No caso da
integraÃ§Ã£o de contexto diretamente nos classificadores (Fig. 6a), nÃ£o
sÃ£o considerada as relaÃ§Ãµes entre a vizinhanÃ§a com si prÃ³pria (Fig. 6b).
Ou seja, se existem propriedades correlacionadas na vizinhanÃ§a.

. . . . 42

Figura 7 â€“ A representaÃ§Ã£o grÃ¡fica de um modelo CRF. Os quadrados em verme-
lho (ğœ™ğ‘¢
ğ‘– (ğ‘¥ğ‘–, ğœƒğ‘¢)) sÃ£o os fatores unitÃ¡rios calculados com o resultado dado
pelo classificador. Os quadrados em azul sÃ£o os fatores locais computa-
dos em cada aresta e utilizados para introduzir informaÃ§Ã£o contextual.
Os circulos verdes representam os superpixeis sendo classificados.

. . . 44

Figura 8 â€“ TendÃªncias que existem para as classes estarem prÃ³ximas umas das
outras, quanto mais claro, maior Ã© a tendÃªncia existente. Por exemplo,
Ã© possÃ­vel perceber que a classe B Ã© provÃ¡vel de aparecer perto de uma
classe C mas nÃ£o prÃ³xima de uma classe E.

. . . . . . . . . . . . . . . 46

Figura 9 â€“ DivisÃ£o especificada em dois nÃ­veis de classificaÃ§Ã£o. O nÃ­vel unÃ¡rio
ğ‘ƒğ‘¢(ğ¿|ğœƒğ‘¢) onde somente a informaÃ§Ã£o do superpixel segmentado Ã© uti-
lizada, apresentado em verde. E o nÃ­vel local ğ‘ƒğ‘™(ğ¿|ğ‘Š), onde um de-
terminado contexto local Ã© incluÃ­do na classificaÃ§Ã£o, representado pelo
circulo azul. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50

Figura 10 â€“Figura do separador linear obtido pelo treinamento do SVM. Dado os
conjuntos de dados jÃ¡ rotulados ( Azuis e Vermelhos), o SVM determina
o separador de mÃ¡xima margem. A saÃ­do numÃ©rica do SVM jÃ¡ Ã© prÃ³pria
para se ter um certo grau de confianÃ§a do classificador.

. . . . . . . . 51

Figura 11 â€“GrÃ¡fico mostrando a probabilidade de acerto em funÃ§Ã£o da mÃ¡xima
confianÃ§a retornada pelo classificador para um conjunto de dados. Em
vermelho tem-se a funÃ§Ã£o ğ¶ğ‘™ğ‘– treinada a partir do conjunto de dados
em azul. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52

Figura 12 â€“Histograma mostrando a distribuiÃ§Ã£o de probabilidades de saÃ­da de
um classificador. Para o caso, a segunda classe, Ã© a que obteve maior
probabilidade, porÃ©m existe uma certa incerteza com relaÃ§Ã£o a primeira
classe.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53

Figura 13 â€“Diagrama geral da adiÃ§Ã£o de contexto local utilizando GeoestatÃ­stica.
Primeiramente Ã© medida a variabilidade entre as classes no contexto
espacial. Tanto diretamente atravÃ©s das frequÃªncias de transiÃ§Ã£o na
imagem (taxa de transiÃ§Ã£o medida), quanto atravÃ©s da inferÃªncia de
propriedades estatÃ­sticas vindas da imagem (taxa de transiÃ§Ã£o mode-
lada). Em seguida sÃ£o calculados os vetores de transiÃ§Ã£o. Na segunda
parte os vetores sÃ£o utilizados para gerar pesos para imagem. Com isso,
utilizando os pesos, o sistema SIS computa a adiÃ§Ã£o de contexto local
para cada superpixel.

. . . . . . . . . . . . . . . . . . . . . . . . . . . 54

Figura 14 â€“Medida feita do nÃºmero de transiÃ§Ãµes que uma classe faz para cada
outra para mÃºltiplas distÃ¢ncias. Foi utilizada um kernel mÃ³vel e foram
contadas as transiÃ§Ãµes desde o centro (ponto vermelho) para todas as
direÃ§Ãµes (representado pelos quadrados)

. . . . . . . . . . . . . . . . . 56

Figura 15 â€“A transiÃ§Ã£o de probabilidade modelada para um determinado dataset.
O eixo y apresenta a distÃ¢ncia em pixeis. As linhas verdes mostram
as proporÃ§Ãµes para cada classe. Pode-se observar uma certa tendÃªncia
na classe Urchin em transitar para categoria de background. Ainda,
percebe-se que a classe de background tem um grande comprimento
mÃ©dio, dado que sua taxa de decaimento Ã© bastante alta.

. . . . . . . . 58

Figura 16 â€“Exemplo de uma vizinhanÃ§a sendo considerada para um superpixel (
apontado em vermelho). Um raio ğ‘Ÿ Ã© considerado e ğ‘ pontos sÃ£o amos-
trados nessa vizinhanÃ§a ( em azul). Cada um dos pontos amostrados
irÃ¡ influenciar no potencial do superpixel apontado em vermelho. . . . . 60

Figura 17 â€“RepresentaÃ§Ã£o grÃ¡fica do modelo de GeoestatÃ­stica (GS). Os fatores
locais sÃ£o representados em azul e usam a estatÃ­stica de probabilidade
de transiÃ§Ã£o computada pela Eq. 3.3. Diferentemente do que no modelo
da Fig. 7, vizinhos de diferentes distÃ¢ncias tambÃ©m contribuem para
calcular a distribuiÃ§Ã£o de cada posiÃ§Ã£o.

. . . . . . . . . . . . . . . . . 61

Figura 18 â€“TrÃªs trajetÃ³rias da luz atÃ© o plano da imagem. O componente direto,
contendo a informaÃ§Ã£o direta da cena. O forward-scattering, contendo
informaÃ§Ã£o da cena espalhada. Por fim, o backscattering contendo in-
formaÃ§Ãµes de fora da cena. . . . . . . . . . . . . . . . . . . . . . . . . . 64

Figura 19 â€“Imagem de exemplo para as degradaÃ§Ãµes do ambiente subaquÃ¡tico. Ã‰
possÃ­vel ver que existe uma variaÃ§Ã£o conforme a distÃ¢ncia e uma perda
significativa da informaÃ§Ã£o de cor. . . . . . . . . . . . . . . . . . . . . . 65
Figura 20 â€“A sequÃªncia utilizada para classificaÃ§Ã£o de imagens em meio subaquÃ¡tico. 66

Figura 21 â€“A cena criada para avaliar os algoritmos de avaliaÃ§Ã£o de features. Ela Ã©
composta por lampadas fluorescentes e uma camera fotografando fotos
impressas do assoalho do oceano.

. . . . . . . . . . . . . . . . . . . . . 72

Figura 22 â€“As imagens utilizadas no teste. As trÃªs imagens foram capturadas nas
Bahamas em condiÃ§Ãµes de turbidez prÃ³ximas do ideal em uma resoluÃ§Ã£o
de 4928x3264 pixeis

. . . . . . . . . . . . . . . . . . . . . . . . . . . . 74

Figura 23 â€“As imagens capturadas sob diferentes nÃ­veis de degradaÃ§Ã£o devido a
turbidez, controlado pela adiÃ§Ã£o de leite. Foram fotografadas trÃªs fotos
impressas diferentes, ğ‘ƒ1 (primeira coluna), ğ‘ƒ2 (segunda coluna) e ğ‘ƒ3
(terceira coluna). Na primeira linha foi mostrada a imagem limpa (sem
leite) para cada foto capturada. A segunda linha apresenta o intervalo
de Baixa Turbidez com por volta de 15ml de leite (ğ‘‡4). O intervalo de
MÃ©dia Turbidez Ã© mostrado na segunda linha e contÃ©m por volta de 50
ml de leite (ğ‘‡10). Finalmente, na ultima (quarta) linha Ã© mostrado o
intervalo com Alta turbidez tendo por volta de 100 ml de leite (ğ‘‡16).
Quantidade de leite setada para uma caixa com 1000 litros de Ã¡gua. . . 76

Figura 24 â€“Repetibilidade ( Taxa de Acerto) contra o indice de degradaÃ§Ã£o estru-
tural normalizado (NSDI). As linhas em laranja indicam os intervalos
de degradaÃ§Ã£o. Baixa Turbidez 0 atÃ© 0.25; MÃ©dia Turbidez, 0.25 atÃ©
0.75, e Alta Turbidez de 0.75 atÃ© 1.

. . . . . . . . . . . . . . . . . . . . 79

Figura 25 â€“ComparaÃ§Ã£o entre a geraÃ§Ã£o de um nÃ­vel do kernel do espaÃ§o de escala
usado por quatro detectores diferentes. O kernel foi aplicado em nÃ­veis
de turbidez diferentes para a imagem ğ‘ƒ1. Sendo que a primeira linha Ã©
a imagem limpa (ğ‘‡0), a segunda linha Ã© uma imagem com baixo nÃ­vel
de degradaÃ§Ã£o (ğ‘‡4), a terceira linha apresenta uma imagem com mÃ©dio
nÃ­vel de degradaÃ§Ã£o (ğ‘‡10), a quarta linha apresenta imagens do nÃ­vel de
degradaÃ§Ã£o alto (ğ‘‡16). Para cada caso Ã© mostrado o resultado de filtro
equivalente a a aproximadamente um kernel gaussiano de ğœ = 59.0.
Primeira Coluna: Gaussiano puro. Segunda Coluna: Borramento apro-
ximado em caixas . Terceira Coluna: DifusÃ£o utilizando um polÃ­gono es-
trelar de seis pontas. Quarta Coluna: kernel anisotrÃ³pico g2 do KAZE.
Ã‰ possÃ­vel ver de certa forma estruturas mais definidas para o esquema
de difusÃ£o usado pelo CenSurE (AGRAWAL; KONOLIGE; BLAS, 2008). 81

Figura 26 â€“Partes manualmente segmentadas utilizadas para treinamento do clas-
sificador. A esquerda sÃ£o mostrados exemplos de nove amostras usadas
para treinar o dataset Redsea. A direita sÃ£o apresentadas nove amostras
do dataset Marker.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85

Figura 27 â€“Curvas de confianÃ§a geradas no treinamento unÃ¡rio de cada classe para
o dataset Redsea. A curva de confianÃ§a ğ¶ğ‘™ğ‘– treinada para cada uma das
classes Ã© mostrada, se bem como o grau de confianÃ§a obtido.

. . . . . . 86

Figura 28 â€“Curvas de confianÃ§a geradas no treinamento unÃ¡rio de cada classe para
o dataset Marker. A curva de confianÃ§a ğ¶ğ‘™ğ‘– treinada para cada uma das
classes Ã© apresentada, bem como o grau de confianÃ§a obtido.

. . . . . . 87

Figura 29 â€“Vetores de transiÃ§Ã£o obtidos na etapa de treinamento para o mÃ©todo
de GeoestatÃ­stica do CapÃ­tulo 3. Os vetores indicam a probabilidade de
uma classe transitar para outra a uma determinada distÃ¢ncia. O eixo
x apresenta a distÃ¢ncia em pixeis. O eixo ğ‘¦ dos grÃ¡ficos apresenta as
probabilidades de transiÃ§Ã£o. Pode-se observar, por exemplo, uma certa
tendÃªncia na classe Urchin em transitar para categoria de background.

88

Figura 30 â€“Mapa temÃ¡tico dos Mosaicos para o dataset Redsea. As figuras mostram
a porcentagem de acerto relativa ao GroundTruth. As classes sÃ£o repre-
sentadas pelas seguintes cores: Verde Brain Coral; Amarelo Branchin
Coral; Azul Faviid Coral; Magenta Urchin e sem cor Ã© o background.
Os seguintes resultados sÃ£o mostrados.(30a) classificaÃ§Ã£o UnÃ¡ria. (30b)
mostra a classificaÃ§Ã£o UnÃ¡ria baseada nas curvas de confianÃ§a. (30c)
classificaÃ§Ã£o com adiÃ§Ã£o de contexto baseada em GeoestatÃ­stica. (30d)
classificaÃ§Ã£o com adiÃ§Ã£o de contexto utilizando CRF.

. . . . . . . . . . 92

Figura 31 â€“Mapa temÃ¡tico dos Mosaicos para o dataset Marker. As figuras mos-
tram a porcentagem de acerto relativa ao GroundTruth. As classes sÃ£o
representadas pelas seguintes cores: Verde Sand; Amarelo Sea Gorgon;
Azul Corals e sem cor Ã© o background. Os seguintes resultados sÃ£o
mostrados.(31a) classificaÃ§Ã£o UnÃ¡ria. (31b) mostra a classificaÃ§Ã£o UnÃ¡-
ria baseada nas curvas de confianÃ§a. (31c) classificaÃ§Ã£o com adiÃ§Ã£o de
contexto baseada em GeoestatÃ­stica. (31d) classificaÃ§Ã£o com adiÃ§Ã£o de
contexto utilizando CRF.

. . . . . . . . . . . . . . . . . . . . . . . . . 93

Figura 32 â€“Resultados de classificaÃ§Ã£o para os datasets Marker e os datasets Red-
sea. A primeira coluna apresenta a classificaÃ§Ã£o unitÃ¡ria. A segunda
coluna apresenta os resultados de GeoestatÃ­stica. A terceira coluna
apresenta os resultados para o CRF. Por fim, a ultima coluna apre-
senta o GroundTruth. Foi utilizada como peso para o potencial local
ğ‘¤ğ‘™ como sendo 0.4 para ambas as abordagens. Na primeira coluna foi
possÃ­vel perceber um resultado melhor para o CRF devido a uma maior
suavizaÃ§Ã£o local. Na segunda linha, o mÃ©todo de GeoestatÃ­stica obteve
melhores resultados devido a suas medidas estatÃ­sticas de longa distÃ¢n-
cia. Na Ãºltima linha Ã© mostrado os resultados para o dataset Marker,
onde ambas as abordagens tiveram melhores resultados para esse caso.

94

Lista de tabelas

Tabela 1 â€“ A quantidade de leite adicionada para cada nÃ­vel de turbidez simulado. 75

Tabela 2 â€“ Matriz de covariÃ¢ncia que mostra as relaÃ§Ãµes de proximidade entre as
classes. Tais medidas sÃ£o fatores que indicam correlaÃ§Ã£o e nÃ£o distri-
buiÃ§Ãµes de probabilidade. Este resultado Ã© normalizado ao final.

. . . . 86

Tabela 3 â€“ Matriz de covariÃ¢ncia que mostra as relaÃ§Ãµes de proximidade entre as
classes. Tais medidas sÃ£o fatores que indicam correlaÃ§Ã£o e nÃ£o distri-
buiÃ§Ãµes de probabilidade. Este resultado Ã© normalizado ao final.

. . . . 87

Tabela 4 â€“ Resultados para a taxa de acerto de diferentes segmentos para o data-
set Redsea. Foram testados diversos segmentos quadrados amostrados
aleatoreamente nos mosaicos. O tamanho do segmento Ã© especificado
pelo lado do quadrado . . . . . . . . . . . . . . . . . . . . . . . . . . . 91

SumÃ¡rio

IntroduÃ§Ã£o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
0.1 MotivaÃ§Ã£o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
0.2 Detectores de Pontos de Interesse . . . . . . . . . . . . . . . . . . . . . . . 24
0.3 ClassificaÃ§Ã£o de Imagens do Assoalho OceÃ¢nico . . . . . . . . . . . . . . . . 24
0.4 SumÃ¡rio desta DissertaÃ§Ã£o . . . . . . . . . . . . . . . . . . . . . . . . . . . 25

1

FundamentaÃ§Ã£o TeÃ³rica 1: Detectores de Pontos de Interesse . . . . . . . 27
1.1 Detectores de Ãšnica Escala . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
1.1.1 Harris . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
1.1.2 Hessian e Laplacian . . . . . . . . . . . . . . . . . . . . . . . . . . 30
1.1.3 ComparaÃ§Ã£o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
1.2 Detectores Invariantes a Escala . . . . . . . . . . . . . . . . . . . . . . . . 31
. . . . . . . . . . . . . . . . . . 32
1.2.1 Hessian-Laplace e Hessian-Laplace
1.2.2 Diference-of-Gaussians(DoG)
. . . . . . . . . . . . . . . . . . . . . 33
1.2.3 Fast Hessian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
1.2.4 Center Surround Extrema Filters(CenSurE) . . . . . . . . . . . . . 35
1.2.5 KAZE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
1.2.6 ComparaÃ§Ã£o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36

2.2

2 FundamentaÃ§Ã£o TeÃ³rica 2: ClassificaÃ§Ã£o de Imagens Utilizando Contexto . 39
2.1 UtilizaÃ§Ã£o do Contexto em VisÃ£o Computacional . . . . . . . . . . . . . . . 39
2.1.1 NÃ­veis de Contexto . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
2.1.2
InteraÃ§Ãµes de Contexto . . . . . . . . . . . . . . . . . . . . . . . . . 40
IntegraÃ§Ã£o de Contexto Na ClassificaÃ§Ã£o . . . . . . . . . . . . . . . . . . . 41
Integrando contexto com base em Classificadores . . . . . . . . . . 42
2.2.1
Integrando contexto com base em Modelos ProbabilÃ­sticos GrÃ¡ficos
2.2.2
42
2.2.2.1 O Problema da InferÃªncia EstatÃ­stica . . . . . . . . . . . . 44
2.2.2.2 Aprendizado de parÃ¢metros . . . . . . . . . . . . . . . . . 45
2.3 Trabalhos utilizando Modelos ProbabilÃ­sticos GrÃ¡ficos
. . . . . . . . . . . 45
2.4 SumÃ¡rio . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47

3 ClassificaÃ§Ã£o Baseada em Contexto utilizando GeoestatÃ­stica

. . . . . . . 49
3.1 VisÃ£o Geral da Proposta . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
3.2 NÃ­vel UnÃ¡rio ğ‘ƒğ‘¢(ğ¿|ğœƒğ‘¢)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
Classificador
. . . . . . . . . . . . . . . . . . . 51
Treinando Curvas de ConfianÃ§a

3.2.1
3.2.2

3.4.1 Medindo TransiÃ§Ãµes de Probabilidades

3.3 DistribuiÃ§Ã£o de Probabilidades . . . . . . . . . . . . . . . . . . . . . . . . . 53
3.4 NÃ­vel Local ğ‘ƒğ‘™(ğ¿|ğ‘Š)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
. . . . . . . . . . . . . . . . 54
Taxa de TransiÃ§Ã£o Medida ğ‘…ğ‘šğ‘’ğ‘ 
. . . . . . . . . . . . . . 55
3.4.1.1
3.4.1.2
Calculo da Matriz ğ‘…ğ‘šğ‘œğ‘‘ . . . . . . . . . . . . . . . . . . . 57
Sequential Indicator Simulation . . . . . . . . . . . . . . . . . . . . 58
. . . . . . . . . . . . . . . . . 59
3.5 GeoestatÃ­stica e CRF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
3.6 SumÃ¡rio . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61

3.4.2
3.4.3 Computando o Potencial Final ğ‘ƒ(ğ¿)

4 ClassificaÃ§Ã£o de Imagens do Assoalho OceÃ¢nico

. . . . . . . . . . . . . . . 63
4.1 Propriedades de Imagens SubaquÃ¡ticas . . . . . . . . . . . . . . . . . . . . 63
4.2 ClassificaÃ§Ã£o AutÃ´noma de Imagens do fundo OceÃ¢nico . . . . . . . . . . . 66
4.2.1 PrÃ©-Processamento . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
4.2.1.1 Contraste . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
4.2.1.2 CorreÃ§Ã£o de Cor
. . . . . . . . . . . . . . . . . . . . . . . 68
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
SegmentaÃ§Ã£o
4.2.2
4.2.3 Descritores
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
4.2.4 Treinamento e ClassificaÃ§Ã£o . . . . . . . . . . . . . . . . . . . . . . 70
4.3 ConclusÃµes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70

5 Testes e Resultados 1: DetecÃ§Ã£o de Pontos de Interesse em Ambiente

SubaquÃ¡tico . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
5.1 DescriÃ§Ã£o do experimento . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
5.1.1 Cena Montada
5.1.2 Procedimento . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
5.2 Avaliando a degradaÃ§Ã£o causada pela turbidez . . . . . . . . . . . . . . . . 75
5.3 Resultados . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
5.3.1 Procedimento de AvaliaÃ§Ã£o . . . . . . . . . . . . . . . . . . . . . . . 77
5.3.2 ComparaÃ§Ã£o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80

5.4 ConclusÃµes finais

6 Testes e Resultados 2: Contexto em ClassificaÃ§Ã£o SubaquÃ¡tica . . . . . . . 83
6.1 Datasets Utilizados . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
6.2 DescriÃ§Ã£o do Geral do Sistema . . . . . . . . . . . . . . . . . . . . . . . . 83
6.2.1 PrÃ©-Processamento . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
6.2.2
. . . . . . . . . . . . . . . . . . . . . . . 84
6.2.3 ClassificaÃ§Ã£o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
6.2.4 AdiÃ§Ã£o de Contexto . . . . . . . . . . . . . . . . . . . . . . . . . . 84
6.3 Treinamento . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84

SegmentaÃ§Ã£o e DescriÃ§Ã£o

. . . . . . . . . . . . . . . . . . . . . 84
6.3.1 Treinamento do Classificador
6.3.2 Treinamento UnÃ¡rio
. . . . . . . . . . . . . . . . . . . . . . . . . . 85
6.3.3 Treinamento Potenciais Locais . . . . . . . . . . . . . . . . . . . . . 86
6.4
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
Sistemas Testados
6.5 ComputaÃ§Ã£o do Mapa TemÃ¡tico . . . . . . . . . . . . . . . . . . . . . . . . 90
6.6 ConclusÃµes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91

7 ConclusÃµes Finais . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
7.1 Detectores de Pontos de Interesse em Imagens SubaquÃ¡ticas Turvas . . . . 95
7.1.1 ContribuiÃ§Ãµes Obtidas
. . . . . . . . . . . . . . . . . . . . . . . . . 95
7.1.2 LimitaÃ§Ãµes e Trabalhos Futuros . . . . . . . . . . . . . . . . . . . . 95
7.2 AdiÃ§Ã£o de Contexto Baseado em GeoestatÃ­stica . . . . . . . . . . . . . . . 96
7.2.1 ContribuiÃ§Ãµes Obtidas
. . . . . . . . . . . . . . . . . . . . . . . . . 96
7.2.2 LimitaÃ§Ãµes e Trabalhos Futuros . . . . . . . . . . . . . . . . . . . . 96

ReferÃªncias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97

IntroduÃ§Ã£o

23

Este trabalho apresenta um estudo sobre tÃ©cnicas de visÃ£o computacional consi-
derando os aspectos fotomÃ©tricos do meio subaquÃ¡tico. Dois pontos chaves no processo
sÃ£o analisados: A extraÃ§Ã£o autÃ´noma de pontos de interesse; e a utilizaÃ§Ã£o do contexto
espacial para a classificaÃ§Ã£o.

0.1 MotivaÃ§Ã£o

VisÃ£o computacional Ã© a ciÃªncia que visa possibilitar Ã s mÃ¡quinas a capacidade de

interpretaÃ§Ã£o e representaÃ§Ã£o de informaÃ§Ãµes visuais.

Com tal capacidade, diversas aplicaÃ§Ãµes podem ser entÃ£o desenvolvidas, como: a
inspeÃ§Ã£o industrial autÃ´noma , a reconstruÃ§Ã£o de cenas em trÃªs dimensÃµes, a localizaÃ§Ã£o
de robÃ´s, a rotulaÃ§Ã£o ou classificaÃ§Ã£o de objetos em imagens, entre outras. Tais aplicaÃ§Ãµes
podem ser implementadas nos mais diversos ambientes, desde o domÃ­nio industrial ou
domÃ©stico, em ambientes fechados ou abertos, em localidades sobre a terra ou no fundo
do mar, etc.

Neste trabalho se dÃ¡ atenÃ§Ã£o especial ao ambiente marinho, o qual, cobrindo em
torno de 70% da terra, e contendo cerca de 90% de sua biodiversidade, Ã© de evidente
importÃ¢ncia. O advento dos Remotely Operated Vehicles(ROV ) e dos Autonomous Un-
derwater Vehicles (AUVs), tem possibilitado a coleta de milhares de imagens para moni-
toramento do oceano, ampliando as possibilidades de aplicaÃ§Ãµes em visÃ£o computacional
em ambientes subsea

Diversas das aplicaÃ§Ãµes para visÃ£o computacional em terra podem ser facilmente
extrapoladas para utilizaÃ§Ã£o no meio subaquÃ¡tico. Como exemplo, tem-se o caso da clas-
sificaÃ§Ã£o autÃ´noma de imagens do assoalho oceÃ¢nico. Para tal aplicaÃ§Ã£o, tem-se o caso dos
recifes de corais, os quais desde 1980 sofrem de massivas perdas devido a poluiÃ§Ã£o, pesca
excessiva e espÃ©cies invasivas (NEMETH et al., 2008). Uma classificaÃ§Ã£o autÃ´noma Ã© fun-
damental, dada a grande Ã¡rea monitorada e a necessidade de reduzir o tempo necessÃ¡rio
de especialistas para se classificar as espÃ©cies.

O monitoramento Ã© tambÃ©m uma realidade que gera demanda para sistemas robÃ³-
ticos autÃ´nomos. Isso gera margem para utilizaÃ§Ã£o de sistemas visuais nas mais diversas
aplicaÃ§Ãµes como: a localizaÃ§Ã£o de robÃ´s subaquÃ¡ticos, a inspeÃ§Ã£o e rastreio de risers e
flows na indÃºstria de Ã³leo e gÃ¡s, entre outras.

Devido a dificuldade de desenvolvimento e instrumentaÃ§Ã£o subsea, o uso de ROVs
e AUVs Ã© recente, implicando em uma limitada quantidade de estudos relacionados ao

24

IntroduÃ§Ã£o

domÃ­nio da visÃ£o computacional para o ambiente subaquÃ¡tico. Nesse ambiente existem
desafios especÃ­ficos que nÃ£o necessitam ser tratados em outros ambientes. A propagaÃ§Ã£o da
luz em meio subaquÃ¡tico apresenta efeitos fotomÃ©tricos associados o que causa degradaÃ§Ã£o
na formaÃ§Ã£o da imagem. Efeitos como borramento, espalhamento da informaÃ§Ã£o luminosa
e atenuaÃ§Ã£o de cor na imagem, sÃ£o alguns exemplos que precisam ser considerados em
aplicaÃ§Ãµes subaquÃ¡ticas

AlÃ©m disso, quando se trata das cenas capturadas em tais ambientes, a monotoni-
cidade do ambiente, dada pela falta de diversificaÃ§Ã£o dos objetos e a falta de estruturas
geomÃ©tricas bem definidas, muito causada pela erosÃ£o, dificulta a interpretaÃ§Ã£o visual, o
que por sua vez, acarreta no aumento da complexidade das aplicaÃ§Ãµes em visÃ£o compu-
tacional.

Neste contexto, o objetivo deste trabalho Ã© analisar duas aplicaÃ§Ãµes fundamentais
para visÃ£o computacional em meio subaquÃ¡tico: a detecÃ§Ã£o de pontos de interesse e a
consideraÃ§Ã£o de contexto para classificaÃ§Ã£o de grande extensÃµes de mosaicos de imagens
do assoalho oceÃ¢nico.

0.2 Detectores de Pontos de Interesse

A detecÃ§Ã£o de pontos de interesse Ã© de fundamental importÃ¢ncia para diversas
Ã¡reas fundamentais de uso no meio subaquÃ¡tico, como a classificaÃ§Ã£o de imagens (PAD-
MAVATHI; MUTHUKUMAR; THAKUR, 2010), reconstruÃ§Ã£o 3D (BEALL et al., 2010)
(NICOSEVICI et al., 2009), localizaÃ§Ã£o de robÃ´s (AULINAS et al., 2011) , etc.

Este trabalho propÃµe um novo dataset de imagens subaquÃ¡ticas, o qual Ã© usado
para apontar os detectores de pontos de interesse mais adaptados ao meio subaquÃ¡tico. Tal
dataset deve ser capaz de isolar a degradaÃ§Ã£o causada pelo comportamento da propagaÃ§Ã£o
da luz em meio subaquÃ¡ticos como a principal fonte de degradaÃ§Ã£o.

SerÃ£o testados detectores, considerando diversos paradigmas para detecÃ§Ã£o, com
respeito a sua robustez a degradaÃ§Ã£o das imagens subaquÃ¡ticas. Um especial tratamento
serÃ¡ dado aos detectores invariantes a escala por sua comprovada baixa performance neste
meio (GARCIA; GRACIAS, 2011).

0.3 ClassificaÃ§Ã£o de Imagens do Assoalho OceÃ¢nico

A outra aplicaÃ§Ã£o analisada diz respeito ao uso do contexto espacial, muito pouco
utilizado para classificaÃ§Ã£o de imagens subaquÃ¡ticas e, de fundamental importÃ¢ncia, a
medida que grandes extensÃµes passam a ser monitoradas.

Na classificaÃ§Ã£o de grandes extensÃµes de recifes de corais, por exemplo, Ã© natural

0.4. SumÃ¡rio desta DissertaÃ§Ã£o

25

que as diferentes espÃ©cies possam estar inseridas dentro de um contexto. A utilizaÃ§Ã£o da
informaÃ§Ã£o de contexto pode auxiliar a interpretaÃ§Ã£o da cena (BAR, 2004). Assim, analisa-
se diversos algoritmos para a utilizaÃ§Ã£o de contexto em situaÃ§Ãµes e cenÃ¡rios genÃ©ricos.

Com isso, este trabalho tem como objetivo tambÃ©m propor um novo algoritmo
para adiÃ§Ã£o de contexto inspirado em GeoestatÃ­stica, Ã¡rea que modela a variabilidade de
grandezas no espaÃ§o geomÃ©trico, de tal forma o mÃ©todo proposto seja capaz de mitigar a
falta de informaÃ§Ã£o anotada no meio subaquÃ¡tico.

0.4 SumÃ¡rio desta DissertaÃ§Ã£o

O CapÃ­tulo 1 apresenta a fundamentaÃ§Ã£o teÃ³rica sobre a detecÃ§Ã£o de pontos de
interesse. Primeiramente, uma definiÃ§Ã£o formal de pontos de interesse Ã© realizada e suas
principais caracterÃ­sticas desejadas sÃ£o apontadas. SÃ£o apresentadas as definiÃ§Ãµes dos
detectores Harris e Hessian, Harris-Laplace, Hessian-Laplace, Difference of Gaussians
(DoG), FastHessian, CenSurE e KAZE.

O CapÃ­tulo 2 apresenta o problema de classificaÃ§Ã£o de imagens usando o contexto.
Uma definiÃ§Ã£o de como representar as relaÃ§Ãµes de contexto em uma imagem Ã© apresentado.
Em seguida Ã© feita uma revisÃ£o dos mÃ©todos de visÃ£o computacional os quais incorporam
esses conceitos. Um destaque Ã© dado aos mÃ©todos que utilizam os Conditional Random
Field (CRF) para incorporar o contexto.

No CapÃ­tulo 3 Ã© apresentado um sistema de classificaÃ§Ã£o de imagens baseado em
GeoestatÃ­stica, a qual Ã© uma Ã¡rea da estatÃ­stica que busca modelar a variabilidade espacial
de uma determinada grandeza. Nesse capÃ­tulo Ã© proposta uma extensÃ£o deste conceito
para adiÃ§Ã£o de informaÃ§Ã£o de contexto na classificaÃ§Ã£o de imagens.

O CapÃ­tulo 4 apresenta os problemas existentes na classificaÃ§Ã£o de imagens su-
baquÃ¡ticas e tambÃ©m, os aspectos especiais que existem para aplicaÃ§Ãµes em visÃ£o su-
baquÃ¡tica. TambÃ©m se apresenta uma breve revisÃ£o dos resultados jÃ¡ obtidos na classifi-
caÃ§Ã£o de mosaicos do bentos sem utilizaÃ§Ã£o do contexto e, tambÃ©m, uma visÃ£o geral do
sistema proposto por Shihavuddin et al. (2013) o qual foi usado como base para os testes
e resultados.

O CapÃ­tulo 5 apresenta um estudo sobre o comportamento dos detectores apresen-
tados no CapÃ­tulo 1 quanto a sua robustez Ã  degradaÃ§Ã£o causada em imagens subaquÃ¡ticas.
Ã‰ especificado um experimento realizado para geraÃ§Ã£o de diferentes nÃ­veis de degradaÃ§Ã£o
nas imagens. Por fim, sÃ£o apresentados os detectores mais adaptados ao meio.

O CapÃ­tulo 6 apresenta um estudo de caso da aplicaÃ§Ã£o de contexto para classifi-
caÃ§Ã£o no meio subaquÃ¡tico. Os resultados gerados para o novo mÃ©todo proposto baseado
em GeoestatÃ­stica sÃ£o mostrados, e tambÃ©m a sua comparaÃ§Ã£o com os demais mÃ©todos

26

IntroduÃ§Ã£o

do estado da arte em adiÃ§Ã£o de contexto.

Por fim, no CapÃ­tulo 7 as conclusÃµes deste trabalho sÃ£o apresentadas.

1 FundamentaÃ§Ã£o TeÃ³rica 1: Detectores de

Pontos de Interesse

27

Este capÃ­tulo apresenta a fundamentaÃ§Ã£o teÃ³rica sobre detectores de pontos de
interesse. Formalmente pontos de interesse sÃ£o definidos como um padrÃ£o de uma ima-
gem que difere de sua vizinhanÃ§a imediata (TUYTELAARS; MIKOLAJCZYK, 2008).
Normalmente sÃ£o pontos com particularidades de uma imagem as quais possuem alguma
caracterÃ­stica visual relevante. Vale notar que, apesar do termo utilizado ser "pontos de
interesse", nÃ£o Ã© utilizada a definiÃ§Ã£o matemÃ¡tica de um ponto infinitesimal sendo de-
finidos como pequenas regiÃµes. Pontos de interesse servem como Ã¢ncoras de regiÃµes da
imagem, determinando quais posiÃ§Ãµes podem ser descritas para se ter uma representaÃ§Ã£o
confiÃ¡vel da mesma. Distintas aplicaÃ§Ãµes fazem uso dos pontos de interesse como: a clas-
sificaÃ§Ã£o de imagens (PADMAVATHI; MUTHUKUMAR; THAKUR, 2010), reconstruÃ§Ã£o
3D (BEALL et al., 2010) (NICOSEVICI et al., 2009), mapeamento e localizaÃ§Ã£o (GIL et
al., 2010) , rastreio (CORKE et al., 2007), etc.

Um exemplo de pontos de interesse seriam as quinas, as quais sÃ£o responsÃ¡veis
por boa parte do processo de interpretaÃ§Ã£o visual de um objeto (TUYTELAARS; MIKO-
LAJCZYK, 2008). Muitas vezes, os pontos de interesse apresentam uma relaÃ§Ã£o semÃ¢ntica
mais estreita com a aplicaÃ§Ã£o. Por exemplo, ao classificar faces, as regiÃµes do olho ou da
boca sÃ£o de grande interesse para a classificaÃ§Ã£o.

A utilizaÃ§Ã£o de pontos de interesse locais traz as seguintes vantagens, em contraste

com o uso do contexto geral da imagem:

âˆ™ ReduÃ§Ã£o significativa do custo computacional;
âˆ™ Descarte de parte do ruÃ­do presente na imagem pois somente os pontos relevantes

sÃ£o utilizados;

âˆ™ ObtenÃ§Ã£o e uso de apenas caracterÃ­sticas mais distintas da imagem;
âˆ™ Possibilidade de reconhecimento de cenas sem a necessidade de segmentaÃ§Ã£o.

PorÃ©m, para um ponto de interesse ser eficaz, a presenÃ§a de algumas propriedades
sÃ£o de fundamental importÃ¢ncia (TUYTELAARS; MIKOLAJCZYK, 2008). Entre as mais
importantes tem-se:

âˆ™ Repetibilidade: Um ponto de interesse deve representar caracterÃ­sticas que possam
ser encontradas em determinados objetos, independente da configuraÃ§Ã£o em que tal

28

CapÃ­tulo 1. FundamentaÃ§Ã£o TeÃ³rica 1: Detectores de Pontos de Interesse

objecto foi fotografado. Dado duas imagens de um mesmo objeto ou cena, o que foi
visto em ambas as cenas deve ser detectado como ponto de interesse em ambas as
cenas.

âˆ™ Distintividade: Um ponto de interesse deve representar caracterÃ­sticas que sejam
distintas, com destaque sobre as demais caracterÃ­sticas e que sejam especificas de
um determinado objeto. SÃ³ assim este objeto pode ser descriminado com relaÃ§Ã£o
aos demais.

A repetibilidade, de fato a propriedade mais importante (TUYTELAARS; MI-
KOLAJCZYK, 2008), pode ser atingida tendo os pontos de interesse sendo invariantes a
determinadas transformaÃ§Ãµes que uma imagem pode sofrer, tais como:

âˆ™ RotaÃ§Ã£o: Um ponto que pertence a uma cena, deve ser encontrado independente da

orientaÃ§Ã£o que a cena foi capturada.

âˆ™ TranslaÃ§Ã£o: Se o ponto representa o mesma objeto, o mesmo deve ser encontrado

independente da posiÃ§Ã£o na imagem onde ele foi capturado.

âˆ™ Escala: Independente da distÃ¢ncia em que a cena foi capturada, o mesmo ponto

deverÃ¡ ser encontrado.

Para outras transformaÃ§Ãµes que a imagem possa sofrer, muitas vezes Ã© interessante
que um ponto seja somente robusto. Ou seja, capaz de ser encontrado somente atÃ© um
determinado nÃ­vel da transformaÃ§Ã£o. Alguns efeitos, ou transformaÃ§Ãµes, a se ter robustez
sÃ£o: efeitos de discretizaÃ§Ã£o, artefatos causados por compressÃ£o, borramento devido a
movimento, ruÃ­do branco, distorÃ§Ã£o de perspectiva, etc.

Diversos algoritmos sÃ£o desenvolvidos para encontrar pontos os quais apresentam
as propriedades acima descritas. SÃ£o eles chamados os Detectores de Pontos de Interesse.
Os Detectores sÃ£o desenvolvidos de forma a terem um valor de retorno alto em relaÃ§Ã£o a
certas estruturas presentes na imagens. Define-se estrutura como um determinado padrÃ£o
com respeito a variaÃ§Ã£o de intensidade luminosa em uma regiÃ£o da imagem.

Divide-se os Detectores de Pontos de Interesse com respeito as determinadas pro-

priedades as quais os mesmos possuem invariÃ¢ncia.

Neste trabalho, selecionou-se nove detectores principais encontrados na bibliografia
a serem fundamentados. Primeiramente, na seÃ§Ã£o 1.1, sÃ£o expostos os detectores capazes
de responder a estruturas possuindo invariÃ¢ncia a rotaÃ§Ã£o e translaÃ§Ã£o. Tais detectores
sÃ£o chamados tambÃ©m de detectores de Ãºnica escala pois, nÃ£o possuindo invariÃ¢ncia a
escala, somente analisam a imagem em uma Ãºnica escala.

1.1. Detectores de Ãšnica Escala

29

Por fim, na seÃ§Ã£o 1.2, sÃ£o apresentados detectores que convivem tambÃ©m com a
invariÃ¢ncia a escala. Estes simulam mÃºltiplas escalas de forma a encontrar pontos invari-
antes a escala. Tais detectores sÃ£o chamados de detectores de mÃºltipla escala.

1.1 Detectores de Ãšnica Escala

Os detectores apresentados nessa seÃ§Ã£o possuem invariÃ¢ncia a translaÃ§Ã£o ou rota-
Ã§Ã£o, podendo possuir em algum nÃ­vel, tambÃ©m invariÃ¢ncia a escala. Os detectores apre-
sentados podem tambÃ©m ter robustez a diversos tipos de ruÃ­do.

Normalmente um detector Ã© implementado como uma funÃ§Ã£o, ou kernel, o qual Ã©
convoluida com a imagem e produz uma imagem de saÃ­da a qual apresenta o resultado
da aplicaÃ§Ã£o deste kernel.

Como jÃ¡ explicado, existem diversas caracterÃ­sticas em uma imagem a serem bus-
cadas como pontos de interesse. Neste trabalho, tanto para o caso de Ãºnica, como de
mÃºltipla escala, seleciona-se caracterÃ­sticas baseadas na alta curvatura de uma regiÃ£o,
calculada atravÃ©s do gradiente da imagem. Duas estruturas sÃ£o escolhidas, quinas e blobs.
Ambas sÃ£o bastante utilizadas como pontos de interesse pelos detectores mais populares.
As quinas, sÃ£o estruturas que nÃ£o necessariamente representam uma quina de
fato. SÃ£o estruturas as quais possuem gradientes de alta intensidade em pelo menos duas
direÃ§Ãµes distintas.

Blobs sÃ£o definidos como regiÃµes que sÃ£o diferentes em intensidade da regiÃ£o ao re-
dor. Normalmente sÃ£o associados com algum ponto de extremo na intensidade da imagem
(LINDEBERG; EKLUNDH, 1991).

1.1.1 Harris

O detector Harris (HARRIS; STEPHENS, 1988) Ã© um dos mais populares detec-
tores de quinas encontrados na literatura. Uma quina Ã© detectada quando existir variaÃ§Ã£o
em duas direÃ§Ãµes principais de uma funÃ§Ã£o analÃ­tica de auto-correlaÃ§Ã£o na imagem. Tal
funÃ§Ã£o indica a variaÃ§Ã£o de intensidade em todas as direÃ§Ãµes para uma imagem ğ¼(ğ‘¥, ğ‘¦) e
pode ser definida pela equaÃ§Ã£o 1.1.

â¡â£

ğ‘€ = ğœ2

ğ·ğ‘”(ğœğ¼) *

onde:

(1.1)

(1.2)

ğ‘¥(ğ‘¥, ğ‘¦, ğœğ·)
ğ¼2

ğ¼ğ‘¥(ğ‘¥, ğœğ·)ğ¼ğ‘¦(ğ‘¥, ğ‘¦, ğœğ·)

ğ¼ğ‘¥(ğ‘¥, ğ‘¦, ğœğ·)ğ¼ğ‘¦(ğ‘¥, ğ‘¦, ğœğ·)

ğ‘¦(ğ‘¥, ğ‘¦, ğœğ·)
ğ¼2

â¤â¦

ğ¼ğ‘¥(ğ‘¥, ğ‘¦, ğœğ·) = ğœ•
ğœ•ğ‘¥

ğ‘”(ğœğ·) * ğ¼(ğ‘¥, ğ‘¦)

30

CapÃ­tulo 1. FundamentaÃ§Ã£o TeÃ³rica 1: Detectores de Pontos de Interesse

e ğ‘” Ã© uma funÃ§Ã£o gaussiana definida por:

ğ‘”(ğœ) = 1

2ğœ‹ğœ2 ğ‘’

âˆ’ ğ‘¥2+ğ‘¦2
2ğœ2

(1.3)

A quina pode ser computada por uma anÃ¡lise dos autovalores da matriz ğ‘€ .
Quando os dois autovalores tiverem valores altos, isso indica a existÃªncia de uma quina.
Uma maneira de medir a intensidade dos autovalores sem a necessidade de computar os
autovalores de ğ‘€ diretamente, Ã© atravÃ©s da medida de Harris dada por:

ğ‘ğ‘–ğ‘› = ğ‘‘ğ‘’ğ‘¡(ğ‘€) âˆ’ ğ‘˜ ğ‘¡ğ‘Ÿğ‘ğ‘ğ‘œ(ğ‘€),

(1.4)

onde ğ‘˜ Ã© uma constante normalmente setada entre 0.04 e 0.06. Quando a medida ğ‘ğ‘–ğ‘› da
Eq. 1.4 for alta, a presenÃ§a de quinas tambÃ©m o serÃ¡ (HARRIS; STEPHENS, 1988). Um
ponto da imagem Ã© considerado uma quina, se a saÃ­da da aplicaÃ§Ã£o da Eq. 1.4 for maior
que um limiar ğ‘¡.

Harris jÃ¡ foi avaliado como sendo o detector com melhor repetibilidade quando
comparado com outros detectores de Ãºnica escala (SCHMID; MOHR; BAUCKHAGE,
2000).

1.1.2 Hessian e Laplacian

O detector Hessian, proposto inicialmente por (BEAUDET, 1978), Ã© um mÃ©todo
bastante usado para detecÃ§Ã£o de blobs em imagens. Para uma imagem ğ¼(ğ‘¥, ğ‘¦), os blobs
podem ser calculado pelo determinante da matriz Hessiana:

â¡â£ğ¼ğ‘¥ğ‘¥(ğ‘¥, ğ‘¦, ğœğ·) ğ¼ğ‘¥ğ‘¦(ğ‘¥, ğ‘¦, ğœğ·)
â¤â¦

ğ¼ğ‘¥ğ‘¦(ğ‘¥, ğ‘¦, ğœğ·) ğ¼ğ‘¦ğ‘¦(ğ‘¥, ğ‘¦, ğœğ·)

ğ» =

(1.5)

O determinante responde aos gradientes em mÃºltiplas direÃ§Ãµes da imagem e tende
a revelar blobs de alta curvatura, o que representa uma regiÃ£o distinta. O detector, deter-
minante de Hessian, ou simplesmente detector Hessian Ã© dado selecionando os pontos os
quais tem uma saÃ­da com respeito a matriz H maior que um valor de limiar ğ‘¡.

Uma variaÃ§Ã£o do Hessian Ã© a aplicaÃ§Ã£o de um kernel Laplacian o qual Ã© computado
pelo traÃ§o da matriz ğ» ( ğ¼ğ‘¥ğ‘¥+ğ¼ğ‘¦ğ‘¦). PorÃ©m o Laplacian tende tambÃ©m a responder a bordas
(TUYTELAARS; MIKOLAJCZYK, 2008). Bordas nÃ£o sÃ£o bons pontos de interesse pois,
nÃ£o possuem uma aceitÃ¡vel invariÃ¢ncia a rotaÃ§Ã£o (TUYTELAARS; MIKOLAJCZYK,
2008).

1.2. Detectores Invariantes a Escala

1.1.3 ComparaÃ§Ã£o

31

A Figura 1 mostra um exemplo de aplicaÃ§Ã£o do Hessian e Harris em uma imagem.
As blobs podem ser vistos como a Ã¡rea mais elevada em morros de intensidade. As quinas
podem ser juntas "T"ou "L", tambÃ©m podendo ter formato mais arredondado.

(a) Original

(b) Harris

(c) Hessian

Figura 1 â€“ Comportamento da aplicaÃ§Ã£o dos kernels Hessian e Harris para uma imagem
teste (1a). (1b) mostra a saÃ­da da medida de Harris (Eq. 1.4). (1c) mostra a saÃ­da do
determinante da matriz Hessian ( Eq. 1.5 ) para a imagem teste. Tanto o Hessian como
o Harris tem como saÃ­da as regiÃµes de alta curvatura ( Figura por Sojka (2003)).

Percebe-se que hÃ¡ semelhanÃ§a entre ambos, dado que ambos sÃ£o associados a

regiÃµes de alto gradiente.

1.2 Detectores Invariantes a Escala

A noÃ§Ã£o de escala Ã© crucial na interpretaÃ§Ã£o de uma imagem (LINDEBERG, 1994).
Alguns objetos sÃ³ sÃ£o entidades visuais significativas em uma determinada escala. Sendo
assim, uma modelagem explicita de cada nÃ­vel de escala se torna necessÃ¡rio para o pro-

32

CapÃ­tulo 1. FundamentaÃ§Ã£o TeÃ³rica 1: Detectores de Pontos de Interesse

cessamento (LINDEBERG, 1998). Ou seja, uma imagem nÃ£o mais pode ser representada
como uma matriz ğ¼(ğ‘¥, ğ‘¦) e passa a ter um terceiro componente de escala ğ‘ , sendo assim
determinada como a funÃ§Ã£o ğ¿(ğ‘¥, ğ‘¦, ğ‘ ).

Para gerar o conjunto espaÃ§os de escala ğ¿(ğ‘¥, ğ‘¦, ğ‘ ), pode-se utilizar o princÃ­pio da
difusÃ£o (LINDEBERG, 1994). O qual determina que uma famÃ­lia de escalas ğ¿ pode ser
determinada atravÃ©s da equaÃ§Ã£o da difusÃ£o:

ğœ•ğœğ¿ = 1

2 â–½2 ğ¿ = 1

2(ğœ•ğ‘¥ğ‘¥ + ğœ•ğ‘¦ğ‘¦)ğ¿

(1.6)

O que representa o fato de que, Ã  medida que a escala se torna menos detalhada,

a informaÃ§Ã£o visual tende a se dispersar.

Portanto, para a geraÃ§Ã£o do espaÃ§o de escala de uma imagem ğ¿(ğ‘¥, ğ‘¦, ğ‘ ) deve ser
proposta uma equaÃ§Ã£o que atenda a EquaÃ§Ã£o 1.6. Inicialmente, foi adotado que a funÃ§Ã£o
gaussiana seria a Ãºnica a ser uma soluÃ§Ã£o da equaÃ§Ã£o 1.6. Posteriormente, outras funÃ§Ãµes
foram colocadas como possÃ­veis para geraÃ§Ã£o do espaÃ§o de escala (LINDEBERG, 1997).
Considerando determinada uma escala ğ‘ , definida igual a um parÃ¢metro de difusÃ£o ğœ, a
geraÃ§Ã£o de um espaÃ§o de escala ğœ Ã© dada por:

ğ¿(ğ‘¥, ğ‘¦, ğœ) = ğ‘”(ğ‘¥, ğ‘¦, ğœ) * ğ¼(ğ‘¥, ğ‘¦)

(1.7)

sendo a funÃ§Ã£o gaussiana ğ‘”(ğ‘¥, ğ‘¦, ğœ) calculada como na Eq.1.3.

De forma a atingir a invariÃ¢ncia a escala, os detectores passam a considerar essa
funÃ§Ã£o ğ¿(ğ‘¥, ğ‘¦, ğœ) para se detectar os pontos de interesse. PorÃ©m, (LINDEBERG, 1994) de-
terminou que Ã© possÃ­vel realizar a escolha de uma escala, e tal escala serÃ¡ sempre escolhida
independente do ambiente e sem a necessidade de escolha de parÃ¢metros. Caracterizando
uma escala onde existe invariÃ¢ncia.

Foi sugerido que os pontos de extremo de funÃ§Ãµes gradientes das estruturas en-
tre as escalas tem propriedades invariantes. Isso representa a escala com o mÃ¡ximo de
sensibilidade a funÃ§Ã£o. Tal escala Ã© chamada de escala caracterÃ­stica.

Nesta seÃ§Ã£o apresentam-se alguns detectores invariantes a escala. A ideia de mÃ¡-
ximo de uma determinada funÃ§Ã£o gradiente entre escalas Ã© usada por todos os mÃ©todos
apresentados.

1.2.1 Hessian-Laplace e Hessian-Laplace

Umas primeiras extensÃµes para detectores de mÃºltipla escala foram feitas para as
as funÃ§Ãµes Harris e Hessian (MIKOLAJCZYK; SCHMID, 2004) Nestes mÃ©todos, o espaÃ§o

1.2. Detectores Invariantes a Escala

33

de escala Ã© gerado por uma equaÃ§Ã£o gaussiana tal como na Eq. 1.7. Os pontos de extremo
entre um conjunto de escalas ğœğ‘› sÃ£o computados conforme a Eq. 1.8.

|ğ¿ğ‘œğº(ğ‘¥, ğ‘¦, ğœğ‘›)| = ğœ2|ğ¿ğ‘¥ğ‘¥(ğ‘¥, ğ‘¦, ğœğ‘›) + ğ¿ğ‘¦ğ‘¦(ğ‘¥, ğ‘¦, ğœğ‘›)|

(1.8)

sendo a Eq. 1.8 uma representaÃ§Ã£o da funÃ§Ã£o Laplacian em mÃºltiplas escalas. Desta forma
, sÃ£o selecionados os pontos extremos que tem alta reposta a funÃ§Ã£o Harris, para o caso
do Harris-Laplace ou da funÃ§Ã£o Hessian para o caso do Hessian-Laplace.

1.2.2 Diference-of-Gaussians(DoG)

O detector DoG Ã© uma otimizaÃ§Ã£o a aplicaÃ§Ã£o do Hessian-Laplace. Ã‰ o detector

proposto pelo mÃ©todo SIFT (LOWE, 2004).

Ao invÃ©s de computar o Laplacian para cada escala, neste aplica-se o Laplacian
pela diferenÃ§a, ğ·(ğ‘¥, ğ‘¦, ğœ), entre mÃºltiplos nÃ­veis do espaÃ§o gaussiano ğ¿(ğ‘¥, ğ‘¦, ğœ). Sendo
assim:

ğ·(ğ‘¥, ğ‘¦, ğœ) = ğ¿(ğ‘¥, ğ‘¦, ğ‘˜ğœ) âˆ’ ğ¿(ğ‘¥, ğ‘¦, ğœ)

(1.9)

Diversos nÃ­veis de escala sÃ£o gerados. A cada determinado nÃºmero de imagens,
chamado oitava, Ã© feito um redimensionamento na imagem. Dentro de uma oitava, as
imagens diferentes sÃ£o criadas pela aplicaÃ§Ã£o do filtro gaussiano. A funÃ§Ã£o ğ· Ã© gerada a
partir da diferenÃ§a entre nÃ­veis vizinhos. O processo utilizado pelo algoritmo Ã© mostrado
na Figura 2.

Para se encontrar a escala caracterÃ­stica, basta encontrar o mÃ¡ximo na funÃ§Ã£o
ğ·(ğ‘¥, ğ‘¦, ğœ) variando ğœ. Ao final, os extremos do espaÃ§o, os quais tem baixa resposta Ã 
funÃ§Ã£o Hessian sÃ£o eliminados.

1.2.3 Fast Hessian

Trata-se de um mÃ©todo que busca fazer uma otimizaÃ§Ã£o ainda maior em relaÃ§Ã£o ao
DoG para geraÃ§Ã£o do espaÃ§o de escala (BAY et al., 2008). Trata-se de um filtro que nÃ£o usa
o filtro gaussiano para geraÃ§Ã£o do espaÃ§o de escala . Os filtros gaussianos sÃ£o aproximados
por filtros caixas. Um filtro caixa basicamente computa a mÃ©dia de uma imagem dado
uma janela de convoluÃ§Ã£o, podendo ser computado rapidamente pela utilizaÃ§Ã£o de Imagens
Integrais (DERPANIS; LEUNG; SIZINTSEV, 2007).

Ã‰ possÃ­vel neste caso fazer a abordagem de diferenÃ§a de caixas, o que permite
juntar a aplicaÃ§Ã£o do filtro Hessian com a geraÃ§Ã£o do espaÃ§o de escala. Uma aproximaÃ§Ã£o
do Hessian jÃ¡ Ã© computada diretamente ao se aplicar as diferenÃ§as de caixas.

34

CapÃ­tulo 1. FundamentaÃ§Ã£o TeÃ³rica 1: Detectores de Pontos de Interesse

Figura 2 â€“ O processo para geraÃ§Ã£o do espaÃ§o de escala pelo DoG. Ao invÃ©s de computar o
Laplacian para cada escala, o mesmo Ã© estimado pela diferenÃ§a entre escalas consecutivas.
Figura adaptada de (LOWE, 2004).

Para um determinado tamanho de caixa de aresta ğ‘, a resposta do Hessian Ã© dada

por:

ğ‘‘ğ‘’ğ‘¡(ğ»ğ‘ğ‘ğ‘ğ‘Ÿğ‘œğ‘¥) = ğ·ğ‘¥ğ‘¥ğ·ğ‘¦ğ‘¦ âˆ’ (0.9ğ·ğ‘¥ğ‘¦)2

(1.10)

A Figura 22 mostra um exemplo de filtros ğ·ğ‘¥ğ‘¥ e ğ·ğ‘¥ğ‘¦ que sÃ£o aplicados. Enquanto
aplicar a diferenÃ§a entre caixas, produz o Hessian, a computaÃ§Ã£o em blocos aplica a
difusÃ£o na imagem.

(a) ğ·ğ‘¥ğ‘¥

(b) ğ·ğ‘¥ğ‘¦

Figura 3 â€“ Exemplo de um filtro caixa de tamanho 9x9 aplicado para geraÃ§Ã£o de um
espaÃ§o de escala equivalente a ğœ = 1.2. Outros espaÃ§os podem ser gerados usando caixas
maiores.

Para relacionar com o espaÃ§o gaussiano, basta saber que uma imagem de filtro
gaussiano ğœ = 1.2 Ã© equivalente a utilizaÃ§Ã£o de um filtro caixa 9x9. EntÃ£o, para geraÃ§Ã£o

1.2. Detectores Invariantes a Escala

35

do espaÃ§o de escala basta gerar a resposta de vÃ¡rios tamanhos de caixa ğ‘ = 9, 11, 13..
etc.

Para encontrar os pontos caracterÃ­sticos basta encontrar o mÃ¡ximo para todos os

nÃ­veis de escala.

1.2.4 Center Surround Extrema Filters(CenSurE)

O Filtro de centro e arredores (AGRAWAL; KONOLIGE; BLAS, 2008) tem uma
abordagem similar a utilizada pelo detector Fast Hessian, porÃ©m realizando a diferenÃ§a
entre mÃºltiplos nÃ­veis como no caso do DoG. Este processo visa tambÃ©m uma aproximaÃ§Ã£o
do Laplacian. Os espaÃ§os de escala sÃ£o criados pela geraÃ§Ã£o de polÃ­gonos de mÃºltiplos
tamanhos. Tal como o filtro caixa, um filtro poligonal representa a media atravÃ©s de uma
janela de convoluÃ§Ã£o.

De maneira similar ao FastHessian, um filtro de polÃ­gono lado ğ‘ = 2 Ã© equivalente
a um espaÃ§o gaussiano de ğœ = 1.88 A Figura 4 mostra alguns tipos de polÃ­gonos usados
para gerar o espaÃ§o de escala. Os polÃ­gonos podem ter estruturas estreladas, poligonais,
entre outras.

(a) Estrela

(b) HexÃ¡gono

(c) Quadrado

Figura 4 â€“ Alguns tipos de filtros utilizados para geraÃ§Ã£o do espaÃ§o de escala pelo Cen-
SurE. O filtro estrela, o filtro hexagonal e o filtro por diferenÃ§a de caixas.

Para gerar o Laplacian uma imagem a qual teve aplicada um filtro maior Ã© sub-

traÃ­da de uma imagem com um polÃ­gono menor aplicado.

O mÃ¡ximo deste espaÃ§o gerado Ã© encontrado como pontos de interesse.
Por fim uma funÃ§Ã£o Harris Ã© aplicada, eliminando os pontos que obtiveram baixa
resposta. Isso segue, pelo fato do Harris ter sido determinado como uma funÃ§Ã£o com
melhor repetibilidade.

1.2.5 KAZE

A ideia do KAZE (ALCANTARILLA; BARTOLI; DAVISON, 2012) Ã© gerar um
espaÃ§o de escala suavizando de maneira diferente em locais com alta intensidade de bordas.
Tal abordagem trata-se de uma difusÃ£o nÃ£o linear.

CapÃ­tulo 1. FundamentaÃ§Ã£o TeÃ³rica 1: Detectores de Pontos de Interesse

A geraÃ§Ã£o do espaÃ§o de escala Ã© dada pela soluÃ§Ã£o para equaÃ§Ã£o da difusÃ£o nÃ£o

36

linear:

= ğ‘‘ğ‘–ğ‘£(ğ‘(ğ‘¥, ğ‘¦, ğ‘¡) â–½ ğ¿),

ğœ•ğ¿
ğœ•ğ‘¡

(1.11)

Abordagens que aplicam uma difusÃ£o nÃ£o linear podem obter resultados melhores
para o caso da segmentaÃ§Ã£o de imagens e remoÃ§Ã£o de ruÃ­do (WEICKERT; ROMENY;
VIERGEVER, 1998).

A ideia principal Ã© que, durante a formaÃ§Ã£o da escala, as bordas das estruturas
devem se manter mais do que de fato acontece com o filtro gaussiano. Desta forma,
primeiramente uma funÃ§Ã£o â–½ğ¼ que responde as bordas Ã© aplicada. Sendo â–½ğ¼ basicamente
um gradiente da imagem. Com base nessa saÃ­da, uma definiÃ§Ã£o alternativa da funÃ§Ã£o
gaussiana Ã© aplicada para geraÃ§Ã£o do espaÃ§o de escala . Perona e Malik (1990), descreveram
algumas possÃ­veis formulaÃ§Ãµes de funÃ§Ãµes:
ğ‘1 = ğ‘’ğ‘¥ğ‘(| â–½ ğ¼ğœ|2
ğ‘˜2

), ğ‘2 = ğ‘’ğ‘¥ğ‘(

(1.12)

1

)

1 + |â–½ğ¼ğœ|2
ğ‘˜2

sendo k um parÃ¢metro que controla o nÃ­vel de difusÃ£o. Alcantarilla, Bartoli e Davison
(2012) propÃ´s uma terceira formulaÃ§Ã£o de kernel:

â§â¨â©

ğ‘3 =

1
1 âˆ’ ğ‘’ğ‘¥ğ‘(
(|â–½ğ¼ğœ|/ğ‘˜)8 )

3.315

,| â–½ ğ¼ğœ|2 = 0
,| â–½ ğ¼ğœ|2 > 0

(1.13)

Levando em conta os kernels definidos, cada nÃ­vel do espaÃ§o de escala ğ¿ğ‘˜(ğ‘¥, ğ‘¦, ğ‘¡)

Ã© gerado pela aplicaÃ§Ã£o da seguinte funÃ§Ã£o recursiva:

ğ¿ğ‘˜(ğ‘¥, ğ‘¦, ğ‘¡ + 1) = (ğ¼ âˆ’ (ğ‘¡ğ‘– + 1 âˆ’ ğ‘¡ğ‘–).ğ‘(ğ‘¥, ğ‘¦, ğ‘¡) * ğ¿ğ‘˜(ğ‘¥, ğ‘¦, ğ‘¡))âˆ’1ğ¿ğ‘˜(ğ‘¥, ğ‘¦, ğ‘¡)

(1.14)

onde ğ‘¡ Ã© um parÃ¢metro de escala temporal facilmente relacionado a ğœ. Ao final, tambÃ©m
sÃ£o desconsideradas as regiÃµes que tem baixa resposta a aplicaÃ§Ã£o de uma matriz Hessian.

1.2.6 ComparaÃ§Ã£o

A Figura 5 apresenta exemplos de geraÃ§Ã£o do espaÃ§o de escala baseado em 4
funÃ§Ãµes diferentes. O gaussiano, utilizado pelo DoG, Harris-Laplace e Hessian-Laplace, a
funÃ§Ã£o de caixas utilizada pelo FastHessian, uma funÃ§Ã£o poligonal estrelar de seis lados,
utilizadas pelo CenSurE e a funÃ§Ã£o de difusÃ£o anisotrÃ³pica utilizado pelo KAZE.

Pode-se perceber que determinadas estruturas se mantem mais que outras para
espaÃ§os diferentes. Claramente algumas aplicaÃ§Ãµes se beneficiariam do uso de um espaÃ§o
de escala diferente. No CapÃ­tulo 5 se estudam os melhores detectores para o campo de
estudo de imagens subaquÃ¡ticas com presenÃ§a de turbidez.

1.2. Detectores Invariantes a Escala

37

ğœ = 3.6

ğœ = 3.6

ğœ = 3.6

ğœ = 3.6

ğœ = 11.6

ğœ = 11.6

ğœ = 11.6

ğœ = 11.6

ğœ = 27.6

ğœ = 27.6

ğœ = 27.6

ğœ = 28.7

ğœ = 59.6

ğœ = 59.6

ğœ = 59.6

ğœ = 57.6

Figura 5 â€“ EspaÃ§os de escala gerados. Primeira coluna mostra o espaÃ§o Gaussiano. Se-
gunda coluna mostra o filtro mÃ©dia de caixas usado pelo FastHessian. Terceira coluna
mostra um filtro poligonal estrelar de seis lados. A quarta Coluna mostra o espaÃ§o de
escala anisotrÃ³pico usado pelo KAZE.

2 FundamentaÃ§Ã£o TeÃ³rica 2: ClassificaÃ§Ã£o de

Imagens Utilizando Contexto

39

Este capÃ­tulo apresenta a fundamentaÃ§Ã£o teÃ³rica utilizada neste trabalho associada
a utilizaÃ§Ã£o de contexto para a classificaÃ§Ã£o de imagens. Inicialmente sÃ£o postuladas as
definiÃ§Ãµes de como representar as relaÃ§Ãµes de contexto em uma imagem. TambÃ©m Ã© feita
a definiÃ§Ã£o de classificaÃ§Ã£o de imagens incorporando o conceito de contexto, bem como
uma revisÃ£o dos mÃ©todos de visÃ£o computacional que os tratam sÃ£o apresentados. Um
destaque Ã© dado aos mÃ©todos que utilizam os Conditional Random Field (CRF).

2.1 UtilizaÃ§Ã£o do Contexto em VisÃ£o Computacional

Existem diversos descritores capazes de discriminar os objetos com base em suas
caracterÃ­sticas visuais, como textura, cor e forma. Tais caracterÃ­sticas buscam capturar a
variabilidade dos objetos para sua classificaÃ§Ã£o (GALLEGUILLOS; BELONGIE, 2010).
PorÃ©m, estudos envolvendo o sistema perceptivo visual humano trazem novas perspectivas
no que tange a como as tÃ­picas configuraÃ§Ãµes dos objetos em uma cena podem contribuir
para a percepÃ§Ã£o, de tal forma que o reconhecimento de objetos no sistema visual humano
considera nÃ£o somente os aspectos locais referentes a interpretaÃ§Ã£o da cena, mas tambÃ©m
a situaÃ§Ã£o geral onde um objeto foi encontrado.

Biederman (BIEDERMAN; MEZZANOTTE; RABINOWITZ, 1982) estabelece al-
guns tipos de relaÃ§Ãµes contextuais importantes que sÃ£o fundamentais para o reconheci-
mento de objetos no sistema visual humano. Estas relaÃ§Ãµes estabelecem nÃ­veis semÃ¢nticos
tais como: i suporte (onde os objetos tendem a sustentar ou ser sustentados por outros),
interposiÃ§Ã£o (relativo a relaÃ§Ãµes de oclusÃ£o), ii probabilidade (objetos tendem a aparecer
na mesma situaÃ§Ã£o), iii posiÃ§Ã£o (objetos tendem a ficar em determinada posiÃ§Ã£o relativa
com outros) e iv tamanho (objetos tendem a ter certo tamanho se comparado com outros
em uma dada escala).

VÃ¡rios modelos computacionais jÃ¡ fizeram uso destas relaÃ§Ãµes semÃ¢nticas as quais
podem ser usadas para classificar objetos. Essas relaÃ§Ãµes normalmente sÃ£o resumidas em
trÃªs tipos de contexto principais: semÃ¢ntico, posiÃ§Ã£o e escala.

O contexto semÃ¢ntico, tende a incluir as relaÃ§Ãµes de ocorrÃªncia entre objetos. Ao
encontrar um determinado objeto em uma cena, o qual se possui certeza de sua pre-
senÃ§a, considera-se uma maior probabilidade de presenÃ§a de outros objetos. Por exemplo,
a existÃªncia de um bule de chÃ¡ implica em uma maior probabilidade de existÃªncia de

40

CapÃ­tulo 2. FundamentaÃ§Ã£o TeÃ³rica 2: ClassificaÃ§Ã£o de Imagens Utilizando Contexto

outros utensÃ­lios de cozinha como talheres ou um fogÃ£o (FISCHLER; ELSCHLAGER,
1973) (HANSON; RISEMAN, 1978). Como exemplo, Rabinovich et al. (2007), incorpo-
rou a informaÃ§Ã£o anotada pelos Google Sets indicando objetos que tendem a aparecer em
situaÃ§Ãµes semelhantes de forma a melhorar a classificaÃ§Ã£o.

O contexto de posiÃ§Ã£o indica que os objetos tendem a ter uma relaÃ§Ã£o espacial
na imagem. Como por exemplo, o cÃ©u em uma imagem tende a estar acima do chÃ£o.
JÃ¡ o contexto de escala esta associado as relaÃ§Ãµes de tamanho entre objetos na cena
(TORRALBA, 2003) (KUMAR; HEBERT, 2005) (TORRALBA; MURPHY; FREEMAN,
2004). Pois, existe jÃ¡, uma relaÃ§Ã£o de tamanho tÃ­pica que os objetos possuem entre eles
(GALLEGUILLOS; BELONGIE, 2010).

Para incluir tais tipos de contexto na classificaÃ§Ã£o de imagens, alguns aspectos
fundamentais devem ser considerados. Primeiramente, qual nÃ­vel de contexto serÃ¡ clas-
sificado. Se as relaÃ§Ãµes a serem consideradas serÃ£o apenas entre objetos prÃ³ximos, ou
no domÃ­nio geral de uma imagem. Ou ainda, por meio de que tipo de estrutura visual
encontrada na imagem, a interaÃ§Ã£o de contexto ocorre.

2.1.1 NÃ­veis de Contexto

Os sistemas que adicionam contexto na classificaÃ§Ã£o normalmente dividem o con-

texto em dois nÃ­veis (GALLEGUILLOS; BELONGIE, 2010): local e global.

O contexto local Ã© onde somente as interaÃ§Ãµes de vizinhanÃ§a sÃ£o utilizadas para
adicionar o contexto a um determinado objeto. O contexto local estÃ¡ relacionado aos
objetos que cercam outros objetos. Vale notar que a aplicaÃ§Ã£o de contexto Ã© recursiva, ou
seja, a prÃ³pria vizinhanÃ§a possuÃ­ tambÃ©m suas prÃ³prias relaÃ§Ãµes de contexto. Isso faz que
nÃ£o somente as relaÃ§Ãµes estritamente prÃ³ximas faÃ§am parte do contexto local.

O contexto global estÃ¡ relacionado as interaÃ§Ãµes de contexto presentes ao longo
de toda a imagem utilizada. O contexto global normalmente estÃ¡ associado ao ambiente
onde os objetos estÃ£o posicionados. Por exemplo, se as relaÃ§Ãµes contextuais indicam que os
objetos estÃ£o em uma cozinha, isso implica em uma alta probabilidade de um dos objetos
ser uma panela.

2.1.2 InteraÃ§Ãµes de Contexto

NÃ£o necessariamente cada componente da imagem deve ser um objeto com um
conceito semÃ¢ntico relacionado. Na literatura se estabelecem trÃªs nÃ­veis bÃ¡sicos de inte-
raÃ§Ã£o nos quais o contexto pode ser integrado (GALLEGUILLOS; BELONGIE, 2010).
AlÃ©m de objetos, as interaÃ§Ãµes tambÃ©m se dÃ£o entre pixeis ou regiÃµes.

A interaÃ§Ã£o em nÃ­vel de pixel estabelece que pixeis vizinhos tendem a ter a mesma
classe. Tais interaÃ§Ãµes ajudam a inferir as bordas existentes na imagem. Vale notar a

2.2.

IntegraÃ§Ã£o de Contexto Na ClassificaÃ§Ã£o

41

utilizaÃ§Ã£o de tais interaÃ§Ãµes sÃ£o mais computacionalmente intensas, dado que existem
diversas combinaÃ§Ãµes entre pixeis da imagem. Ressalta-se que o uso de nÃ­veis mais baixos
de contexto nÃ£o necessariamente implica na perda da informaÃ§Ã£o semÃ¢ntica. Ou seja,
encontrar que pixeis de determinado objeto sÃ£o prÃ³ximos, Ã© tambÃ©m identificar a alta
probabilidade de proximidade de tais objetos.

O conceito de pixel pode ser estendido para o nÃ­vel de representaÃ§Ã£o de regiÃµes. Ao
utilizar regiÃµes, tende-se a reduzir a complexidade levantada pelo grande nÃºmero de pixeis
existentes na imagem. Uma estrutura de regiÃ£o bastante utilizada Ã© a consideraÃ§Ã£o de pe-
quenas regiÃµes adaptadas a estrutura local da imagem. Tais regiÃµes chamadas, superpixeis,
capturam a redundÃ¢ncia dos dados, facilitando a utilizaÃ§Ã£o do contexto (FULKERSON;
VEDALDI; SOATTO, 2009).

JÃ¡ a interaÃ§Ã£o em nÃ­vel de objetos Ã© a representaÃ§Ã£o mais natural para reconhe-
cimento de contexto humano (BAR, 2004). Sabendo-se jÃ¡ a classe do objeto Ã© possÃ­vel
treinar as relaÃ§Ãµes de contexto. No trabalho de (TORRALBA; MURPHY; FREEMAN,
2004), os objetos mais fÃ¡ceis de classificar ajudam, atravÃ©s do contexto, a obter a classe de
objetos mais difÃ­ceis. Se por um lado usar objetos tende a capturar melhor as interaÃ§Ãµes
existentes na cena, o uso do contexto em nÃ­vel de objetos implica jÃ¡ o conhecimento prÃ©-
vio ( classificaÃ§Ã£o) dos objetos existentes na imagem. A interaÃ§Ã£o entre regiÃµes, por outro
lado, ajuda a reduzir a quantidade de combinaÃ§Ãµes existentes na interaÃ§Ã£o de pixeis, sem a
necessidade de um conhecimento maior sobre a imagem (GALLEGUILLOS; BELONGIE,
2010).

2.2 IntegraÃ§Ã£o de Contexto Na ClassificaÃ§Ã£o

Nesta seÃ§Ã£o sÃ£o apresentadas algumas abordagens para integraÃ§Ã£o do contexto na
classificaÃ§Ã£o de imagens. SÃ£o escolhidos mÃ©todos com integraÃ§Ã£o baseada em superpixeis,
com foco para integraÃ§Ã£o local de contexto. Quanto aos tipos de contexto, por considerar
o nÃ­vel de interaÃ§Ã£o como superpixeis, os principais tipos integrados sÃ£o os de posiÃ§Ã£o e
escala.

Dado a representaÃ§Ã£o da imagem como uma matriz ğ‘†ğ‘ƒ(ğ‘¥, ğ‘¦) onde cada elemento
ğ‘ ğ‘ Ã© um superpixel, a definiÃ§Ã£o de classificaÃ§Ã£o Ã© dada pela determinaÃ§Ã£o de um rÃ³tulo
ğ‘™ğ‘– dentre um conjunto possÃ­vel de rÃ³tulos ğ¿ = ğ‘™1, ğ‘™2, ...ğ‘™ğ‘› para cada ğ‘ ğ‘. Para classificaÃ§Ãµes
sem contexto, apenas o superpixel ğ‘ ğ‘ Ã© considerado, jÃ¡ para o caso apresentado nesta
seÃ§Ã£o, a vizinhanÃ§a de ğ‘ ğ‘ Ã© tambÃ©m importante para determinar um rÃ³tulo ğ‘™ğ‘–.

Nesta seÃ§Ã£o sÃ£o especificadas duas formas de incorporar o contexto. Utilizando
a vizinhanÃ§a de um superpixel ğ‘ ğ‘ diretamente no classificador ou atravÃ©s de modelos
probabilÃ­sticos grÃ¡ficos (MPGs).

42

CapÃ­tulo 2. FundamentaÃ§Ã£o TeÃ³rica 2: ClassificaÃ§Ã£o de Imagens Utilizando Contexto

2.2.1

Integrando contexto com base em Classificadores

As informaÃ§Ãµes locais advindas de uma anÃ¡lise de contexto podem ser incorporadas
diretamente aos sistemas de classificaÃ§Ã£o, considerando uma janela de contexto em torno
da regiÃ£o a ser classificada (Figura 6)

Fink e Perona (2003) incorporou o contexto local usando a janela da regiÃ£o para

treinamento de classificador fracos em um esquema de boosting.

Kruppa e Schiele (2003), visando melhorar a classificaÃ§Ã£o de rostos, incorporou a

descriÃ§Ã£o dos descritores da vizinhanÃ§a local da face em um sistema Naive Bayes

O principal problema Ã© que tais aplicaÃ§Ãµes nÃ£o levam em conta as possÃ­veis corre-
laÃ§Ãµes entre os vizinhos. Este problema Ã© demonstrado na Figura 6, a vizinhanÃ§a sÃ³ afeta
o que foi considerado no centro da imagem, sem afetar a si prÃ³pria. Tais problemas sÃ£o
parcialmente resolvidos criando-se interaÃ§Ãµes mais conectadas, como no caso dos modelos
probabilÃ­sticos grÃ¡ficos a serem explicados na prÃ³xima seÃ§Ã£o.

(a)

(b)

Figura 6 â€“ Janela considerada para a classificaÃ§Ã£o usando contexto. No caso da integraÃ§Ã£o
de contexto diretamente nos classificadores (Fig. 6a), nÃ£o sÃ£o considerada as relaÃ§Ãµes entre
a vizinhanÃ§a com si prÃ³pria (Fig. 6b). Ou seja, se existem propriedades correlacionadas
na vizinhanÃ§a.

2.2.2

Integrando contexto com base em Modelos ProbabilÃ­sticos GrÃ¡ficos

Nesta seÃ§Ã£o, serÃ£o apresentados os principais conceitos associados aos Modelos
ProbabilÃ­sticos GrÃ¡ficos (MPGs) e seu uso na integraÃ§Ã£o contextual em classificaÃ§Ã£o de
imagens.

Uma forma natural de representar a dependÃªncia entre variÃ¡veis Ã© utilizando os
Modelos Probabilisticos GrÃ¡ficos (MPGs)(SUTTON; MCCALLUM, 2006). Estes mode-
los representam algumas fatorizaÃ§Ãµes de uma funÃ§Ã£o de probabilidades como o Markov
Random Fields MRF ou Conditional Random Fields CRF.

2.2.

IntegraÃ§Ã£o de Contexto Na ClassificaÃ§Ã£o

43

Um MPG Ã© usado para capturar a correlaÃ§Ãµes existentes dentro de um conjunto de
dados. Baseado neste modelo, Ã© possÃ­vel calcular uma funÃ§Ã£o potencial. Esta abordagem,
quando baseada em modelos probabilÃ­sticos nÃ£o direcionadas, Ã© usada no MRF e no CRF
(SUTTON; MCCALLUM, 2006).

O MRF modela a funÃ§Ã£o de probabilidade ğ‘(ğ‘¦, ğ‘¥) de um dado conjunto de rÃ³tulos
ğ‘¦ e os conjunto de descritores de entrada ğ‘¥. Esse modelo necessita um alto custo compu-
tacional para classificaÃ§Ã£o de imagens (CARBONETTO; FREITAS; BARNARD, 2004).
Ainda, dado que deve seguir a premissa de Markov, nenhuma caracterÃ­stica global deve
ser adicionada. Paro caso de Markov, a computaÃ§Ã£o de ğ‘(ğ‘¦, ğ‘¥) necessita a computaÃ§Ã£o
de ğ‘(ğ‘¦) e tambÃ©m ğ‘(ğ‘¥), o qual nÃ£o se tem conhecimento sobre, pois estÃ¡ relacionado a
probabilidade das descriÃ§Ãµes de entrada aparecerem.

Uma abordagem mais comumente utilizada para classificaÃ§Ã£o de imagens Ã© o mo-
delo CRF. Neste modelo, somente a distribuiÃ§Ã£o condicional, ğ‘(ğ‘¦|ğ‘¥) , Ã© computada. Nor-
malmente o CRF tem uma melhor associaÃ§Ã£o aos dados, dado que nÃ£o Ã© necessÃ¡rio com-
putar a probabilidade a priori para os dados de entrada (ğ‘(ğ‘¥)) (SUTTON; MCCALLUM,
2006).

Considerando um dado modelo, Ã© definida a probabilidade para um conjunto de
rÃ³tulos serem atribuÃ­dos . Esta probabilidade Ã© estabelecida como uma funÃ§Ã£o de um fator
unitÃ¡rio e um fator local. Deste modo, define-se a probabilidade de uma imagem possuir
um certo conjunto de rÃ³tulos ğ¿ = ğ‘™1, ğ‘™2, ...ğ‘™ğ‘› dado um grafo de um modelo ğº e um conjunto
de parÃ¢metros ğœƒğ‘¢ e ğœƒğ‘™ como a Eq. 2.1.

ğ‘™ğ‘œğ‘”ğ‘ƒ(ğ¿|ğº, ğœƒ) = ğ‘¤ğ‘¢

ğ‘¢âˆ‘ï¸

ğ‘¥ğ‘–ğœ–ğ‘‹

ğ‘– (ğ‘¥ğ‘–, ğœƒğ‘¢) + ğ‘¤ğ‘™
ğœ™ğ‘¢

ğœ–âˆ‘ï¸

(ğ‘¥ğ‘–,ğ‘¥ğ‘—)ğœ–ğœ€

ğ‘–ğ‘—(ğ‘¥ğ‘–, ğ‘¥ğ‘—, ğœƒğ‘™)
ğœ™ğ¿

(2.1)

onde ğ‘‹ Ã© um conjunto de vÃ©rtices no modelo probabilÃ­stico grÃ¡fico, cada um relacionado
a um superpixel da imagem e ğœ€ Ã© o conjunto de arestas no grafo de adjacÃªncia ğº(ğ‘‹, ğœ€).
ğ‘– (ğ‘¥ğ‘–, ğœƒğ‘¢) Ã© a distribuiÃ§Ã£o de probabilidades a priori de um rÃ³tulo, para este caso, o con-
ğœ™ğ‘¢
junto de parÃ¢metros ğœƒğ‘¢ esta associado com o treinamento da geraÃ§Ã£o da distribuiÃ§Ã£o a
priori . ğœ™ğ¿
ğ‘– (ğ‘¥ğ‘–, ğœƒğ‘™) Ã© o fator local associado com a probabilidade de duas classes serem
vizinhas uma da outra. Neste caso, o parÃ¢metro ğœƒğ‘™ estÃ¡ associado as matrizes de covari-
Ã¢ncia, treinadas para indicar a probabilidades proximidade entre os rÃ³tulos do conjuntos
ğ¿ = ğ‘™1, ğ‘™2, ...ğ‘™ğ‘› considerando sua conexÃ£o ğº.

Os pesos ğ‘¤ğ‘¢ e ğ‘¤ğ‘™ facilitam a calibraÃ§Ã£o empÃ­rica do modelo, determinando a im-
portÃ¢ncia de cada termo na Eq. 2.1. A Figura 7, mostra uma representaÃ§Ã£o visual de
parte da modelagem usando CRF para a aplicaÃ§Ã£o de interesse que Ã© a classificaÃ§Ã£o e
segmentaÃ§Ã£o de imagens.

No modelo CRF tambÃ©m Ã© possÃ­vel incluir diferentes aspectos baseado em proprie-

44

CapÃ­tulo 2. FundamentaÃ§Ã£o TeÃ³rica 2: ClassificaÃ§Ã£o de Imagens Utilizando Contexto

Figura 7 â€“ A representaÃ§Ã£o grÃ¡fica de um modelo CRF. Os quadrados em vermelho
(ğœ™ğ‘¢
ğ‘– (ğ‘¥ğ‘–, ğœƒğ‘¢)) sÃ£o os fatores unitÃ¡rios calculados com o resultado dado pelo classificador.
Os quadrados em azul sÃ£o os fatores locais computados em cada aresta e utilizados para
introduzir informaÃ§Ã£o contextual. Os circulos verdes representam os superpixeis sendo
classificados.

dades da imagem. A funÃ§Ã£o de bordas de Potts (SHOTTON et al., 2009) (FULKERSON;
VEDALDI; SOATTO, 2009) reforÃ§a nodos que nÃ£o sÃ£o separados por bordas a perten-
cerem a mesma classe. Isto Ã© implementado incluindo o atributo ğ‘”ğ‘–ğ‘— em ğœ™ğ¿
ğ‘– . Onde ğ‘”ğ‘–ğ‘— Ã©
definido pela Eq. 2.2.

â¡â£ğ‘’ğ‘¥ğ‘(âˆ’ğ›½||ğ‘¥ğ‘– âˆ’ ğ‘¥ğ‘—||2)

â¤â¦

(2.2)

ğ‘”ğ‘–ğ‘— =

1

Onde ğ›½ Ã© uma funÃ§Ã£o de contraste dependente da imagem que pode ser facilmente esti-
mada como explicado em (SHOTTON et al., 2009).

2.2.2.1 O Problema da InferÃªncia EstatÃ­stica

Dado um modelo probabilÃ­stico grÃ¡fico e uma funÃ§Ã£o de probabilidades, uma das
principais dificuldades Ã© encontrar um conjunto de rÃ³tulos ğ¿â€² que maximize uma funÃ§Ã£o
de probabilidades como a funÃ§Ã£o da Eq. 2.1. Em outras palavras, seria encontrar a con-
figuraÃ§Ã£o de classificaÃ§Ã£o na imagem mais provÃ¡vel, dado um modelo probabilÃ­stico. Este
problema Ã© considerado NP-Hard, dado que existe uma combinaÃ§Ã£o de rÃ³tulos exponen-
cialmente grande. O problema de inferÃªncia Ã© especialmente difÃ­cil quando se utiliza a
abordagem MRF, dado que existem muito mais casos para computar a distribuiÃ§Ã£o de
probabilidades conjunta.

Algumas aproximaÃ§Ãµes sÃ£o introduzidas de forma reduzir o custo computacional.

2.3. Trabalhos utilizando Modelos ProbabilÃ­sticos GrÃ¡ficos

45

Por exemplo, a abordagem loopy belief propagation (LBP) propaga as informaÃ§Ãµes de dis-
tribuiÃ§Ã£o de probabilidades dos vÃ©rtices ao longo do grafo atravÃ©s de mensagens e obtem
boa performance (WEISS, 2000), entretanto, a convergÃªncia nÃ£o pode ser garantida. Ou-
tra estratÃ©gia Ã© o corte de grafos baseado no alpha-cut (BOYKOV; JOLLY, 2001). Este
algoritmo produz melhores resultados apesar de possuir maior complexidade.

2.2.2.2 Aprendizado de parÃ¢metros

Ã‰ necessÃ¡rio estimar os parÃ¢metros ğœƒğ‘¢ and ğœƒğ‘™. Estes parÃ¢metros sÃ£o a matriz de
covariÃ¢ncia que representa as tendÃªncias das classes serem vizinhas (ğœƒğ‘™), a matriz ğœƒğ‘™ esta
associada Ã s relaÃ§Ãµes espaciais entre as classes.

Os parÃ¢metros podem ser estimados utilizando a tÃ©cnica de mÃ¡ximo a-posteriori
(MAP). Esta tÃ©cnica seleciona os parÃ¢metros que maximizam os resultados para a Eq.
2.1. Isto Ã© custoso, dado que existe a necessidade de computar a inferÃªncia diversas vezes.
PorÃ©m, Ã© possÃ­vel realizar a estimativa, parte a parte, dividindo os parÃ¢metros os quais
maximizar (SHOTTON et al., 2009), entÃ£o reduzindo o custo computacional.

A Figura 10 mostra um exemplo de uma matriz de covariÃ¢ncia estimada, sendo
quanto mais claro for o quadrado mais relacionadas espacialmente as classes estÃ£o. Ã‰
possÃ­vel perceber que a classe B tem uma probabilidade muito maior de ficar prÃ³xima ao
C mas nÃ£o necessariamente a classe E.

2.3 Trabalhos utilizando Modelos ProbabilÃ­sticos GrÃ¡ficos

Diversos trabalhos jÃ¡ utilizaram os modelos probablÃ­stico grÃ¡ficos (MPGs) para
adiÃ§Ã£o de contexto. Apresenta-se aqui alguns relevantes para elaboraÃ§Ã£o deste trabalho.
Carbonetto, Freitas e Barnard (2004) foi um dos primeiros trabalhos a usar MPGs
para classificaÃ§Ã£o de imagens . O autor utilizou uma versÃ£o usando um modelo MRF com
o contexto local e propÃ´s uma forma de reduzir o tempo de inferÃªncia usando uma tÃ©cnica
de expectation maximization (EM) (DEMPSTER; LAIRD; RUBIN, 1977).

Shotton et al. (2009) utilizou uma combinaÃ§Ã£o de descritores diretamente dos
descritores de textura, cor e localizaÃ§Ã£o como fatores unÃ¡rios e adicionou a informaÃ§Ã£o
de contexto local usando a medida de potts. O nÃ­vel de interaÃ§Ã£o foi baseado em regiÃµes
utilizando um novo esquema de representaÃ§Ã£o de imagens atravÃ©s "canais de textura". A
inferÃªncia foi feita utilizando o algoritmo de alpha-cut (BOYKOV; JOLLY, 2001).

Fulkerson, Vedaldi e Soatto (2009), utiliza uma representaÃ§Ã£o usando SIFT (LOWE,
2004) para bag-of-words (SIVIC; ZISSERMAN, 2006) para o fator unÃ¡rio. Como nÃ­vel de
interaÃ§Ã£o, foram utilizadas regiÃµes baseadas em superpixeis. Em seguida, aplica-se um
sistema CRF similar ao proposto por (SHOTTON et al., 2009).

46

CapÃ­tulo 2. FundamentaÃ§Ã£o TeÃ³rica 2: ClassificaÃ§Ã£o de Imagens Utilizando Contexto

Figura 8 â€“ TendÃªncias que existem para as classes estarem prÃ³ximas umas das outras,
quanto mais claro, maior Ã© a tendÃªncia existente. Por exemplo, Ã© possÃ­vel perceber que a
classe B Ã© provÃ¡vel de aparecer perto de uma classe C mas nÃ£o prÃ³xima de uma classe E.

Koltun e Vladlen (2011), utiliza um CRF com uma maior conectividade entre os
vÃ©rtices e utiliza um nÃ­vel de interaÃ§Ã£o por pixel. Neste caso, dado o conjunto de pixeis,
cada par possÃ­vel de pixeis Ã© conectado. O aumento da complexidade de inferÃªncia Ã©
resolvido com um sistema aproximado baseado em mÃ©dias.

Boix et al. (2012) propÃµem adicionar o contexto global ao CRF. Para isso, a

EquaÃ§Ã£o 2.1 pode ser extendida para a Eq. 2.3.

ğ‘™ğ‘œğ‘”ğ‘ƒ(ğ¿|ğº, ğœƒ) = ğ‘¤ğ‘¢

ğ‘¢âˆ‘ï¸

ğ‘– (ğ‘¥ğ‘–, ğœƒğ‘¢) + ğ‘¤ğ‘™
ğœ™ğ‘¢

ğœ–âˆ‘ï¸

ğ‘¥ğ‘–ğœ–ğ‘‹

(ğ‘¥ğ‘–,ğ‘¥ğ‘—)ğœ–ğœ€

ğœ–âˆ‘ï¸

(ğ‘¥ğ‘–,ğ‘¥ğ‘”) ğœ€ğ‘”

onde a porÃ§Ã£o ğœ™ğº

(2.3)
ğ‘–ğ‘”(ğ‘¥ğ‘–, ğ‘¥ğ‘”, ğœƒğ‘”) representa as conexÃµes com um nodo global que, tendo esti-

ğ‘–ğ‘—(ğ‘¥ğ‘–, ğ‘¥ğ‘—, ğœƒğ‘™) + ğ‘¤ğ‘”
ğœ™ğ¿

ğ‘–ğ‘”(ğ‘¥ğ‘–, ğ‘¥ğ‘”, ğœƒğ‘”)
ğœ™ğº

2.4. SumÃ¡rio

47

mado seu conjunto de parÃ¢metros ğœƒğ‘”, indica as configuraÃ§Ãµes mais provÃ¡veis entre todas
as porÃ§Ãµes da imagem. Com tal modelo, as relaÃ§Ãµes de contexto global, como o conjunto
tÃ­pico de objetos possÃ­veis em cena, podem ser incorporadas.

Por fim, Lucchi et al. (2011) critica o funcionamento do CRF, comparando a
utilizaÃ§Ã£o do CRF com pontos de interesse globais (BOIX et al., 2012), concluindo que
nÃ£o hÃ¡ ganho significativo. AlÃ©m disso, o contexto local adicionado pelo CRF a tende
apenas melhorar a suavizaÃ§Ã£o da classificaÃ§Ã£o local. Ou seja, dado uma pequena regiÃ£o
da imagem, as variabilidade de classes Ã© reduzida.

2.4 SumÃ¡rio

Este capÃ­tulo apresentou os conceitos de utilizaÃ§Ã£o de contexto para visÃ£o compu-
tacional. Apresentou-se em quais nÃ­veis o contexto pode ser utilizado, sendo eles globais
ou locais. TambÃ©m foi apresentado quais nÃ­veis de interaÃ§Ã£o o contexto podem se dar,
sendo eles no nÃ­vel de pixeis, regiÃµes ou objetos.

Nesse Ã¢mbito, formalizou-se a classificaÃ§Ã£o utilizando contexto, considerando o
nÃ­vel de interaÃ§Ã£o baseado em regiÃµes, no caso, superpixeis. Em seguida, foram apresen-
tadas formas de utilizar o contexto. Primeiramente foram apresentadas formas de aplicar
o contexto diretamente no classificador. Em seguida, foi apresentado o uso de modelos
probabilÃ­sticos grÃ¡ficos para a aplicaÃ§Ã£o de contexto.

Por fim, alguns dos principais trabalhos em modelos probabilÃ­sticos grÃ¡ficos foram

apresentados.

3 ClassificaÃ§Ã£o Baseada em Contexto utili-

zando GeoestatÃ­stica

49

Tendo em vista as limitaÃ§Ãµes existentes no CRF (LUCCHI et al., 2011) e o conhe-
cimento obtido atravÃ©s de estudos em GeoestatÃ­stica, neste capÃ­tulo, busca-se propor um
novo mÃ©todo para adiÃ§Ã£o de contexto baseado em GeoestatÃ­stica.

Tal abordagem agrega duas Ã¡reas com aplicaÃ§Ãµes distintas mas conceitos seme-
lhantes. No campo da modelagem geolÃ³gica, uma abordagem baseada em geostatÃ­stica
primeiramente busca modelar a variabilidade espacial de uma determinada medida com
o objetivo de interpolar este comportamento para Ã¡reas desconhecidas (CARLE; FOGG,
1996). Esta estratÃ©gia Ã© bastante utilizada para aplicaÃ§Ãµes como modelagem de reservatÃ³-
rios em campos de de extraÃ§Ã£o Ã³leo (BEATTIE; MILLS; MAYO, 1998) ou mapeamento
geolÃ³gico (PURKIS; VLASWINKEL; GRACIAS, 2012).

PorÃ©m, neste trabalho busca-se tambÃ©m mostrar que este conceito se aplica para
o caso de adiÃ§Ã£o de contexto classificaÃ§Ã£o de imagens. A abordagem apresentada Ã© capaz
de assegurar a suavizaÃ§Ã£o de estruturas espaciais de maneira similar que o CRF, porÃ©m,
o mÃ©todo proposto, tambÃ©m considera correlaÃ§Ãµes em longa distÃ¢ncia entre rÃ³tulos.

3.1

VisÃ£o Geral da Proposta

A EquaÃ§Ã£o 3.1 apresenta a adiÃ§Ã£o do contexto espacial utilizando GeoestatÃ­stica:

ğ‘ƒ(ğ¿) = ğ‘¤ğ‘¢ğ‘ƒğ‘¢(ğ¿|ğœƒğ‘¢) + ğ‘¤ğ‘™ğ‘ƒğ‘™(ğ¿|ğ‘Š),

(3.1)

sabe-se que a probabilidade, ğ‘ƒ de um dado conjunto de rÃ³tulos ğ¿, Ã© dada pela soma
ponderada, pelos pesos ğ‘¤ğ‘¢ e ğ‘¤ğ‘™, das probabilidades unÃ¡ria ğ‘ƒğ‘¢(ğ¿|ğœƒğ‘¢), da parte segmentada,
e a probabilidade do contexto local ğ‘ƒğ‘™(ğ¿|ğ‘Š). A matriz W estÃ¡ associada ao peso atribuÃ­do
aos superpixeis da vizinhanÃ§a. ğœƒğ‘¢ estÃ¡ associado aos parÃ¢metros de usados para obtenÃ§Ã£o
da distribuiÃ§Ã£o unÃ¡ria.

Como nÃ­vel de interaÃ§Ã£o, parte-se de uma segmentaÃ§Ã£o baseada em superpixeis
em um grid retangular e uniforme onde cada superpixel tem aproximadamente o mesmo
tamanho. O sistema de classificaÃ§Ã£o proposto neste capÃ­tulo, Ã© mostrado na Figura 10.
Nesta proposta, o nÃ­vel de interaÃ§Ã£o, em uma imagem segmentada ocorre atravÃ©s de
Turbopixels (LEVINSHTEIN et al., 2009).

No nÃ­vel unÃ¡rio ğ‘ƒğ‘¢(ğ¿|ğœƒğ‘¢), somente as informaÃ§Ãµes visuais descritas de um Ãºnico

50

CapÃ­tulo 3. ClassificaÃ§Ã£o Baseada em Contexto utilizando GeoestatÃ­stica

Figura 9 â€“ DivisÃ£o especificada em dois nÃ­veis de classificaÃ§Ã£o. O nÃ­vel unÃ¡rio ğ‘ƒğ‘¢(ğ¿|ğœƒğ‘¢)
onde somente a informaÃ§Ã£o do superpixel segmentado Ã© utilizada, apresentado em verde.
E o nÃ­vel local ğ‘ƒğ‘™(ğ¿|ğ‘Š), onde um determinado contexto local Ã© incluÃ­do na classificaÃ§Ã£o,
representado pelo circulo azul.

superpixel sÃ£o relevantes para a classificaÃ§Ã£o do mesmo. Na seÃ§Ã£o 3.2, mostra-se a compu-
taÃ§Ã£o do nÃ­vel unÃ¡rio e a necessidade do mesmo de produzir uma distribuiÃ§Ã£o confiÃ¡vel.
No nÃ­vel local ğ‘ƒğ‘™(ğ¿|ğ‘Š), se considera as conexÃµes de uma determinada Ã¡rea onde
medidas estatÃ­sticas sÃ£o utilizadas (Circulo Azul Fig. 10). O nÃ­vel ğ‘ƒğ‘™(ğ¿|ğ‘Š) Ã© apresentado
na seÃ§Ã£o 3.4.

3.2 NÃ­vel UnÃ¡rio ğ‘ƒğ‘¢(ğ¿|ğœƒğ‘¢)

Para o sistema proposto, o foco da classificaÃ§Ã£o unÃ¡ria Ã© obter uma distribuiÃ§Ã£o
de probabilidades previa para cada superpixel. Essa probabilidade a priori vai ser usada
para inferir a vizinhanÃ§a.

Em um dado superpixel o qual pode ser classificado como um dentre um conjunto
onde âˆ‘ï¸€ ğ‘ƒğ‘¢(ğ¿|ğœƒğ‘¢) = 1. Onde ğœƒğ‘¢ Ã© conjunto de parÃ¢metros usados para se ter essa saÃ­da.
de rÃ³tulos ğ¿ = ğ‘™1, ğ‘™2..ğ‘™ğ‘› busca-se obter uma saÃ­da ğ‘ƒğ‘¢(ğ¿|ğœƒğ‘¢) = ğ‘ƒğ‘¢(ğ‘™1|ğœƒğ‘¢), ğ‘ƒğ‘¢(ğ‘™2|ğœƒğ‘¢)...ğ‘ƒğ‘¢(ğ‘™ğ‘›|ğœƒğ‘¢)

Para se chegar em tal resultado, a geraÃ§Ã£o do nÃ­vel unÃ¡rio Ã© dividida em duas
etapas. A primeira corresponde ao treinamento da funÃ§Ã£o de discriminaÃ§Ã£o ğ‘“(ğ‘¥), do clas-
sificador. No caso, Ã© feito o treinamento de um kernel linear para uma Support Vector
Machine (SVM). A segunda etapa Ã© a determinaÃ§Ã£o das curvas de confianÃ§a, que corres-
ponde ao grau de certeza da classificaÃ§Ã£o. Ou seja, o grau de certeza ğ¶ğ‘™ğ‘– Ã© dado como uma

3.2. NÃ­vel UnÃ¡rio ğ‘ƒğ‘¢(ğ¿|ğœƒğ‘¢)

51

funÃ§Ã£o treinada, e Ã© usado para gerar ğ‘ƒğ‘¢(ğ¿|ğœƒğ‘¢) (ABFALG et al., 2007).

3.2.1

Classificador

Como classificador, foi utilizado uma Support Vector Machine(SVM) com um ker-
nel linear. A ideia do algoritmo Ã© encontrar uma funÃ§Ã£o de hiperplano ğ‘“ğ‘™ğ‘–(ğ‘¥), para cada
classe ğ‘™ğ‘– que separa linearmente um conjunto de dados previamente rotulados, porÃ©m
maximizando uma determinada margem. Trata-se de uma abordagem de classificaÃ§Ã£o
supervisionada.

Para dada aplicaÃ§Ã£o Ã© necessÃ¡rio que o classificador produza uma distÃ¢ncia de um
objeto Ã  borda de classe mais prÃ³xima a borda entre classes (ABFALG et al., 2007). Tal
resultado Ã© obtido diretamente pelo SVM dado que sua funÃ§Ã£o de hiperplano jÃ¡ maximiza
a margem entre classes. A saÃ­do numÃ©rica do SVM jÃ¡ Ã© prÃ³pria para se ter um certo grau
de confianÃ§a do classificador.

Figura 10 â€“ Figura do separador linear obtido pelo treinamento do SVM. Dado os conjun-
tos de dados jÃ¡ rotulados ( Azuis e Vermelhos), o SVM determina o separador de mÃ¡xima
margem. A saÃ­do numÃ©rica do SVM jÃ¡ Ã© prÃ³pria para se ter um certo grau de confianÃ§a
do classificador.

3.2.2 Treinando Curvas de ConfianÃ§a

Somente as distÃ¢ncias de saÃ­da do SVM (CRISTIANINI; SHAWE-TAYLOR, 2000),
nÃ£o estabelecem diretamente o grau de confianÃ§a de uma classificaÃ§Ã£o (ABFALG et al.,
2007) . Ou seja, uma distÃ¢ncia de valor nÃºmero 5 para o SVM pode ser para alguns casos
uma saÃ­da confiÃ¡vel, para outros nÃ£o. Ã‰ necessÃ¡rio treinar para quais distÃ¢ncias existe uma

52

CapÃ­tulo 3. ClassificaÃ§Ã£o Baseada em Contexto utilizando GeoestatÃ­stica

grande probabilidade da prediÃ§Ã£o ser correta. Isso depende do dataset que foi utilizado,
da classe (rÃ³tulo) e tambÃ©m do classificador.

O estudo apresentado por Platt (PLATT, 1999) demonstra que comportamento

sigmoidal pode modelar a distribuiÃ§Ã£o de probabilidade do SVM.

Sendo assim, para cada classe ğ‘™ğ‘– Ã© feito um ajuste de uma funÃ§Ã£o sigmoidal ğ¶ğ‘™ğ‘–. Esta
funÃ§Ã£o ğ¶ğ‘™ğ‘– nÃ£o mais retorna uma distÃ¢ncia e sim uma probabilidade de uma determinada
entrada na funÃ§Ã£o ğ‘“ğ‘™ğ‘–(ğ‘¥) ser correta.

A Figura 11 apresenta a saÃ­da esperada para a curva de confianÃ§a treinada ğ¶ğ‘™ğ‘–,
para um dado rÃ³tulo ğ‘™ğ‘–. Dado um conjunto de validaÃ§Ã£o em que jÃ¡ se possui os retornos
ğ‘“ğ‘™ğ‘–(ğ‘¥), o treinamento Ã© feito da seguinte forma: ObtÃ©m-se os pontos em azul os quais
indicam a porcentagem de acertos que se tem utilizando apenas as parcelas de dados
as quais seu retorno vindo do classificador (ğ‘“ğ‘™ğ‘–(ğ‘¥)) Ã© de no mÃ¡ximo o que Ã© mostrado
no eixo ğ‘¥. Por exemplo, na Figura 11, para um conjunto de dados com uma distÃ¢ncia
do classificador(ğ‘“ğ‘™ğ‘–(ğ‘¥)) de atÃ© dois, tem-se uma porcentagem de acerto de 70%. Dado
esse conjunto de pontos, o algoritmo de Levenberg-Marquardt (MARQUARDT, 1963) de
otimizaÃ§Ã£o Ã© utilizado para encontrar os coeficientes, ğ›¼ğ‘˜ e ğ›½ğ‘˜ da funÃ§Ã£o sigmoidal:

ğ¶ğ‘™ğ‘– = ğ‘ğ‘™ğ‘–

0.5

1 + ğ‘’ğ‘¥ğ‘(ğ›¼ğ‘™ğ‘– * ğ‘“ğ‘™ğ‘–(ğ‘¥) + ğ›½ğ‘™ğ‘–) + 0.5

(3.2)

onde ğ‘ğ‘™ğ‘– Ã© uma constante de normalizaÃ§Ã£o para a classe ğ‘™ğ‘–. A funÃ§Ã£o inicia de 0.5 pois Ã©
a probabilidade inicial de uma classe ser acertada aleatoriamente. A abordagem proposta
contrasta com a proposta apresentada por ABfalg et al. (2007) por propor uma curva de
confianÃ§a para cada classe. Isso tambÃ©m leva em conta as diferenÃ§as existentes em cada
classe. ğœƒğ‘¢ estÃ¡ relacionado aos parÃ¢metros ğ›¼ğ‘˜ e ğ›½ğ‘˜ de ğ‘ƒğ‘¢(ğ¿|ğœƒğ‘¢).

Figura 11 â€“ GrÃ¡fico mostrando a probabilidade de acerto em funÃ§Ã£o da mÃ¡xima confianÃ§a
retornada pelo classificador para um conjunto de dados. Em vermelho tem-se a funÃ§Ã£o
ğ¶ğ‘™ğ‘– treinada a partir do conjunto de dados em azul.

3.3. DistribuiÃ§Ã£o de Probabilidades
3.3 DistribuiÃ§Ã£o de Probabilidades

53

Normalizando o grau de confianÃ§a para a distÃ¢ncia com relaÃ§Ã£o a todas as classes,
se obtÃ©m a distribuiÃ§Ã£o de probabilidades para um determinado objeto. Sendo assim, um
classificador nÃ£o mais produz somente uma saÃ­da, mas tambÃ©m uma chance de cada item
de um conjunto de dados a pertencer a cada uma das classes. A Figura 12 mostra a saÃ­da
da classificaÃ§Ã£o de um exemplo calculado. Sendo que cada barra representa a chance do
objeto pertencer a tal classe.

Figura 12 â€“ Histograma mostrando a distribuiÃ§Ã£o de probabilidades de saÃ­da de um clas-
sificador. Para o caso, a segunda classe, Ã© a que obteve maior probabilidade, porÃ©m existe
uma certa incerteza com relaÃ§Ã£o a primeira classe.

Observa-se na Figura 12 que a saÃ­da do classificador mostra a classe mais provÃ¡vel

mas existe uma incerteza significativa para uma segunda classe ser a correta.

3.4 NÃ­vel Local ğ‘ƒğ‘™(ğ¿|ğ‘Š)

A Figura 13 mostra o conjunto de passos para calcular os parcelas locais utilizando

o contexto baseado em GeoestatÃ­stica.

A primeira parte do mÃ©todo (lado direito da Figura 13) Ã© estimar e modelar a
incerteza existente nos conjunto de dados os quais se quer adicionar o contexto. Essa mo-
delagem se da atravÃ©s da estimativa da matriz de probabilidade de transiÃ§Ã£o (ğ‘‡) entre os
possÃ­veis rÃ³tulos presentes no conjunto de dados. Isso Ã© feito em uma etapa de treinamento
offline do mÃ©todo A estimativa Ã© feita em duas partes que sÃ£o combinadas: analisando
as frequÃªncias de transiÃ§Ãµes entre as classes de um conjunto de imagens (CARLE et al.,
1998) e medindo propriedades estatÃ­sticas nos dados como proporÃ§Ãµes e espessuras.

54

CapÃ­tulo 3. ClassificaÃ§Ã£o Baseada em Contexto utilizando GeoestatÃ­stica

Figura 13 â€“ Diagrama geral da adiÃ§Ã£o de contexto local utilizando GeoestatÃ­stica. Primei-
ramente Ã© medida a variabilidade entre as classes no contexto espacial. Tanto diretamente
atravÃ©s das frequÃªncias de transiÃ§Ã£o na imagem (taxa de transiÃ§Ã£o medida), quanto atravÃ©s
da inferÃªncia de propriedades estatÃ­sticas vindas da imagem (taxa de transiÃ§Ã£o modelada).
Em seguida sÃ£o calculados os vetores de transiÃ§Ã£o. Na segunda parte os vetores sÃ£o utiliza-
dos para gerar pesos para imagem. Com isso, utilizando os pesos, o sistema SIS computa
a adiÃ§Ã£o de contexto local para cada superpixel.

A segunda parte do mÃ©todo Ã© a geraÃ§Ã£o do contexto local ğ‘ƒğ‘™(ğ¿|ğ‘Š). Deste modo,
para cada superpixel que se deseja computar, a matriz de probabilidades de transiÃ§Ã£o
Ã© usada para computar os pesos ğ‘Š, para servir como entrada em um sistema de SIS,
Sequential Indicator Simulation (Indicador de simulaÃ§Ã£o sequencial) (EMERY, 2004) o
qual computa ğ‘ƒğ‘™(ğ¿|ğ‘Š). Cada um dos processos apontados na Figura 13 sÃ£o detalhados
no restante desta seÃ§Ã£o.

3.4.1 Medindo TransiÃ§Ãµes de Probabilidades

Primeiramente, um sistema de transiÃ§Ã£o de probabilidades baseado em cadeias de
Markov Ã© medido. Este modelo representa a variabilidade espacial existente juntamente
com os dados da imagem.

A estrategia proposta Ã© calcular uma matriz ğ‘‡, onde cada componente Ã© a funÃ§Ã£o
ğ‘¡ğ‘–ğ‘—(â„ğœ‘) a qual modela a probabilidade de uma classe ğ‘– de transitar para a classe ğ‘— em
uma distÃ¢ncia â„ considerando a direÃ§Ã£o ğœ‘. Ã‰ necessÃ¡rio obter tal medida para cada par
de classes ğ‘– e ğ‘— presente no conjunto de dados.

Neste mÃ©todo, assume-se que os dados sÃ£o isomÃ³rficos. Portanto, para uma dada
transiÃ§Ã£o, todas as direÃ§Ãµes sÃ£o consideradas como idÃªnticas. NÃ£o obstante, a abordagem
pode ser utilizada em casos nÃ£o isomÃ³rficos, considerando duas ou mais direÃ§Ãµes, cada
uma com sua prÃ³pria matriz de probabilidade de transiÃ§Ãµes.

Foi assumido que a transiÃ§Ã£o de probabilidades tem um comportamento exponen-

3.4. NÃ­vel Local ğ‘ƒğ‘™(ğ¿|ğ‘Š)

55

cial, como proposto por (CARLE et al., 1998). As equaÃ§Ã£o 3.3 mostra como calcular a
transiÃ§Ã£o de probabilidades entre classes diferentes e tambÃ©m para a mesma classe ( auto
transiÃ§Ã£o).

ğ‘¡ğ‘–ğ‘— =

ğ‘¡ğ‘–ğ‘— = ğ‘ğ‘— âˆ’ ğ‘’âˆ’ğ‘Ÿğ‘–ğ‘— â„ğœ‘ [ğ‘– Ì¸= ğ‘—]

se [ğ‘– = ğ‘—]
se [ğ‘– Ì¸= ğ‘—]

(3.3)

A funÃ§Ã£o de transiÃ§Ã£o na EquaÃ§Ã£o 3.3 tambÃ©m depende da distÃ¢ncia â„ e da probabilidade
a priori da classe ğ‘ğ‘—. Cada fator ğ‘Ÿğ‘–ğ‘— Ã© um componente da matriz ğ‘… Essa matriz Ã© a taxa
de transiÃ§Ã£o entre as classes, para ğ‘˜ classes, ğ‘… pode ser calculada como:

â§âªâ¨âªâ©ğ‘¡ğ‘–ğ‘— = ğ‘’ğ‘Ÿğ‘–ğ‘— â„ğœ‘ + ğ‘ğ‘– [ğ‘– = ğ‘—]
â¡â¢â¢â¢â¢â¢â¢â¢â¢â¢â£

... ğ‘Ÿ1ğ‘–
.
.
.
ğ‘Ÿğ‘—ğ‘–
.
.
... ğ‘Ÿğ‘˜ğ‘–

ğ‘Ÿ11
.
ğ‘Ÿğ‘—1
.
ğ‘Ÿğ‘˜1

ğ‘… =

â¤â¥â¥â¥â¥â¥â¥â¥â¥â¥â¦

... ğ‘Ÿ1ğ‘˜
.
.
.
ğ‘Ÿğ‘—ğ‘˜
.
.
... ğ‘Ÿğ‘˜ğ‘˜

(3.4)

Cada elemento da matriz representa a taxa na qual ocorre a transiÃ§Ã£o, sendo assim:

ğ‘Ÿğ‘–ğ‘— = ğœ•ğ‘¡ğ‘–ğ‘–(0))

ğœ•â„

(3.5)

A matriz ğ‘‡ nÃ£o pode ser diretamente calculada a partir dos dados (AGTERBERG,
1988). Para tal, primeiramente, Ã© necessario estimar a matriz ğ‘…. Carle et al. (1998),
propÃµem obter o calculo de ğ‘… atravÃ©s da multiplicaÃ§Ã£o elemento a elemento das medidas
da correlaÃ§Ã£o entre as configuraÃ§Ãµes espaciais diretas (ğ‘…ğ‘šğ‘’ğ‘ ) e da medida de conceitos
estatÃ­sticos (ğ‘…ğ‘šğ‘œğ‘‘) extraÃ­dos das imagens:

ğ‘… = ğ‘…ğ‘šğ‘’ğ‘  * ğ‘…ğ‘šğ‘œğ‘‘.

(3.6)

Esta foi a tÃ©cnica adotada neste trabalho para calcular ğ‘…, porÃ©m com algumas
modificaÃ§Ãµes. Na seÃ§Ã£o a seguir, mostra-se o processo para calcular as matrizes ğ‘…ğ‘šğ‘’ğ‘  e
ğ‘…ğ‘šğ‘œğ‘‘.

3.4.1.1 Taxa de TransiÃ§Ã£o Medida ğ‘…ğ‘šğ‘’ğ‘ 

ğ‘…ğ‘šğ‘’ğ‘  Ã© calculado medindo a frequÃªncia de transiÃ§Ã£o cumulativa da matriz ğ¹. Para
computar ğ¹, foi somado o nÃºmero de vezes que cada classe transita para cada outra classe.
Foi considerado um conjunto de direÃ§Ãµes e um conjunto de distÃ¢ncias â„. Tal treinamento Ã©
feito em um conjunto de imagens jÃ¡ previamente classificadas. Este processo Ã© apresentado
na Figura 14. Foi utilizado um kernel de janela deslizante que iterativamente se desloca
ao longo de toda a imagem.

56

CapÃ­tulo 3. ClassificaÃ§Ã£o Baseada em Contexto utilizando GeoestatÃ­stica

Para cada posiÃ§Ã£o do kernel, foram contadas todas as transiÃ§Ãµes que o rÃ³tulo do
ponto central do kernel faz. Isto Ã© feito para diversas distÃ¢ncias, o qual Ã© representado
pelos quadrados coloridos da Fig. 14. Ao final, cada linha de ğ¹ Ã© normalizada.

Figura 14 â€“ Medida feita do nÃºmero de transiÃ§Ãµes que uma classe faz para cada outra
para mÃºltiplas distÃ¢ncias. Foi utilizada um kernel mÃ³vel e foram contadas as transiÃ§Ãµes
desde o centro (ponto vermelho) para todas as direÃ§Ãµes (representado pelos quadrados)

A equaÃ§Ã£o 3.7 mostra um exemplo da matriz ğ¹ feitas para um "datasetâ€ exemplo.

â¡â¢â¢â¢â¢â¢â¢â¢â¢â¢â£

ğ¹ =

0.762 0.007 0.037 0.060 0.132
0.639 0.066 0.048 0.061 0.184
0.607 0.010 0.214 0.052 0.115
0.594 0.006 0.032 0.261 0.104
0.642 0.010 0.036 0.054 0.255

â¤â¥â¥â¥â¥â¥â¥â¥â¥â¥â¦

(3.7)

(3.9)

A matriz estimada ğ¹ Ã© afetada pelas incertezas nas premissas de probabilidade
assumidas. Por exemplo, a premissa do isomorfismo assumida nÃ£o Ã© perfeitamente verda-
deira. Para reduzir o efeito das incertezas e encontrar um padrÃ£o na representaÃ§Ã£o, uma
anÃ¡lise de autovetores e autovalores Ã© aplicada na Eq. 3.7 (CARLE; FOGG, 1996).

A partir disso, Ã© possÃ­vel computar ğ‘…ğ‘šğ‘’ğ‘  aplicando a equaÃ§Ã£o 3.8.

ğ¿âˆ‘ï¸

ğ‘˜=1

ğ‘…ğ‘šğ‘’ğ‘  =

ğœƒğ‘˜ğ‘ğ‘˜

(3.8)

onde o termo ğœƒğ‘˜ de ğ‘˜ = 1, ..., ğ¿ denota os autovalores de F e ğ‘ğ‘˜ denota os componentes
espectrais das matrizes desde a analise de auto-vetores. Foi calculado ğ‘ğ‘˜ como mostrado
na Eq. 3.9.

âˆï¸€
âˆï¸€
ğ‘šÌ¸=ğ‘˜(ğœƒğ‘˜ğ¼ âˆ’ ğ¹)
ğ‘šÌ¸=ğ‘˜(ğœƒğ‘š âˆ’ ğœƒğ‘˜) ğ‘˜ = 1, ..., ğ¿

ğ‘ğ‘˜ =

Esta computaÃ§Ã£o consiste em uma medida inicial que congrega as tendÃªncias de
verossimilhanÃ§a espacial entre as classes. Contudo, esta medida ainda contÃ©m muita im-

3.4. NÃ­vel Local ğ‘ƒğ‘™(ğ¿|ğ‘Š)

57

precisÃ£o para ser usada como entrada para a simulaÃ§Ã£o. A medida pode ser ainda mais
estabilizada adicionando a computaÃ§Ã£o de ğ‘…ğ‘šğ‘œğ‘‘ assim como mostrado na Eq. 3.6.

3.4.1.2

Calculo da Matriz ğ‘…ğ‘šğ‘œğ‘‘

Computa-se ğ‘…ğ‘šğ‘œğ‘‘ utilizando estatÃ­sticas extraÃ­da dos dados, como: proporÃ§Ãµes das

classes, comprimentos mÃ©dios das classes e as tendÃªncias de justaposiÃ§Ã£o.

A proporÃ§Ã£o de uma classe ğ‘™ğ‘– Ã© a probabilidade a priori desta classe aparecer. Em
outras palavras, a proporÃ§Ã£o Ã© a chance de selecionar uma parcela da classe ğ‘™ğ‘– aleatoria-
mente da imagem classificada (CARLE; FOGG, 1996).

O comprimento mÃ©dio Ã© calculado pela quantidade mÃ©dia de pixeis contÃ­nuos de
uma certa classe ao longo de uma determinada direÃ§Ã£o. Como assume-se isomorfismo nos
dados, esta direÃ§Ã£o Ã© arbitraria. Considerando em termos de transiÃ§Ã£o de probabilidades,
o comprimento mÃ©dio ğ¿â„ğœ‘ Ã© a taxa de decaimento da curva de transiÃ§Ã£o da funÃ§Ã£o ğ‘¡ğ‘–ğ‘–(â„ğœ‘)
na direÃ§Ã£o ğœ‘ . O comprimento mÃ©dio Ã© mostrado na equaÃ§Ã£o 3.10.

âˆ’ğœ•ğ‘¡ğ‘–ğ‘–(0))

ğœ•â„

= 1
ğ¿â„,ğœ‘

(3.10)

Isso Ã© anÃ¡logo a taxa de uma classe transitar para si mesma, como mostrado na

Eq. 3.11 (CARLE; FOGG, 1996).

Ëœğ‘Ÿğ‘–ğ‘– = âˆ’ 1
ğ¿â„,ğœ‘

(3.11)

O conceito de tendÃªncia de justaposiÃ§Ã£o modela as probabilidades de uma classe
transitar fora de si mesmo e depois em outra dado uma distÃ¢ncia. Considerando ğ‘Ÿğ‘–ğ‘– como
a taxa que a uma certa classe transita para si mesma, ğ‘Ÿğ‘–ğ‘— depende das proporÃ§Ãµes de ğ‘—
como mostrado na Eq. 3.12.

Ëœğ‘Ÿğ‘—,ğ‘˜(â„ğœ‘) =

ğ‘ğ‘˜

ğ¿ğ‘—ğœ‘(1 âˆ’ ğ‘ğ‘—)

(3.12)

Para o caso de um dataset de cinco classes, a matrix ğ‘…ğ‘šğ‘œğ‘‘ tem a seguinte estrutura:

â¡â¢â¢â¢â¢â¢â¢â¢â¢â¢â£

1
ğ¿11
Ëœğ‘Ÿ21
Ëœğ‘Ÿ31
Ëœğ‘Ÿ41
Ëœğ‘Ÿ51

Ëœğ‘Ÿ11
1
ğ¿22
Ëœğ‘Ÿ32
Ëœğ‘Ÿ42
Ëœğ‘Ÿ52

Ëœğ‘Ÿ13
Ëœğ‘Ÿ23
1
ğ¿33
Ëœğ‘Ÿ43
Ëœğ‘Ÿ53

Ëœğ‘Ÿ14
Ëœğ‘Ÿ24
Ëœğ‘Ÿ34
1
ğ¿44
Ëœğ‘Ÿ54

Ëœğ‘Ÿ15
Ëœğ‘Ÿ25
Ëœğ‘Ÿ35
Ëœğ‘Ÿ45
1
ğ¿55

â¤â¥â¥â¥â¥â¥â¥â¥â¥â¥â¦

ğ‘…ğ‘šğ‘œğ‘‘ =

(3.13)

Finalmente, usando ğ‘…ğ‘šğ‘’ğ‘  e ğ‘…ğ‘šğ‘œğ‘‘ Ã© possÃ­vel computar a Eq 3.6. Na Figura 15
sÃ£o mostradas as transiÃ§Ãµes de probabilidades calculadas para um dataset. Os grÃ¡ficos

58

CapÃ­tulo 3. ClassificaÃ§Ã£o Baseada em Contexto utilizando GeoestatÃ­stica

da Figura 15 foram computados aplicando a Eq. 3.3 variando a distÃ¢ncia â„. Como um
exemplo, pode-se perceber que o comprimento mÃ©dio da classe background Ã© bem alto.
Isso acontece por que o seu decaimento exponencial Ã© muito baixo.

Figura 15 â€“ A transiÃ§Ã£o de probabilidade modelada para um determinado dataset. O
eixo y apresenta a distÃ¢ncia em pixeis. As linhas verdes mostram as proporÃ§Ãµes para cada
classe. Pode-se observar uma certa tendÃªncia na classe Urchin em transitar para categoria
de background. Ainda, percebe-se que a classe de background tem um grande comprimento
mÃ©dio, dado que sua taxa de decaimento Ã© bastante alta.

3.4.2 Sequential Indicator Simulation

Dado que a matriz de transiÃ§Ã£o ğ‘‡ jÃ¡ foi calculada para um dataset, o algoritmo
Sequential Indicator Simulation (SIS) tenta simular o ğ‘ƒğ‘™(ğ¿|ğ‘Š) de um superpixel com base
em sua vizinhanÃ§a espacial. Para simular os fatores locais de um certo superpixel ğ‘¥0, um
certo nÃºmero ğ‘ de posiÃ§Ãµes aleatÃ³rias amostradas ğ‘¥ğ›¼ sÃ£o computados em torno da regiÃ£o
em um raio ğ‘Ÿ. Cada uma das posiÃ§Ãµes amostradas vai contribuir para o computar o fator
local, sendo que a contribuiÃ§Ã£o Ã© feita de forma a minimizar a varianÃ§a desta vizinhanÃ§a
com respeito ao modelo.

Com isso, a probabilidade relacionada com o contexto espacial para cada classe ğ‘˜

3.4. NÃ­vel Local ğ‘ƒğ‘™(ğ¿|ğ‘Š)

59

em uma certa parcela ğ‘¥0 Ã© computada como:

ğ‘ƒğ‘™(ğ‘¥0 = ğ‘˜|ğ‘ƒğ‘¢(ğ‘‹ğ›¼)) = ğ‘ƒğ‘¢(ğ‘¥0 = ğ‘˜)

ğ‘âˆ‘ï¸

ğ¾âˆ‘ï¸

ğ›¼=1

ğ‘—=1

ğ‘ƒğ‘¢(ğ‘‹ğ›¼ = ğ‘—)ğ‘¤ğ‘—ğ‘˜,ğ›¼

(3.14)

onde ğ‘ƒğ‘¢ Ã© a probabilidade a priori (unÃ¡rio) de uma regiÃ£o, sendo ğ‘ƒğ‘¢(ğ‘‹0 = ğ‘˜) o superpixel
em questÃ£o e ğ‘ƒğ‘¢(ğ‘‹ğ›¼ = ğ‘—), os amostrados. ğ‘¤ğ‘—ğ‘˜,ğ›¼ Ã© o peso da posiÃ§Ã£o ğ›¼ para a classe ğ‘—
transitar para a classe ğ‘˜. Ou seja a probabilidade local de um superpixel Ã© funÃ§Ã£o da
distribuiÃ§Ã£o do mesmo (ğ‘ƒğ‘¢(ğ‘‹0 = ğ‘˜)) e o quanto cada posiÃ§Ã£o amostrada contribui para

âˆ‘ï¸€ğ¿
ğ‘—=1 ğ‘ƒğ‘¢(ğ‘‹ğ›¼ = ğ‘—)). O fator Ã© controlado pelo peso ğ‘¤ğ‘—ğ‘˜,ğ›¼.

este superpixel (âˆ‘ï¸€ğ‘

ğ›¼=1

Os pesos para cada posiÃ§Ã£o amostrada formam o conjunto de matrizes ğ‘Šğ‘ e sÃ£o

calculados resolvendo o sistema linear da Eq. 3.15 :

â¡â¢â¢â¢â£ ğ‘‡(ğ‘¥1 âˆ’ ğ‘¥1)

ğ‘‡(ğ‘¥1 âˆ’ ğ‘¥ğ‘)

.

â¤â¥â¥â¥â¦

... ğ‘‡(ğ‘¥ğ‘ âˆ’ ğ‘¥1)
.
... ğ‘‡(ğ‘¥ğ‘ âˆ’ ğ‘¥ğ‘)

.

onde:

.

â¡â¢â¢â¢â£ ğ‘Š1
â¤â¥â¥â¥â¦
â¡â¢â¢â¢â£ğ‘¤11,ğ›¼

ğ‘Šğ‘

ğ‘Šğ‘– =

.

â¡â¢â¢â¢â£ ğ‘‡(ğ‘¥0 âˆ’ ğ‘‹1)

ğ‘‡(ğ‘¥0 âˆ’ ğ‘‹ğ‘)

.

â¤â¥â¥â¥â¦ (3.15)

(3.16)

â¤â¥â¥â¥â¦

=

... ğ‘¤1ğ¿,ğ›¼
.

.

ğ‘¤ğ¿1,ğ›¼ ... ğ‘¤ğ¿ğ¿,ğ›¼

A Figura 16 mostra o exemplo de um superpixel arbitrÃ¡rio e sua respectiva regiÃ£o
amostrada, para a computaÃ§Ã£o do potencial local. Para tal regiÃ£o a Eq. 3.15 serÃ¡ aplicada
de forma a encontrar o peso para cada uma das posiÃ§Ãµes. O peso encontrado Ã© o que
tornaria a regiÃ£o o mais homogÃªnea possÃ­vel.

3.4.3 Computando o Potencial Final ğ‘ƒ(ğ¿)

Depois de obter uma saÃ­da da curva de confianÃ§a para cada superpixel, primeira-
mente se busca os superpixeis com uma saÃ­da bem alta de confianÃ§a. Foi decido computar
o potencial local apenas para superpixeis onde a confianÃ§a estÃ¡ abaixo de um limiar ğ‘¡. O
limiar Ã© selecionado como a confianÃ§a mÃ¡xima, dado pelo conjunto de validaÃ§Ã£o.

O processo do SIS Ã© repetido para cada superpixel presente na imagem em ordem
aleatÃ³ria e os pesos jÃ¡ sÃ£o atualizados. Isso garante que a correlaÃ§Ã£o entre a prÃ³pria
vizinhanÃ§a seja considerada.

Os pesos sÃ£o obtidos diretamente pela Eq. 3.15. Dois parÃ¢metros devem ser escolhi-
dos para este mÃ©todo, o nÃºmero de amostras ğ‘ e o raio ğ‘Ÿ onde vai ser feita a amostragem.
Experimentos preliminares mostraram que nÃ£o existe vantagem pratica em usar mais de

60

CapÃ­tulo 3. ClassificaÃ§Ã£o Baseada em Contexto utilizando GeoestatÃ­stica

Figura 16 â€“ Exemplo de uma vizinhanÃ§a sendo considerada para um superpixel ( apontado
em vermelho). Um raio ğ‘Ÿ Ã© considerado e ğ‘ pontos sÃ£o amostrados nessa vizinhanÃ§a ( em
azul). Cada um dos pontos amostrados irÃ¡ influenciar no potencial do superpixel apontado
em vermelho.

25 amostras. TambÃ©m, o raio ğ‘Ÿ passa e se tornar irrelevante a partir de uma certa dis-
tÃ¢ncia, dado que Ã s transiÃ§Ãµes de probabilidade tendem a ser iguais as proporÃ§Ãµes no
limite.

3.5 GeoestatÃ­stica e CRF

Tanto as abordagem de CRF, quanto de GeoestatÃ­stica (GS) tentam minimizar
uma funÃ§Ã£o que tenta impor uma certa homogeneidade espacial. Ou seja, superpixeis
prÃ³ximos tendem a ser da mesma classe. A diferenÃ§a Ã© que o modelo de GeoestatÃ­stica Ã©
baseado em uma amostragem o que torna o problema da inferÃªncia mais simples. O modelo
de GS Ã© tambÃ©m anÃ¡logo a um CRF densamente conectado (KOLTUN; VLADLEN, 2011),
mas com amostragens mais esparsas.

A abordagem de GS pode ser vista como uma representaÃ§Ã£o mais esparsa do CRF
porÃ©m, com medidas estatÃ­sticas mais ricas. NÃ£o obstante, a computaÃ§Ã£o da matriz de
pesos ğ‘Š para a Eq. 3.14 pode ser considerado como a minimizaÃ§Ã£o de uma funÃ§Ã£o de
energia, usando uma soma ponderada.

Como uma forma de comparar ambos os mÃ©todos, a Figura 17 mostra o modelo
GS como um modelo grÃ¡fico probabilÃ­stico. O vÃ©rtice central, em verde claro, Ã© o caso
atual sendo calculado. Os vÃ©rtices em verde escuro sÃ£o aqueles amostrados. Cada vertice
em verde escuro contribui para a distribuiÃ§Ã£o do vertice central dependendo das proba-
bilidades de transiÃ§Ã£o estimadas da Fig. 15. Em vermelho sÃ£o representados os fatores

3.6. SumÃ¡rio

61

unÃ¡rios de cada quadrado em azul Ã© a contribuiÃ§Ã£o desses mesmos ( fatores locais).

Figura 17 â€“ RepresentaÃ§Ã£o grÃ¡fica do modelo de GeoestatÃ­stica (GS). Os fatores locais
sÃ£o representados em azul e usam a estatÃ­stica de probabilidade de transiÃ§Ã£o computada
pela Eq. 3.3. Diferentemente do que no modelo da Fig. 7, vizinhos de diferentes distÃ¢ncias
tambÃ©m contribuem para calcular a distribuiÃ§Ã£o de cada posiÃ§Ã£o.

3.6 SumÃ¡rio

Neste CapÃ­tulo apresentou-se um novo mÃ©todo para adiÃ§Ã£o de contexto na classifi-
caÃ§Ã£o. O mÃ©todo foi inspirado nas tÃ©cnicas de modelagem da variabilidade espacial usada
em GeoestatÃ­stica.

Foi feita, por fim, uma comparaÃ§Ã£o do mÃ©todo proposto com o CRF. Acredita-
se que o mÃ©todo apresentado neste capÃ­tulo tende a se comportar melhor que o CRF
quando existem menos dados de treinamento, e os mesmos dados nÃ£o possuem padrÃµes
bem definidos, como no caso do ambiente subaquÃ¡tico. Isso pode ser atingido visto que
o mÃ©todo proposto estima padrÃµes de forma para as classes. Sendo assim as relaÃ§Ãµes de
correlaÃ§Ã£o espacial, sÃ£o tambÃ©m estimadas com base em um modelo para as classes. O
mÃ©todo proposto serÃ¡ testado e avaliado no CapÃ­tulo 6.

4 ClassificaÃ§Ã£o de Imagens do Assoalho

63

OceÃ¢nico

Neste CapÃ­tulo Ã© apresentado o domÃ­nio de aplicaÃ§Ã£o no qual serÃ¡ aplicado o

mÃ©todo de adiÃ§Ã£o de contexto proposto no CapÃ­tulo 3.

Como apresentado na introduÃ§Ã£o, o conhecimento sobre as espÃ©cies presentes no
fundo do mar, especialmente os recifes de corais, Ã© de fundamental importÃ¢ncia para os
especialistas na Ã¡rea.

Ao se fazer monitoramento do assoalho oceÃ¢nico, assim como para o caso do sen-
soriamento remoto, Ã© interessante ser capaz de rotular automÃ¡ticamente cada pixel das
imagens e assim ser capaz de medir propriedades relevantes. O objetivo desta classifica-
Ã§Ã£o Ã© fazer os chamados mapas temÃ¡ticos. Tais mapas sÃ£o a representaÃ§Ã£o final de um
mapa classificado de uma imagem, feito de forma visualmente interpretÃ¡vel. Mapas temÃ¡-
ticos agregam grandes conjuntos de imagens em mosaicos representando Ã¡reas de grande
extensÃ£o.

Considerando o ambiente subaquÃ¡tico, suas propriedades fotomÃ©tricas demandam
um tratamento especial para contornar a degradaÃ§Ã£o da imagem. Esses desafios prÃ³prios
do meio nÃ£o sÃ£o comumente endereÃ§ados na literatura. Tais propriedades causam proble-
mas como bordas confusas entre objetos, variaÃ§Ã£o na qualidade da imagem, etc.

Neste capÃ­tulo primeiramente Ã© formalizada as propriedades do meio subaquÃ¡tico,
o que sera Ãºtil tambÃ©m para capÃ­tulos posteriores. Depois, Ã© apresentada uma visÃ£o geral
dos principais sistemas utilizados para classificaÃ§Ã£o de mosaicos do assoalho oceÃ¢nico.
Entre os sistemas apresentados, um em especial serÃ¡ detalhado, o qual serÃ¡ utilizado
como um estudo de caso para adiÃ§Ã£o de contexto.

4.1

Propriedades de Imagens SubaquÃ¡ticas

De forma a obter imagens capturadas em ambiente subaquÃ¡tico com uma melhor
qualidade visual, Ã© fundamental o entendimento de sua formaÃ§Ã£o, levando em conta os
aspectos especÃ­ficos que ocorrem no meio subaquÃ¡tico.

Um modelo de formaÃ§Ã£o de imagens busca descrever os caminhos pelos quais a luz
passa, desde a fonte atÃ© a sua captura, onde Ã© formada a imagem. A Figura 18 ilustra
este processo de propagaÃ§Ã£o. Em meios participativos, a irradiaÃ§Ã£o, ou seja, a quantidade
de energia luminosa em um pixel, pode ser obtida pelo somatÃ³rio de trÃªs componentes as
quais chegam por caminhos distintos. A componente direta, a qual contÃ©m a luz sem es-

64

CapÃ­tulo 4. ClassificaÃ§Ã£o de Imagens do Assoalho OceÃ¢nico

palhamento que veio diretamente do objeto. Muitas vezes, informaÃ§Ãµes que vinham de um
Ãºnico ponto sÃ£o espalhadas entre seus pontos vizinhos causando um efeito de borramento
na imagem. Este fenÃ´meno Ã© chamado espalhamento dianteiro (forward scattering),
representado pela componente forward scattering. O forward-scattering faz com que as
informaÃ§Ãµes visuais da cena fiquem espalhadas, causando um efeito de borramento.

Figura 18 â€“ TrÃªs trajetÃ³rias da luz atÃ© o plano da imagem. O componente direto, con-
tendo a informaÃ§Ã£o direta da cena. O forward-scattering, contendo informaÃ§Ã£o da cena
espalhada. Por fim, o backscattering contendo informaÃ§Ãµes de fora da cena.

Por Ãºltimo, tem-se a componente de backscattering, a qual luz chega no plano da
imagem a partir de um ponto que nÃ£o faz parte da cena observada. Isso acontece devido
Ã  alguma partÃ­cula flutuante que desvia a trajetÃ³ria da luz para o plano da imagem. O
backscattering se comporta tal como um ruÃ­do aditivo.

Para calcular cada uma das componentes, algumas simplificaÃ§Ãµes devem ser con-
sideradas. Tais simplificaÃ§Ãµes visam tornar o modelo mais simples e tratÃ¡vel computacio-
nalmente, ressaltando somente alguns aspectos principais na formaÃ§Ã£o da imagem.

Primeiramente, se assume o objeto como tendo sua reflectividade uniforme. Assume-
se uma iluminaÃ§Ã£o completa e uniforme da cena. Por fim, pode-se descartar os parÃ¢metros
da cÃ¢mera e considerar a captura da luz como sendo tambÃ©m uniforme.

Normalmente o efeito causado pelo forward-scattering tende ser desprezado, por
contribuir com uma participaÃ§Ã£o menor que o backscattering na formaÃ§Ã£o da imagem
(TREIBITZ; SCHECHNER, 2006).

A descriÃ§Ã£o final do modelo Ã© dada pela equaÃ§Ã£o de Koschmieder (KOSCHMI-
EDER, 1924), bastante utilizada para a propagaÃ§Ã£o da luz na nÃ©voa. Sendo assim, a

4.1. Propriedades de Imagens SubaquÃ¡ticas

formaÃ§Ã£o de um ponto (ğ‘¥, ğ‘¦) na imagem Ã© dado por:

ğ¼(ğ‘¥, ğ‘¦) = ğ½(ğ‘¥, ğ‘¦) ğ‘’âˆ’ğ‘ğ‘§(ğ‘¥,ğ‘¦) + ğµâˆ(1 âˆ’ ğ‘’âˆ’ğ‘ğ‘§(ğ‘¥,ğ‘¦)),

65

(4.1)

Sendo ğ½(ğ‘¥, ğ‘¦) a imagem sem degradaÃ§Ã£o e ğ‘§(ğ‘¥, ğ‘¦) uma funÃ§Ã£o da distÃ¢ncia para
cada ponto na imagem. Essa equaÃ§Ã£o pode ser interpretada da seguinte forma: quanto
mais distante estiver o objeto maior serÃ¡ o componente backscattering, menos da cena real
irÃ¡ existir na imagem.

Sabe-se que, devido as propriedades do meio subaquÃ¡tico, existe uma diferenÃ§a sig-
nificativa entre a absorÃ§Ã£o e espalhamento dos comprimentos de onda (DUNTLEY, 1963)
Desta forma, o modelo pode ser estendido de forma incorporar diferentes comprimentos
de onda. A equaÃ§Ã£o 4.1 modela a quantidade de luminosidade capturada relativa a um
determinado pixel. PorÃ©m Ã© possÃ­vel adequÃ¡-la para diferentes comprimentos de onda, ou
no caso do padrÃ£o RGB de representaÃ§Ã£o, dividi-la em trÃªs canais conforme a equaÃ§Ã£o
4.2,

ğ¼ ğœ†(ğ‘¥, ğ‘¦) = ğ½ ğœ†(ğ‘¥, ğ‘¦) ğ‘’âˆ’ğ‘ğœ†ğ‘§(ğ‘¥,ğ‘¦) + ğµğœ†âˆ(1 âˆ’ ğ‘’âˆ’ğ‘ğœ†ğ‘§(ğ‘¥,ğ‘¦)), ğœ† ğœ– {ğ‘…, ğº, ğµ}

(4.2)

A Figura 19 apresenta uma tÃ­pica imagem com alto nÃ­vel de turbidez. Turbidez Ã©
uma propriedade comum no meio aquÃ¡tico que esta relacionada com a quantidade de luz
que Ã© absorvida ou espalhada ao invÃ©s de ser transmitida em uma linha reta (OMAR;
MATJAFRI, 2009).

Figura 19 â€“ Imagem de exemplo para as degradaÃ§Ãµes do ambiente subaquÃ¡tico. Ã‰ possÃ­vel
ver que existe uma variaÃ§Ã£o conforme a distÃ¢ncia e uma perda significativa da informaÃ§Ã£o
de cor.

66

CapÃ­tulo 4. ClassificaÃ§Ã£o de Imagens do Assoalho OceÃ¢nico

Ã‰ interessante observar que a degradaÃ§Ã£o nÃ£o afeta uniformemente a imagem.
Existem nÃ­veis de degradaÃ§Ã£o mais altos de acordo com a distÃ¢ncia. AlÃ©m disso, o com-
primento de onda vermelho tende a se perder rapidamente, tendo a cor verde nesse caso
como predominante.

Por fim, vale notar que, fenÃ´menos adicionais tambÃ©m acontecem. Um exemplo Ã©
o efeito da "neve submarina", a qual causa aparecimento de pequenos pontos brancos na
imagem. Por estes e outros fatos Ã© relevante constatar que o meio subaquÃ¡tico jÃ¡ tem uma
alta presenÃ§a de ruÃ­do (BAZEILLE et al., 2006).

4.2 ClassificaÃ§Ã£o AutÃ´noma de Imagens do fundo OceÃ¢nico

A Figura 20 mostra uma adaptaÃ§Ã£o do que Ã© usado pela maioria dos frameworks
em visÃ£o computacional para criaÃ§Ã£o de mapas temÃ¡ticos de mosaicos em ambientes
subaquÃ¡ticos (SHIHAVUDDIN et al., 2013).

Figura 20 â€“ A sequÃªncia utilizada para classificaÃ§Ã£o de imagens em meio subaquÃ¡tico.

Para classificar os objetos de uma imagem Ã© necessÃ¡rio passar por diversas etapas.
A seguir sÃ£o listadas as etapas apresentando algumas das tÃ©cnicas usadas na literatura:

âˆ™ PrÃ©-processamento: etapa fundamental em ambientes subaquÃ¡ticos. Normalmente Ã©
onde correÃ§Ãµes de cor (PIZARRO et al., 2008) e contraste (JOHNSON-ROBERSON;
KUMAR; WILLAMS, 2007) sÃ£o aplicadas para atenuar a degradaÃ§Ã£o e ressaltar
aspectos importantes das imagens subaquÃ¡ticas.

âˆ™ SegmentaÃ§Ã£o: Nesta etapa a imagem Ã© super-segmentada em regiÃµes com proprie-
dades similares. Tal etapa pode ser evitada, para o caso onde ocorre uma seleÃ§Ã£o
manual do que ser classificado.

âˆ™ ExtraÃ§Ã£o de Descritores: Ã© onde as caracterÃ­sticas relevantes para cada segmento
sÃ£o extraÃ­das e representadas. Diversas abordagens sÃ£o utilizadas, um exemplo seria
o uso de descritores locais e bag-of-words por Pizarro, Eustice e Singh (2004). Os

4.2. ClassificaÃ§Ã£o AutÃ´noma de Imagens do fundo OceÃ¢nico

67

descritores de textura e cor sÃ£o bastante utilizados no meio subaquÃ¡tico (BEIJBOM
et al., 2012) (STOKES; DEANE, 2009), (MARCOS; SORIANO; SALOMA, 2005).
âˆ™ ClassificaÃ§Ã£o: Ã© onde se realiza o treinamento do classificador e classificaÃ§Ã£o para os
testes. Diversos classificadores sÃ£o utilizados como o SVM (PIZARRO; EUSTICE;
SINGH, 2004) ou o LDA (MARCOS; SORIANO; SALOMA, 2005).

âˆ™ PÃ³s-processamento: Ã© onde informaÃ§Ãµes adicionais sÃ£o utilizadas para refinar o resul-
tado da classificaÃ§Ã£o. Em (SHIHAVUDDIN et al., 2013) Ã© feito um simples sistema
de votaÃ§Ã£o para verificar a consistÃªncia da vizinhanÃ§a No caso, atÃ© onde se sabe, nÃ£o
ocorreram outras aplicaÃ§Ãµes de tÃ©cnicas mais elaboradas para adiÃ§Ã£o de contexto.

Nesta seÃ§Ã£o Ã© especificado em detalhe cada etapa apresentada elucidando o que
foi utilizado por Shihavuddin et al. (2013) para geraÃ§Ã£o de mapas temÃ¡ticos. Tal mÃ©todo
foi escolhido como base para aplicaÃ§Ã£o de tÃ©cnicas para adiÃ§Ã£o de contexto. O mesmo foi
escolhido devido a alta taxa de acerto na classificaÃ§Ã£o quando comparados com diversos
mÃ©todos do estado da arte (SHIHAVUDDIN et al., 2013). Os algoritmos usados em cada
uma das etapas da Figura 20 sÃ£o elucidados a seguir.

4.2.1 PrÃ©-Processamento

O processo de prÃ©-processamento almeja deixar a imagem o mais prÃ³xima possÃ­vel
da cena em qual a mesma foi capturada. Isso Ã© feito tanto no escopo radiomÃ©trico quanto
geomÃ©trico. Ou seja, o objectivo Ã© tornar, as estruturas geomÃ©tricas , seu brilho e cor o
mais prÃ³ximos possÃ­vel da cena (GONZALEZ; WOODS, 2006).

Para lidar com o processamento embaixo dâ€™Ã¡gua, primeiramente, precisa-se consi-
derar todos os princÃ­pios bÃ¡sicos de propagaÃ§Ã£o da luz nesse meio os quais foram colocados
na SeÃ§Ã£o 4.1. (SCHETTINI; CORCHS, 2010)

Seguindo a ideia de que a qualidade visual subjetiva Ã© importante, pode-se melhor a
qualidade de imagens subaquÃ¡ticas utilizando tÃ©cnicas que abordam diretamente os efeitos
degradantes apontados. Esta seÃ§Ã£o apresenta as alternativas existentes para corrigir cada
um dos tipos de degradaÃ§Ã£o.

4.2.1.1 Contraste

Observa-se pela EquaÃ§Ã£o 4.1 que o processo de degradaÃ§Ã£o da imagem em ambiente
subaquÃ¡tico nÃ£o Ã© uniforme ao longo da imagem. O mesmo depende da distÃ¢ncia de cada
ponto a cÃ¢mera.

Nesse contexto, o Shihavuddin et al. (2013) faz o uso do CLAHE (Contrast Limited
Adaptative Histogram Equalization) (ZUIDERVELD, 1994) para correÃ§Ã£o de contraste.

68

CapÃ­tulo 4. ClassificaÃ§Ã£o de Imagens do Assoalho OceÃ¢nico

Tal mÃ©todo faz uma construÃ§Ã£o de histograma diferente para cada segmento da imagem
e aplica uma equalizaÃ§Ã£o de histograma somente nesse segmento. AlÃ©m disso, o mÃ©todo
coloca um limite de intensidade maxima, redistribuindo todos as intensidades que ficam
acima deste limite.

4.2.1.2 CorreÃ§Ã£o de Cor

Como mostrado na seÃ§Ã£o 4.1, existe uma nÃ£o uniformidade na absorÃ§Ã£o de cada
comprimento de onda no ambiente subaquÃ¡tico. Isso causa que boa parte da informaÃ§Ã£o
cromÃ¡tica da cena seja perdida.

De forma a obter cores mais prÃ³ximas de realidade existe a necessidade de estimar
tais diferenÃ§as de absorÃ§Ã£o. Uma das formas de resolver isso Ã© considerar que Ã© possÃ­-
vel obter as diferenÃ§as de absorÃ§Ã£o considerando essas diferenÃ§as como uma questÃ£o de
estimativa da fonte de luz. Colocado de tal forma, o problema, se torna basicamente a
aplicaÃ§Ã£o de algoritmos de balanceamento de branco, os quais podem ser uma simples
normalizaÃ§Ã£o.

O mÃ©todo de (SHIHAVUDDIN et al., 2013) aplicou a premissa de que que o
ponto de maior intensidade da imagem foi causado por reflexÃ£o perfeita. Desta forma a
iluminaÃ§Ã£o pode ser estimada achando o ponto de maior intensidade da imagem. Sendo
assim, para tornar a cor da imagem balanceada, o ganho para cada pixel pode ser dado
como:

ğ‘…ğ‘”ğ‘ğ‘›â„ğ‘œ = 1/ğ‘…ğ‘šğ‘ğ‘¥
ğºğ‘”ğ‘ğ‘›â„ğ‘œ = 1/ğºğ‘šğ‘ğ‘¥
ğµğ‘”ğ‘ğ‘›â„ğ‘œ = 1/ğµğ‘šğ‘ğ‘¥

(4.3)

4.2.2 SegmentaÃ§Ã£o

Diversos desafios em classificaÃ§Ã£o de imagens colocam o desafio atual como classi-

ficar os objetos pixel a pixel (FULKERSON; VEDALDI; SOATTO, 2009).

O caso da aplicaÃ§Ã£o em sensoriamento remoto, claramente se beneficia deste fato,
onde cada pixel da imagem Ã© relevante. A questÃ£o Ã© que, devido ao custo computacional,
e ao fato que somente um pixel nÃ£o possuir grande significado semÃ¢ntico para efetuar
a classificaÃ§Ã£o e extrair os descritores, muitas vezes a abordagem de usar segmentos da
imagem, ajuda a melhorar a consistÃªncia.

Existe e a tendÃªncia de muitos autores fazer uma prÃ©-segmentaÃ§Ã£o, a qual aparen-
temente nÃ£o esta relacionada com a classificaÃ§Ã£o final. PorÃ©m, tal segmentaÃ§Ã£o ajuda a
a garantir que cada parte da imagem sendo classificada tenha uma homogeneidade local.
Tal segmentaÃ§Ã£o Ã© chamada de segmentaÃ§Ã£o em superpixeis.

4.2. ClassificaÃ§Ã£o AutÃ´noma de Imagens do fundo OceÃ¢nico

69

Para o caso da abordagem de Shihavuddin et al. (2013), os superpixeis sÃ£o utiliza-
dos como estrutura de interaÃ§Ã£o. A imagem Ã© definida como um conjunto de superpixeis
a serem classificados.

Diversos algoritmos existem para a criaÃ§Ã£o de superpixeis. PorÃ©m, Shihavuddin et
al. (2013) selecionou aquele que tende a manter uma estrutura o mais regular possÃ­vel.
No caso foi utilizado os Turbopixels (LEVINSHTEIN et al., 2009) .

4.2.3 Descritores

Para descrever a imagem foi utilizado majoritariamente descritores de textura.
Textura pode ser definida como a variaÃ§Ã£o dos dados visuais em escalas menores que a
escala observada (PETROU; GARCÃA-SEVILLA, 2006).

O assoalho submarino Ã© tipicamente texturizado. Observou-se diversos bancos de
dados de corais, e outras estruturas encontradas no meio, e percebe-se que existem sem-
pre oscilaÃ§Ãµes na estrutura dos objetos em diferentes escalas. Tal fenÃ´meno caracteriza a
existÃªncia da textura. AlÃ©m da tendÃªncia existente na literatura em usar textura (SHIHA-
VUDDIN et al., 2013).

Com isso em vista, (SHIHAVUDDIN et al., 2013) utiliza trÃªs descritores como des-
critores de texturas: Gabor Filter, Grey Level Co-occurence Matrix (GLCM) e Completed
Local Binary Pattern (CLBP).

Os Gabor Filters sÃ£o um grupo de Wavelets 2D que tomam forma de uma gaussiana
2D modulada no espaÃ§o 2D (PORTER; CANAGARAJAH, 1997). Basicamente sÃ£o uma
representaÃ§Ã£o da variaÃ§Ã£o de frequÃªncia em um segmento da imagem.

O GLCM (HARALICK; SHANMUGAM; DINSTEIN, 1973), utiliza a represen-
taÃ§Ã£o de padrÃµes de variaÃ§Ãµes espaciais dos segmentos da imagem em uma matriz, que
representa a variaÃ§Ã£o de intensidade dos pixeis em diferentes Ã¢ngulos e distÃ¢ncias. Diver-
sos indicadores sÃ£o computados a partir dessas matrizes como a mÃ©dia de variaÃ§Ãµes ou a
entropia.

O CLBP (GUO; ZHANG, 2010), Ã© um descritor de textura invariante a rotaÃ§Ã£o o
qual retrata, principalmente, a variaÃ§Ã£o de sinais de um pixel central para com pixeis ao
redor em uma determinada posiÃ§Ã£o.

A utilizaÃ§Ã£o de cor Ã© complexa dado a perda de cor nÃ£o uniforme entre os compri-
mentos de onda como mostrado na SeÃ§Ã£o 4.1. PorÃ©m ainda Ã© possÃ­vel utilizar um descritor
de cor que possui propriedades importantes como robustez a variaÃ§Ãµes fotomÃ©tricas causa-
das por sombras, sombreamento e tambÃ©m mudanÃ§as geomÃ©tricas como escala e alteraÃ§Ã£o
de ponto de vista. (SHIHAVUDDIN et al., 2013) utiliza o trabalho de (WEIJER; SCH-
MID, 2006) que aproximou tais propriedades.

70

CapÃ­tulo 4. ClassificaÃ§Ã£o de Imagens do Assoalho OceÃ¢nico

Ao final, ao utilizar mÃºltiplos descritores, se tem uma representaÃ§Ã£o da imagem
com uma grande quantidade de dimensÃµes e muitas vezes com um padrÃ£o pouco evidente.
Para resolver isso Ã© aplicado normalizaÃ§Ãµes e modificaÃ§Ãµes nos descritores. Por exemplo,
os descritores podem ser manipulados de forma que os mesmos sejam o mais prÃ³ximos a
se tornaram linearmente separÃ¡veis. Essa modificaÃ§Ã£o Ã© fundamental para se melhorar a
qualidade da classificaÃ§Ã£o. Por fim, os descritores sÃ£o normalizados de forma a que todos
os descritores estejam numa escala compatÃ­vel.

4.2.4 Treinamento e ClassificaÃ§Ã£o

O treinamento foi feito utilizando trÃªs classificadores distintos de forma mutu-
almente exclusiva. Foram utilizados o Support Vector Machine (SVM), o K-nearest-
neighbors e o PDWMD proposto por (STOKES; DEANE, 2009). Cada um destes classi-
ficadores foi utilizado dependendo das caracterÃ­sticas dos dados.

Dado que o aprendizado foi feito, novos dados podem ser classificados. Ã‰ feito
um mapa temÃ¡tico baseado na segmentaÃ§Ã£o em superpixeis. Sendo que cada superpixel Ã©
classificado individualmente.

4.3 ConclusÃµes

Neste capÃ­tulo apresentou-se o cenÃ¡rio onde vai ser feito o estudo desta dissertaÃ§Ã£o.
TambÃ©m se apresentou alguns mÃ©todos os quais jÃ¡ fizeram classificaÃ§Ã£o de imagens do
bentos.

No capÃ­tulo 6 serÃ£o apresentados os resultados de aplicaÃ§Ã£o do mÃ©todo de (SHIHA-
VUDDIN et al., 2013) e serÃ¡ feito o estudo sobre a incorporaÃ§Ã£o de contexto para esse
mÃ©todo.

5 Testes e Resultados 1: DetecÃ§Ã£o de Pontos

de Interesse em Ambiente SubaquÃ¡tico

71

Uma das principais contribuiÃ§Ãµes desta dissertaÃ§Ã£o foi a criaÃ§Ã£o de um experi-
mento para analizar e compreender o comportamento dos detectores de pontos de interesse
quando utilizados em ambiente subaquÃ¡tico.

Como apresentado no CapÃ­tulo 1, diversos detectores foram desenvolvidos para
serem invariantes a uma serie de fenÃ´menos. A ideia Ã© que o mesmo ponto de interesse
possa ser encontrado independentemente de diversas circunstÃ¢ncias da cena.

PorÃ©m, existem fenÃ´menos adicionais que atuam sobre a cena no ambiente su-
baquatico os quais devem ser considerados. Quando a luz se propaga neste meio, ela Ã©
absorvida e espalhada pelos diferentes coeficientes de refraÃ§Ã£o encontrados nas particulas
presentes no meio. Isso espalha a informaÃ§Ã£o capturada e cria o efeito de "enevoado"na
imagem. Tais fenÃ´menos foram descritos mais detalhadamente no CapÃ­tulo 4, SeÃ§Ã£o 4.1.
Um estudo feito por Garcia e Gracias (GARCIA; GRACIAS, 2011), comparou
os detectores de pontos de interesse mais populares na literatura. Eles encontraram que
estruturas do tipo blob, obtidos por mÃ©todos baseados em Hessian (BEAUDET, 1978),
por exemplo, sÃ£o melhores detectadas tanto para o caso de mÃ©todos invariantes a escala
como os de Ãºnica escala. A justificativa Ã© que a turbidez da Ã¡gua tende a suavizar quinas e
borrar regiÃµes definidas, fazendo com que mÃ©todos como Harris (HARRIS; STEPHENS,
1988) ou Harris-Laplace (MIKOLAJCZYK; SCHMID, 2004) sejam menos propÃ­cios para
o ambiente. Entretanto, eles avaliaram somente algumas estruturas em uma Ãºnica cena.
Ã‰ do interesse desta dissertaÃ§Ã£o melhorar este estudo. Neste contexto, alguns principais
objetivos sÃ£o buscados.

Foi proposto um novo dataset no qual Ã© possÃ­vel utilizar diferentes estruturas
submarinas obtidas atravÃ©s da impressÃ£o de fotos subaquÃ¡ticas. Estas estruturas foram
refotografadas dentro de um tanque de Ã¡gua onde imagens com a degradaÃ§Ã£o controlada
foram produzidas. Isso Ã© uma melhoria a tentativas anteriores em termos de diversidade
de elementos visuais. Considerando que a degradaÃ§Ã£o causada por imagens com baixa e
alta turbidez nÃ£o Ã© linear, uma contribuiÃ§Ã£o Ã© dividir a anÃ¡lise em diferentes intervalos
de turbidez.

Foram testados detectores de pontos de interesse, considerando diferentes aborda-
gens, com respeito a sua robustez a degradaÃ§Ã£o causada pela turbidez. Foi focado inves-
tigar o problema de que detectores invariantes a escala tendem a ter baixa performance
(GARCIA; GRACIAS, 2011). Isto Ã© feito atravÃ©s da anÃ¡lise de diferentes espaÃ§os de es-

72

CapÃ­tulo 5. Testes e Resultados 1: DetecÃ§Ã£o de Pontos de Interesse em Ambiente SubaquÃ¡tico

cala. Finalmente, foi indicado o melhor detector invariante para imagens subaquÃ¡ticas
como sendo o DoG (LOWE, 2004).

Este CapÃ­tulo estÃ¡ organizado da seguinte maneira. A seÃ§Ã£o 5.1 apresenta a des-
criÃ§Ã£o completa do experimento a ser realizado. Tal seÃ§Ã£o mostra todos os detalhes da
experimentaÃ§Ã£o necessÃ¡rios para que o mesmo seja bem sucedido. TambÃ©m explica to-
das as consideraÃ§Ãµes feitas para se ter dados aceitaveis. Por fim, a SeÃ§Ã£o 5.3 mostra os
resultados obtidos para tal experimento, e apresenta uma discussÃ£o sobre os resultados
encontrados.

5.1 DescriÃ§Ã£o do experimento

Na literatura, poucos sÃ£o os trabalhos que analisam o comportamento dos detec-
tores de pontos de interesse em ambiente subaquÃ¡tico. Nesta seÃ§Ã£o, descreve-se todo o
processo de realizaÃ§Ã£o do experimento para que ele seja completamente reproduzÃ­vel.

Neste experimento foram capturadas diversas imagens em uma cena onde a Ãºnica
modificaÃ§Ã£o entra as cenas Ã© a degradaÃ§Ã£o causada pela turbidez. O objetivo fundamental
do experimento Ã© tentar obter o mÃ¡ximo de isolamento desta degradaÃ§Ã£o possÃ­vel. Para
tal, a cÃ¢mera utilizada deve estar estÃ¡tica e a iluminaÃ§Ã£o deve ser controlada.

5.1.1 Cena Montada

ConstruÃ­-se uma cena onde as imagens foram colocadas. A Figura 21 mostra a

especificaÃ§Ã£o da cena.

Figura 21 â€“ A cena criada para avaliar os algoritmos de avaliaÃ§Ã£o de features. Ela Ã© com-
posta por lampadas fluorescentes e uma camera fotografando fotos impressas do assoalho
do oceano.

Na cena montada existe uma fotografia a ser capturada por uma cÃ¢mera posicio-
nada a uma distÃ¢ncia perpendicular de 0.58ğ‘ğ‘š . A fotografia esta posicionada em uma

5.1. DescriÃ§Ã£o do experimento

73

caixa de Ã¡gua de mil litros. Duas luminÃ¡rias usando lÃ¢mpadas fluorescentes brancas foram
posicionadas perto do tanque.

TrÃªs fotografias diferentes foram utilizadas, representando o fundo do mar captu-
rado nas Bahamas em condiÃ§Ãµes prÃ³ximas ao ideal de turbidez (ZVULONI et al., 2009). As
diferentes cenas contÃ©m os mais variados tipos de textura que podem ser encontradas no
ambiente subaquÃ¡tico e tambÃ©m objetos feitos pelo homem. As fotografias foram impres-
sas usando um "ploter"a laser usando uma mÃ­dia de vinil adesivo fosco e a prova dâ€™Ã¡gua.
Cada fotografia foi impressa num tamanho de 91cm X 60 cm e possuem 4928x3264 pixeis
de resoluÃ§Ã£o. O diferencial desta deste dataset Ã© que ele contÃ©m verdadeiras estruturas
do assoalho oceano, e ainda algumas estruturas feitas pelo homem, sendo o seu princi-
pal problema, a perda de resoluÃ§Ã£o devido a impressÃ£o e a refotografia. Isso cria uma
perda de resoluÃ§Ã£o de 20 ğ‘ğ‘–ğ‘¥ğ‘’ğ‘™ğ‘ /ğ‘šğ‘š2 para 4 ğ‘ğ‘–ğ‘¥ğ‘’ğ‘™ğ‘ /ğ‘šğ‘š2 e adiÃ§Ã£o de algumas pequenas
imperfeiÃ§Ãµes devido a erros de impressÃ£o.

A Figura mostra as imagens que foram impressas, nomea-se cada uma das imagens

como ğ‘ƒ1, ğ‘ƒ2 e ğ‘ƒ3.

A cÃ¢mera utilizada para a captura foi uma Gopro Hero 3 Black edition. Cada

imagem foi capturada em uma resoluÃ§Ã£o de 12 mega pixels(3000x4000).

5.1.2 Procedimento

Foi decidido simular principalmente o efeito do fenÃ´meno de backscattering. Sabe-
se que os motivos que levam a degradaÃ§Ã£o de uma imagem capturada em meio subaquÃ¡tico
sÃ£o complexos (DUNTLEY, 1963). PorÃ©m, neste experimento tentou-se isolar o princi-
pal fenÃ´meno que causa a degradaÃ§Ã£o na imagem. Um estudo feito por Narasimhan et
al. (2006) mostra que uma soluÃ§Ã£o de Ã¡gua e leite integral apresenta um alto grau de
backscattering, apontado por alguns como a principal fonte de degradaÃ§Ã£o da imagem
(TREIBITZ; SCHECHNER, 2006). Isso Ã© causado pelo maior tamanho das partÃ­culas do
leite integral que fazem que o Ã¢ngulo de refraÃ§Ã£o seja maior, aumentando o backscattering.
Foi decidido dividir o experimento em 3 ensaios, cada um contendo uma imagem

diferente.

Cada ensaio foi capturado com 19 nÃ­veis de turbidez diferentes, cada um contendo
uma determinada quantidade de leite. Chamou-se cada nÃ­vel de turbidez de ğ‘‡1...ğ‘‡19.
Considera-se ğ‘‡0 como o nÃ­vel de turbidez com a imagem limpa. A Tabela 1 mostra os nÃ­veis
de turbidez e suas respectivas quantidades de leite (Em uma caixa com aproximadamente
1000 litros de Ã¡gua).

Para capturar as imagens, a cÃ¢mera foi setada para capturar uma foto a cada
10 segundos. Para cada nÃ­vel de turbidez foi escolhido um grupo de fotos com o menor
nÃ­vel de perturbaÃ§Ã£o. Como explicado no CapÃ­tulo 4, SeÃ§Ã£o 4.1, o meio subaquÃ¡tico Ã©

74

CapÃ­tulo 5. Testes e Resultados 1: DetecÃ§Ã£o de Pontos de Interesse em Ambiente SubaquÃ¡tico

(a) ğ‘ƒ1

(b) ğ‘ƒ2

(c) ğ‘ƒ3

Figura 22 â€“ As imagens utilizadas no teste. As trÃªs imagens foram capturadas nas Bahamas
em condiÃ§Ãµes de turbidez prÃ³ximas do ideal em uma resoluÃ§Ã£o de 4928x3264 pixeis

composto por uma certa quantidade de ruÃ­do. Em um ambiente controlado, como o que
foi feito Ã© possÃ­vel, tendo uma seleÃ§Ã£o de fotos em um mesmo nÃ­vel de turbidez ğ‘‡ğ‘–, reduzir
o ruÃ­do extraindo mediana entre as imagens iguais (GARCIA; GRACIAS, 2011). Desta
forma busca-se reduzir a degradaÃ§Ã£o na imagem por ruÃ­dos que podem ter diversas causas

5.2. Avaliando a degradaÃ§Ã£o causada pela turbidez

75

Imagem (ğ‘‡ğ‘–) Quantidade de Leite Integral Leite Adicionado

5 ml
5 ml
5 ml
5 ml
5 ml
5 ml
6 ml
6 ml
8 ml
8 ml
8 ml
8 ml
8 ml
8 ml
10 ml
10 ml
10 ml
10 ml
60 ml

T1
T2
T3
T4
T5
T6
T7
T8
T9
T10
T11
T12
T13
T14
T15
T16
T17
T18
T19

5 ml
10 ml
15 ml
20 ml
25 ml
30 ml
36 ml
42 ml
50 ml
58 ml
66 ml
74 ml
82 ml
90 ml
100 ml
110 ml
120 ml
130 ml
190 ml

Tabela 1 â€“ A quantidade de leite adicionada para cada nÃ­vel de turbidez simulado.

como erro no sensor da cÃ¢mera, partÃ­culas bloqueando totalmente a passagem da luz, etc.
Tenta-se de certa forma isolar significativamente a degradaÃ§Ã£o por turbidez (IDT), como
o principal fenÃ´meno da cena.

A Figura 23 apresenta as imagens geradas pelo experimento. Neste caso fez-se
a distinÃ§Ã£o entre diversos intervalos de turbidez. Na Figura 23 Ã© mostrado um nÃ­vel de
turbidez por intervalo, para cada imagem.

5.2 Avaliando a degradaÃ§Ã£o causada pela turbidez

Medir a quantidade de degradaÃ§Ã£o Ã© fundamental neste experimento de forma a
comparar os detectores somente relativo a este fenÃ´meno. A degradaÃ§Ã£o causada pela tur-
bidez Ã© dependende da quantidade de particulas em suspenÃ§Ã£o na Ã¡gua, e tambÃ©m os tipos
de particulas em suspenÃ§Ã£o. AlÃ©m disso, a quantidade de iluminaÃ§Ã£o e a maneira como a
cena Ã© iluminada Ã© tambÃ©m fundamental para determinaÃ§Ã£o da degradaÃ§Ã£o causada pela
turbidez.

Este conceito difere do conceito de turbidez que esta relacionado somente com a
quantidade de sedimentos flutuantes (SSC) na Ã¡gua os quais espalham a luz. A degradaÃ§Ã£o
causada pela turbidez difere pois ela nÃ£o esta relacionado somente as partÃ­culas presentes
na Ã¡gua e sim a degradaÃ§Ã£o que o SSC causa na cena, levando em conta os parÃ¢metros

76

CapÃ­tulo 5. Testes e Resultados 1: DetecÃ§Ã£o de Pontos de Interesse em Ambiente SubaquÃ¡tico

Figura 23 â€“ As imagens capturadas sob diferentes nÃ­veis de degradaÃ§Ã£o devido a turbi-
dez, controlado pela adiÃ§Ã£o de leite. Foram fotografadas trÃªs fotos impressas diferentes,
ğ‘ƒ1 (primeira coluna), ğ‘ƒ2 (segunda coluna) e ğ‘ƒ3 (terceira coluna). Na primeira linha foi
mostrada a imagem limpa (sem leite) para cada foto capturada. A segunda linha apre-
senta o intervalo de Baixa Turbidez com por volta de 15ml de leite (ğ‘‡4). O intervalo de
MÃ©dia Turbidez Ã© mostrado na segunda linha e contÃ©m por volta de 50 ml de leite (ğ‘‡10).
Finalmente, na ultima (quarta) linha Ã© mostrado o intervalo com Alta turbidez tendo por
volta de 100 ml de leite (ğ‘‡16). Quantidade de leite setada para uma caixa com 1000 litros
de Ã¡gua.

da cÃ¢mera e o volume de Ã¡gua iluminado.

Uma forma de medir a turbidez Ã© usando um turbidÃ­metro nefelÃ´metro, o qual
mede a turbidez pela quantidade de luz espalhada ao emitir um feixe de laser numa
porÃ§Ã£o da Ã¡gua.

Esta alternativas nÃ£o Ã© capaz de estimar a degradaÃ§Ã£o causada pela turbidez,
que Ã© tambÃ©m dependente da cena. Com essas consideraÃ§Ãµes, Garcia e Gracias (2011)
propuseram a utilizaÃ§Ã£o de uma variaÃ§Ã£o Structural Similarity Index (WANG et al., 2004),
para avaliar a degradaÃ§Ã£o, chamado Structural Degradation Index (SDI). Essa abordagem
avalia a degradaÃ§Ã£o pela perda de informaÃ§Ã£o estrutural, o que de fato esta relacionado

5.3. Resultados

77

com a turbidez. PorÃ©m, a mesma nÃ£o tenta isolar a mediÃ§Ã£o do fenÃ´meno de absorÃ§Ã£o e
espalhamento como principais causadores da degradaÃ§Ã£o.

Neste trabalho utiliza-se a mÃ©trica proposta por (GARCIA; GRACIAS, 2011),
porÃ©m normalizada em funÃ§Ã£o da imagem completamente turva pelo leite. Tal mÃ©trica
Ã© capaz de medir a porcentagem de degradaÃ§Ã£o em funÃ§Ã£o da imagem onde teve sua
informaÃ§Ã£o visual inicial completamente eliminada. O mÃ©todo Ã© explicado na secÃ§Ã£o 5.3.1

5.3 Resultados

Nesta seÃ§Ã£o sÃ£o mostradas a comparaÃ§Ãµes entre os detectores. Foram comparados
os seguintes detectores, previamente definidos no CapÃ­tulo 1. Para Ãºnica escala, Harris
(HARRIS; STEPHENS, 1988), Hessian (BEAUDET, 1978) e Laplacian (TUYTELAARS;
MIKOLAJCZYK, 2008). Com mÃºltiplas escalas avaliou-se Fast Hessian do SURF (BAY
et al., 2008) e Difference of Gaussians do SIFT (LOWE, 2004). Foram avaliados tambÃ©m
outros detectores com propriedades relevantes. Os trÃªs kernels baseados em difusÃ£o aniso-
trÃ³pica do detector KAZE (ALCANTARILLA; BARTOLI; DAVISON, 2012) e o gerado
pelo centro e arredores usando estruturas poligonais CenSurE (AGRAWAL; KONOLIGE;
BLAS, 2008) usando tanto um polÃ­gono convexo de seis lados e um polÃ­gono estrelado de
tambÃ©m seis lados.

5.3.1 Procedimento de AvaliaÃ§Ã£o

Os resultados sÃ£o avaliados quanto ao critÃ©rio de repetibilidade descrito em (SCH-
MID; MOHR; BAUCKHAGE, 2000). Tal critÃ©rio indica a porcentagem dos pontos de
interesse que se repetiram, ou seja, ainda foram encontrados apÃ³s a aplicaÃ§Ã£o da trans-
formaÃ§Ã£o.

Primeiramente computa-se ğ‘ = 1000 pontos de interesse para cada detector na
imagem com a turbidez ğ‘‡0 e para todos os nÃ­veis ğ‘‡1...ğ‘‡19. Os ğ‘ pontos de interesse
selecionados sÃ£o os N melhores pontos de interesse segundo o critÃ©rio do detector, no caso
Hessian ou Harris. Na imagem com turbidez ğ‘‡0 Ã© selecionada cada ponto-chave e Ã© testado
se esse ponto Ã© resistente na presenÃ§a de turbidez. Para esse ponto-chave ser resistente Ã©
necessÃ¡rio que o mesmo seja encontrado nas imagens turvas sem sofrer um deslocamento
maior que um fator de ğ‘’ = 5 pixeis. Esse valor Ã© determinado de forma a escolher somente
os melhores pontos de interesse. Subsequentemente, para determinar a repetibilidade de
um certo detector, o nÃºmero de pontos chaves encontrados em cada imagem tÃºrbida sÃ£o
contados. Considerando essa questÃ£o, a repetibilidade quanto ao degradaÃ§Ã£o por turbidez

78

CapÃ­tulo 5. Testes e Resultados 1: DetecÃ§Ã£o de Pontos de Interesse em Ambiente SubaquÃ¡tico

(ğ‘…) Ã© calculada como:

ğ‘… = ğ‘ğ‘–
ğ‘0

(5.1)

Onde ğ‘0 Ã© o nÃºmero de pontos de interesse na imagem limpa (capturada em ğ‘‡0) e ğ‘ğ‘– Ã©
a imagem com a degradaÃ§Ã£o estimada.

Para medir a degradaÃ§Ã£o causada pela turbidez, foi usado uma versÃ£o diferente do
SDI (WANG et al., 2004) (GARCIA; GRACIAS, 2011). O Ã­ndice SDI nÃ£o responde com
os mesmos valores para as mesmas quantidades de turbidez. Por esta razÃ£o, foi utilizado
uma versÃ£o normalizada do SDI. Considerando a imagem ğ‘‡19 como sendo totalmente
degradada, pode-se medir o SDI como uma percentagem da degradaÃ§Ã£o mÃ¡xima, o que
facilita a comparaÃ§Ã£o:

ğ‘ ğ‘†ğ·ğ¼ğ‘– = ğ‘†ğ·ğ¼ğ‘–/ğ‘†ğ·ğ¼ğ‘

(5.2)

Onde ğ‘†ğ·ğ¼ğ‘ Ã© o Ã­ndice de degradaÃ§Ã£o da imagem ğ‘‡19.

5.3.2 ComparaÃ§Ã£o

A Figura 24, mostra os grÃ¡ficos com os valores de repetibilidade para as trÃªs fotos
impressas (ğ‘ƒ1,ğ‘ƒ2,ğ‘ƒ3) testando multiplos detectores. No eixo ğ‘¥ Ã© mostrado o indice ğ‘ ğ‘†ğ·ğ¼
e a quantidade de leite adicionada.

Da Figura 24, sÃ£o mostrados as analises para trÃªs intervalos diferentes de degra-
daÃ§Ã£o causada por turbidez baseado no NSDI . Desde 0 a 0.25 de ğ‘ ğ‘†ğ·ğ¼ foi considerado
como um ambiente de Baixa Turbidez(Fig. 23 segunda linha). Nestes casos a maioria da
informaÃ§Ã£o estrutural Ã© mantida e o backscattering Ã© mÃ­nimo. No intervalo de 0.25 atÃ©
0.75 foi considerado como imagens de parte de um intervalo de MÃ©dia Turbidez( Fig. 23
terceira linha). Nestes nÃ­veis, a informaÃ§Ã£o estrutural Ã© parcialmente mantidas, mas as
bordas passam a ser mal definidas. Ao final, desde 0.75 atÃ© 1, em Alta Turbidez(Fig. 23
quarta linha), quase nenhuma informaÃ§Ã£o estrutural Ã© mantida. Nestes nÃ­veis, os detec-
tores podem somente fazer uso de algumas poucas pistas visuais que ainda resistiram a
turbidez.

Para todos os intervalos de turbidez, Ã© possÃ­vel separar claramente os detectores

analisados em quatro grupos.

Os detectores baseados em Ãºnica escala (Azul Fig. 24) obtiveram os melhores
resultados em todos os intervalos de turbidez. Comparado com outras comparaÃ§Ãµes de
detectores de pontos de interesse (GIL et al., 2010) (CRISTINACCE; COOTES, 2006),
a superioridade dos detectores nÃ£o invariantes a escala em comparaÃ§Ã£o a aqueles que sÃ£o

5.3. Resultados

79

Figura 24 â€“ Repetibilidade ( Taxa de Acerto) contra o indice de degradaÃ§Ã£o estrutural
normalizado (NSDI). As linhas em laranja indicam os intervalos de degradaÃ§Ã£o. Baixa
Turbidez 0 atÃ© 0.25; MÃ©dia Turbidez, 0.25 atÃ© 0.75, e Alta Turbidez de 0.75 atÃ© 1.

80

CapÃ­tulo 5. Testes e Resultados 1: DetecÃ§Ã£o de Pontos de Interesse em Ambiente SubaquÃ¡tico

invariantes a escala, em situaÃ§Ãµes onde a escala nÃ£o varia, Ã© mais expressiva. O detector
Harris foi melhor para o caso de ğ‘ƒ1 (Fig. 24a) atÃ© um nÃ­vel mÃ©dio de turbidez. ApÃ³s isso
o mesmo teve um decaimento maior, quando as estruturas comeÃ§aram a se perder.

O detector baseado em espaÃ§os de escala com difusÃ£o anisotrÃ³pica, obteve os piores
resultados (Vermelho Fig. 24). Isso Ã© o oposto do que Ã© mostrado em cenas fora dâ€™Ã¡gua
(ALCANTARILLA; BARTOLI; DAVISON, 2012). O algoritmo KAZE necessita calcular
as respostas das bordas antes de obter o espaÃ§o de escala. Isso Ã© mais dificil em ambientes
subaquÃ¡ticos devido a suas propriedades naturais. PorÃ©m, no intervalo de Baixa Turbidez,
KAZE foi capaz de obter uma repetibilidade maior que o FastHessian e o DoG, dado que
as bordas ainda estÃ£o bem definidas. Para o caso de MÃ©dia Turbidez, a taxa de acerto cai
rapidamente, chegando a zero em Alta Turbidez.

Os melhores resultados para nÃ­veis de media e alta turbidez, foram, de fato, obtidos
pelo detector DoG (LOWE, 2004). O detector Fast Hessian, possui uma aproximaÃ§Ã£o mais
brusca do espaÃ§o de escala a qual tende a produzir artefatos. Por isso, tratou-se do pior
resultado dentre os analisados.

CenSurPoly e o CenSurStar apresentaram resultados similares ao Kaze tendo pi-

ores resultados para nÃ­veis mais altos de turbidez.

A Figura 25 mostra a comparaÃ§Ã£o de um determinado nÃ­vel de espaÃ§o de escala
gerado por um kernel gaussiano , um kernel baseado em caixas (FastHessian), um kernel
baseado em polÃ­gonos estrelados (CenSurE) e um gerado pelo filtro anisotrÃ³pico (KAZE)
ğ‘”2 da Eq. 1.12 . Tais kernels sÃ£o aplicados em mÃºltiplos nÃ­veis de turbidez, sendo que
cada linha da figura apresenta um nÃ­vel de turbidez diferente.

Ã‰ possÃ­vel perceber que a informaÃ§Ã£o estrutural se mantÃ©m mais para o polÃ­gono
estrelado. O que justifica o seu estudo, principalmente para o caso de maior turbidez. JÃ¡
o KAZE tambÃ©m possui um comportamento interessante, porÃ©m muito da informaÃ§Ã£o
tende a se perder com a turbidez para um mesmo nÃ­vel de escala.

Como mostrado no CapÃ­tulo 4, SeÃ§Ã£o 4.1 , existe um comportamento de borra-
mento regido por um certo fenÃ´meno. Ã‰ possÃ­vel que funÃ§Ãµes, como as utilizadas pelo
CenSurE e o KAZE, as quais tendem a nÃ£o seguir o comportamento do borramento cau-
sado pelas propriedades do meio subaquÃ¡tico, tendam a manter as estruturas geomÃ©tricas
, e , ao encontrar pontos que possuem mÃ¡ximo sobre escala, encontrem regiÃµes em que
ainda existe informaÃ§Ã£o visual provida pela imagem.

5.4 ConclusÃµes finais

Este capÃ­tulo apresentou a avaliaÃ§Ã£o a invariÃ¢ncia a degradaÃ§Ã£o em ambientes
subaquÃ¡ticos para detectores de pontos de interesse mais utilizados na literatura. Foi

5.4. ConclusÃµes finais

81

Figura 25 â€“ ComparaÃ§Ã£o entre a geraÃ§Ã£o de um nÃ­vel do kernel do espaÃ§o de escala usado
por quatro detectores diferentes. O kernel foi aplicado em nÃ­veis de turbidez diferentes
para a imagem ğ‘ƒ1. Sendo que a primeira linha Ã© a imagem limpa (ğ‘‡0), a segunda linha
Ã© uma imagem com baixo nÃ­vel de degradaÃ§Ã£o (ğ‘‡4), a terceira linha apresenta uma ima-
gem com mÃ©dio nÃ­vel de degradaÃ§Ã£o (ğ‘‡10), a quarta linha apresenta imagens do nÃ­vel de
degradaÃ§Ã£o alto (ğ‘‡16). Para cada caso Ã© mostrado o resultado de filtro equivalente a a
aproximadamente um kernel gaussiano de ğœ = 59.0. Primeira Coluna: Gaussiano puro.
Segunda Coluna: Borramento aproximado em caixas . Terceira Coluna: DifusÃ£o utilizando
um polÃ­gono estrelar de seis pontas. Quarta Coluna: kernel anisotrÃ³pico g2 do KAZE. Ã‰
possÃ­vel ver de certa forma estruturas mais definidas para o esquema de difusÃ£o usado
pelo CenSurE (AGRAWAL; KONOLIGE; BLAS, 2008).

proposto um novo dataset, completamente aberto, usando fotos impressas reais as quais
tinham uma quantidade controlada de turbidez.

Foi concluÃ­do que para, imagens subaquÃ¡ticas, mÃ©todos de Ãºnica escala tem uma
repetibilidade consideravelmente melhor que abordagens de multipla escala. Entre Harris
(HARRIS; STEPHENS, 1988) e Hessian (BEAUDET, 1978), Hessian obteve resultados
melhores principalmente para nÃ­veis mais altos de turbidez e em imagens onde hÃ¡ pouca
informaÃ§Ã£o estrutural.

82

CapÃ­tulo 5. Testes e Resultados 1: DetecÃ§Ã£o de Pontos de Interesse em Ambiente SubaquÃ¡tico

Considerando mÃºltipla escala, foram avaliados novos detectores os quais nÃ£o usam
os espaÃ§os de escala Gaussianos. Foi proposto que nestes espaÃ§os diferentes, como os center
surround ou os baseados em difusÃ£o anisotrÃ³pica, a difusÃ£o nÃ£o acontece com a mesma
estrutura que o fenÃ´meno de degradaÃ§Ã£o da turbidez, assim entÃ£o produzindo melhores
resultados, em alguns nÃ­veis de turbidez.

Os melhores resultados para mÃºltipla escala foram obtidos pelo DoG (LOWE,
2004) TambÃ©m mostrou-se que o KAZE (ALCANTARILLA; BARTOLI; DAVISON, 2012)
apresenta resultados relevantes mas tende a perder precisÃ£o em nÃ­veis mais altos de tur-
bidez.

Finalmente, a avaliaÃ§Ã£o proposta mostra que um espaÃ§o de escala nÃ£o Gaussiano
pode tambÃ©m produzir melhores resultados. Como trabalho futuro, buscar-se-Ã¡ explorar
que espaÃ§os de escala que consideram a degradaÃ§Ã£o causada pela turbidez de forma a
obter melhores resultados de repetibilidade.

6 Testes e Resultados 2: Contexto em Clas-

sificaÃ§Ã£o SubaquÃ¡tica

83

Aqui sÃ£o apresentados os resultados de aplicaÃ§Ã£o do mÃ©todo proposto baseado em
GeoestatÃ­stica (no Cap. 3), comparado com outros mÃ©todos, com e sem a incorporaÃ§Ã£o
do contexto.

O mÃ©todo serÃ¡ aplicado em mosaicos de imagens do assoalho oceÃ¢nico, para obten-
Ã§Ã£o de mapas temÃ¡ticos. As imagens resultantes sÃ£o a representaÃ§Ã£o final de um mosaico,
feito de forma visualmente interpretÃ¡vel, contendo a classificaÃ§Ã£o realizada de forma pixel-
a-pixel.

O capÃ­tulo apresenta os datasets, compostos por mosaicos, utilizados como caso de
teste para a classificaÃ§Ã£o e tambÃ©m as configuraÃ§Ãµes utilizadas para os testes e, por fim,
os resultados da classificaÃ§Ã£o dos mosaicos sÃ£o mostrados.

6.1 Datasets Utilizados

Para avaliaÃ§Ã£o dos resultados obtidos foi proposto utilizar dois datasets distintos
de mosaicos de recifes de corais. Cada dataset Ã© composto por um mosaico obtido pela
junÃ§Ã£o de centenas de imagens coletadas por especialistas da Universidade de Miami.

O dataset Redsea contÃ©m imagens do Mar Vermelho, capturadas em Ã¡guas bastante
rasas perto da cidade de Eilat, como parte de uma pesquisa por ecologistas de recifes
de corais (ZVULONI et al., 2009). Para a classificaÃ§Ã£o, foram considerado cinco classes:
Urchin, Branching Coral, Brain Coral, Favid Coral e o Background. Os mosaicos utilizados
foram capturados a uma resoluÃ§Ã£o de 1.1 ğ‘ğ‘–ğ‘¥ğ‘’ğ‘–ğ‘ /ğ‘šğ‘š2.

O segundo dataset chamado Marker, foi capturado nas Bahamas. Foi feita a divisÃ£o
em quatro classes para classificaÃ§Ã£o. General Corals, Sea Gorgons, Sand e Background.
Os mosaicos utilizados foram capturado em uma resoluÃ§Ã£o de 2.2 ğ‘ğ‘–ğ‘¥ğ‘’ğ‘–ğ‘ /ğ‘šğ‘š2.

6.2 DescriÃ§Ã£o do Geral do Sistema

Nesta seÃ§Ã£o Ã© descrito uma versÃ£o geral do sistema, tanto para a classificaÃ§Ã£o em

nÃ­vel unÃ¡rio, quanto os tipos de classificaÃ§Ã£o integrando contexto.

84

CapÃ­tulo 6. Testes e Resultados 2: Contexto em ClassificaÃ§Ã£o SubaquÃ¡tica

6.2.1 PrÃ©-Processamento

Tanto para os dados do dataset Redsea quanto para o caso do dataset Marker , a
qualidade visual da imagem Ã© bastante satisfatÃ³ria, contendo baixa presenÃ§a de degrada-
Ã§Ã£o devido a turbidez. O principal tipo de degradaÃ§Ã£o encontrado Ã© a variaÃ§Ã£o de cor ao
longo do datasets existentes durante a captura. Para resolver esta questÃ£o foi utilizado
CLAHE (ZUIDERVELD, 1994) e uma normalizaÃ§Ã£o de cor em ambos os datasets.

6.2.2 SegmentaÃ§Ã£o e DescriÃ§Ã£o

Os datasets foram segmentados como superpixeis baseados em TurboPixels (LE-
VINSHTEIN et al., 2009). O tamanho do pixel foi escolhido como que para caber em uma
janela de tamanho de aproximadamente 32x32 pixeis.

Para cada superpixel, a combinaÃ§Ã£o entre trÃªs descritores de textura foi utilizada.
Filtros de Gabor, CLBP e GLCM. Um kernel mapping Ã© feito depois para tornar os
descritores mais linearmente separÃ¡veis. O resultado Ã© tambÃ©m por fim, normalizado.

6.2.3 ClassificaÃ§Ã£o

No caso, para todos os testes, foi utilizado um SVM configurado com um kernel

linear.

6.2.4

AdiÃ§Ã£o de Contexto

A adiÃ§Ã£o de contexto Ã© apresentada feita de duas formas distintas: utilizando os
Conditional Random Fields (CRF) e utilizando o modelo de GeoestatÃ­stica, o quais foram
explicados nos CapÃ­tulos 2 e 3.

Para o CRF utilizado o algoritmo de Loopy Belief Propagation(LBP) para realizar
a inferÃªncia estatÃ­stica, dado a sua baixa taxa de erros e alta performance (WEISS, 2000).

6.3 Treinamento

Aqui Ã© descrito como foi realizado o treinamento das partes do sistema onde o

treinamento Ã© necessÃ¡rio.

6.3.1 Treinamento do Classificador

O treinamento unÃ¡rio diz respeito ao treinamento da funÃ§Ã£o de discriminaÃ§Ã£o do

classificador.

6.3. Treinamento

85

Tanto para o dataset Redsea quanto para o Marker foram feitas diversas amostra-
gens dos mosaicos existentes para o treinamento do classificador de cada uma das classes.
A Figura 26, mostra exemplos de segmentos usados para o treinamento do classificador
para os dois datasets utilizados. Foram feitas amostras de por volta de 200 segmentos
para cada classe sendo cada uma tendo 64x64 pixeis. As amostras foram indicadas por
especialistas da universidade de Miami.

Figura 26 â€“ Partes manualmente segmentadas utilizadas para treinamento do classifica-
dor. A esquerda sÃ£o mostrados exemplos de nove amostras usadas para treinar o dataset
Redsea. A direita sÃ£o apresentadas nove amostras do dataset Marker.

6.3.2 Treinamento UnÃ¡rio

Para gerar a curva de confianÃ§a, usada para gerar os distribuiÃ§Ã£o de probabilida-
des unÃ¡ria tanto para o CRF, quanto para o modelo de GeoestÃ¡tistica, foram tambÃ©m
utilizadas amostras do mosaico de treinamento da Figura 26.

As Figuras 27 e 28 mostram as curvas de confianÃ§a obtidas para cada um dos
dois datasets em cada uma das classes. O processo de geraÃ§Ã£o das curvas Ã© descrito no
CapÃ­tulo 3 SeÃ§Ã£o 3.2.

Pelos grÃ¡ficos das Figuras 27 e 28 percebe-se que determinadas classes se adaptam
melhor que outras a uma curva de confianÃ§a, como a classe Sea Gorgon ( Fig. 28c) do
dataset Marker. Para esta classe Ã© possÃ­vel saber quais distÃ¢ncias do classificador que
existe uma grande probabilidade de se acertar a classe, enquanto para outras o modelo
nÃ£o se adaptou tÃ£o adequadamente (Fig. 29b).

Entretanto, o principal erro em adaptaÃ§Ã£o da curva se da na classe Background
para os dois casos (Fig. 27a e Fig. 29a). Isso se da devido a alta variabilidade intra-classe
inerente a classe Background, a qual contÃ©m todos os tipos de objetos que nÃ£o sÃ£o de
interesse para classificaÃ§Ã£o.

86

CapÃ­tulo 6. Testes e Resultados 2: Contexto em ClassificaÃ§Ã£o SubaquÃ¡tica

(a) Classe Background

(b) Classe Urchin

(c) Classe Branching Coral

(d) Classe Brain Coral

(e) Classe Faviid Coral

Figura 27 â€“ Curvas de confianÃ§a geradas no treinamento unÃ¡rio de cada classe para o
dataset Redsea. A curva de confianÃ§a ğ¶ğ‘™ğ‘– treinada para cada uma das classes Ã© mostrada,
se bem como o grau de confianÃ§a obtido.

6.3.3 Treinamento Potenciais Locais

Para treinamento dos potenciais locais foi utilizado cada dataset na sua totalidade.
Para o caso do CRF foi utilizado o algoritmo de treinamento descrito no CapÃ­tulo

2, SeÃ§Ã£o 2.2.2.2 .

As Tabelas 2 e 3 mostram a matriz de covariÃ¢ncia obtida para cada um dos dois
datasets. Tal matriz estÃ¡ relacionada a uma indicaÃ§Ã£o de determinada classe estar prÃ³xima
a outra.

Background Urchin Branching Coral Brain Coral Faviid Coral

Classes
Background
Urchin
Branching Coral
Brain Coral
Faviid Coral
Tabela 2 â€“ Matriz de covariÃ¢ncia que mostra as relaÃ§Ãµes de proximidade entre as classes.
Tais medidas sÃ£o fatores que indicam correlaÃ§Ã£o e nÃ£o distribuiÃ§Ãµes de probabilidade. Este
resultado Ã© normalizado ao final.

0.8559
0.9458
1.3870
0.9461
0.9111

1.9115
0.7844
0.7658
0.8767
0.7612

0.8599
0.9679
0.9897
0.9605
0.9427

0.9094
0.9670
1.0219
1.6384
0.9353

0.8400
1.0424
0.9745
0.8972
1.6972

Os resultados do treinamento dos vetores de transiÃ§Ã£o, necessÃ¡rios para a simulaÃ§Ã£o

6.3. Treinamento

87

(a) Classe Background

(b) Classe General Corals

(c) Classe Sea Gorgon

(d) Classe Sand

Figura 28 â€“ Curvas de confianÃ§a geradas no treinamento unÃ¡rio de cada classe para o da-
taset Marker. A curva de confianÃ§a ğ¶ğ‘™ğ‘– treinada para cada uma das classes Ã© apresentada,
bem como o grau de confianÃ§a obtido.

Background General Coral

Classes
Background
General Coral
Sea Gorgon
Sand

1.8831
0.9010
0.8905
0.8950

0.8967
0.9544
0.9516
0.8990

Sea Gorgon Sand
0.9086
0.9028
0.8930
1.8773

0.8899
0.9507
0.9738
0.8906

Tabela 3 â€“ Matriz de covariÃ¢ncia que mostra as relaÃ§Ãµes de proximidade entre as classes.
Tais medidas sÃ£o fatores que indicam correlaÃ§Ã£o e nÃ£o distribuiÃ§Ãµes de probabilidade. Este
resultado Ã© normalizado ao final.

do mÃ©todo de GeoestatÃ­stica, sÃ£o mostrados na Figura 29 para ambos os datasets testados.
Para ambos os datasets analisados se observa a classe background como sendo
predominante nas estatÃ­sticas medidas em ambos os treinamentos. No caso do treinamento
dos vetores de transiÃ§Ã£o (GeoestatÃ­stica), tambÃ©m foi vista uma tendÃªncia de outras classes
em transitar para o Background (Fig. 29). Algo que, para o treinamento dos potenciais
locais do CRF, indicou principalmente uma tendÃªncia do Background ter proximidade
consigo prÃ³prio.

Para o dataset Redsea, nas relaÃ§Ãµes locais treinadas pelo CRF se observa algumas

tendÃªncias:

88

CapÃ­tulo 6. Testes e Resultados 2: Contexto em ClassificaÃ§Ã£o SubaquÃ¡tica

(a) Dataset Redsea

(b) Dataset Marker

Figura 29 â€“ Vetores de transiÃ§Ã£o obtidos na etapa de treinamento para o mÃ©todo de
GeoestatÃ­stica do CapÃ­tulo 3. Os vetores indicam a probabilidade de uma classe transitar
para outra a uma determinada distÃ¢ncia. O eixo x apresenta a distÃ¢ncia em pixeis. O eixo
ğ‘¦ dos grÃ¡ficos apresenta as probabilidades de transiÃ§Ã£o. Pode-se observar, por exemplo,
uma certa tendÃªncia na classe Urchin em transitar para categoria de background.

6.4.

Sistemas Testados

89

âˆ™ As classes Urchin tem uma grande possibilidade de estar prÃ³xima a classe Faviid

Coral;

âˆ™ Cada classe tem uma forte tendÃªncia de estar prÃ³xima a si prÃ³pria, o que enfatiza

a pouca variabilidade de classes em espaÃ§os pequenos;

âˆ™ Existe algumas tendÃªncias assimÃ©tricas treinadas, como a grande tendÃªncia da classe

Faviid Coral estar prÃ³xima da classe Urchin, mas nÃ£o ao contrÃ¡rio.

As transiÃ§Ãµes assimÃ©tricas, ou seja, uma dada classe A estar prÃ³xima a classe B
mas nÃ£o B prÃ³xima da A, nÃ£o sÃ£o incentivadas pelos potenciais treinados pelo mÃ©todo de
GeoestatÃ­stica.

Considerando as relaÃ§Ãµes treinadas pelo mÃ©todo de GeoestatÃ­stica, existe uma
tendÃªncia forte principalmente de transiÃ§Ã£o da classe Urchin para a classe Faviid Coral e
a classe Background.

Para o dataset Marker, nenhuma outra tendÃªncia de proximidade foi obtida para
o CRF, fora a tendÃªncia de background estar prÃ³ximo de si mesmo. As mesmas tendÃªncias
sÃ£o observadas para o treinamento dos vetores de transiÃ§Ã£o para o caso da GeoestatÃ­stica.

6.4

Sistemas Testados

Quatro sistemas sÃ£o testados quanto a sua taxa de acerto em relaÃ§Ã£o a classificaÃ§Ã£o
de mosaicos de imagens. Isso foi feito principalmente de forma a avaliar a consideraÃ§Ã£o
de contexto, juntamente com a nova proposta apresentada no CapÃ­tulo 3

Inicialmente foi avaliado o sistema UnÃ¡rio, proposto por Shihavuddin et al. (2013)
onde somente as informaÃ§Ãµes unÃ¡rias sÃ£o consideradas, ou seja, dada a definiÃ§Ã£o de classifi-
caÃ§Ã£o considerando uma segmentaÃ§Ã£o em regiÃµes (SHIHAVUDDIN et al., 2013). Somente
a descriÃ§Ã£o da prÃ³pria regiÃ£o foi usada para classificaÃ§Ã£o, o sistema Ã© detalhado no Cap.
4 .

ApÃ³s foi testado e analisado o sistema UnÃ¡rio porÃ©m baseado em distribuiÃ§Ã£o de
probabilidades. Em tal sistema foi feita a classificaÃ§Ã£o apenas considerando a parcela unÃ¡-
ria do sistema com base no modelo em GeoestatÃ­stica proposto no Cap. 3. A classificaÃ§Ã£o
de um segmento foi escolhida como o rÃ³tulo com mÃ¡xima a probabilidade.

Apresenta-se tambÃ©m o sistema, GS, baseado em GeoestatÃ­stica proposto no Cap.

3. A classificaÃ§Ã£o de cada segmento (Superpixel) Ã© dada pela Eq. 3.1, do Cap. 3.

Por fim, apresenta-se os resultados do sistema CRF o qual Ã© uma implementaÃ§Ã£o

dos Conditional Random Fields , tal qual explicada no Cap 2.

90

CapÃ­tulo 6. Testes e Resultados 2: Contexto em ClassificaÃ§Ã£o SubaquÃ¡tica

Todos os sistemas foram implementados em Matlab, para o CRF, foi utilizada a

biblioteca UGM para inferÃªncia estatÃ­stica (SCHMIDT et al., 2009).

6.5 ComputaÃ§Ã£o do Mapa TemÃ¡tico

No dataset Redsea, um mosaico de 3256x2939 pixeis foi utilizado para testes. JÃ¡

para dataset Marker,foi utilizado um mosaico de 2592x3963.

As Figuras 30 e 31 mostram os mapas temÃ¡ticos completos computados para ambos

os datasets.

Observa-se que num caso geral o CRF Ã© o mÃ©todo que obtÃ©m os melhores resulta-
dos. O mÃ©todo de GeoestatÃ­stica Ã© capaz de melhorar um pouco, porÃ©m depende muito
de um bom treinamento da distribuiÃ§Ã£o de probabilidades de cada segmento.

Para o caso do dataset Marker, a adiÃ§Ã£o de contexto foi mais eficaz para ambos os
casos. Isso ocorre dado que muitas posiÃ§Ãµes geraram resultados com distribuiÃ§Ã£o unÃ¡ria
uniforme, ou seja sem uma classe com alta probabilidade. O que contribuiu para adiÃ§Ã£o
de contexto foi, que tais regiÃµes, estavam cercadas por locais onde existia uma classe
predominante.

Ao se observar a configuraÃ§Ã£o do dataset Redsea se percebe uma tendÃªncia espacial
em se ter "ilhas"de classes envolvidas pela classe background. Dado que a classe background
tem uma alta variabilidade intra-classe, Ã© bastante complicado se ter uma tendÃªncia forte
para uma classe na distribuiÃ§Ã£o unÃ¡ria. Isso dificulta a proliferaÃ§Ã£o da informaÃ§Ã£o de
contexto na regiÃ£o.

De forma a analisar melhor as diferenÃ§as entre o CRF e o mÃ©todo de GeoestatÃ­stica,
Ã© mostrado na Figura 32 duas Ã¡reas diferentes do mosaico do Redsea para mostrar algumas
vantagens da abordagem com base em GS. Ã‰ apresentada a Ã¡rea original da imagem com
a classificaÃ§Ã£o mostrada em cores. Na primeira linha, pode se perceber o grau de acerto
maior para o CRF (Fig. 32c). O CRF tende a melhorar significativamente a suavizaÃ§Ã£o
local das estruturas classificadas. Ou seja, impÃµe que Ã¡reas pequenas devam ter menos
variaÃ§Ãµes de classes. Por esta razÃ£o o CRF teve uma boa classificaÃ§Ã£o especialmente para
o caso da classe representada em azul (Faviid Corals).

Por outro lado, na segunda linha, GS (Fig. 32f) obteve um grau de acerto maior.
Dado que o CRF (Fig. 32g) impÃµe mais suavidade local, isso tende a eliminar classes
menores (Fig. 32g). Este caso Ã© evitado pela GS pelo fato de que a abordagem baseada
em GeostatÃ­stica usa estatÃ­sticas medidas em longas distÃ¢ncias e assim o tamanho da
classe Ã© considerado. Na terceira linha da Figura 32, sÃ£o mostrados resultados similares
para o dataset Marker.

TambÃ©m os algoritmos foram testados para mÃºltiplos segmentos diferentes extraÃ­do

6.6. ConclusÃµes

91

Tamanho do Segmento

Unitary

GS
CRF
Voting

1700

1100

500
2300 MÃ©dia
81.1% 78.0% 79% 80.2% 79.7%
81.2% 78.3% 79% 80.2% 79.8%
80.5% 78.9% 79.3% 79.7% 79.8%
78%
78% 78.4% 80.2% 79.2%

Tabela 4 â€“ Resultados para a taxa de acerto de diferentes segmentos para o dataset Redsea.
Foram testados diversos segmentos quadrados amostrados aleatoreamente nos mosaicos.
O tamanho do segmento Ã© especificado pelo lado do quadrado

dos mosaicos. Foram recortadas amostras quadradas aleatÃ³rias de diferentes tamanhos.
Para cada tamanho recortado, a tabela 4 mostra a taxa de acerto mÃ©dia do mÃ©todo
aplicado em 20 segmentos aleatÃ³rios. TambÃ©m foi testado um mÃ©todo simples de votaÃ§Ã£o
onde um superpixel Ã© modificado caso a classe de todos os vizinhos seja differente.

No dataset Redsea, para todas as abordagens , nÃ£o foi percebido mais do que ganhos
marginais quando comparados com a versÃ£o unitÃ¡ria. O mÃ©todo de votaÃ§Ã£o tambÃ©m
obteve resultados similares.

6.6 ConclusÃµes

Conclui-se que o uso de estatÃ­sticas mais ricas, inspiradas pelos conceitos de Geo-
estatÃ­sticas, Ã© benÃ©fico e pode conduzir a melhores resultados que o CRF tradicional em
alguns casos.

As melhorias obtidas, foram, no entanto, de pequena magnitude, o que esta ali-
nhado com o que Ã© discutido em (LUCCHI et al., 2011). Os resultados utilizando contexto
normalmente nÃ£o melhoram mais do que a suavidade local dos resultados, ou seja, nÃ£o
mais do que evitam grande variaÃ§Ã£o de classes em uma pequena Ã¡rea. PorÃ©m ainda Ã©
possÃ­vel obter melhorias significativas, para alguns datasets como no caso do Marker.

92

CapÃ­tulo 6. Testes e Resultados 2: Contexto em ClassificaÃ§Ã£o SubaquÃ¡tica

(a) UnitÃ¡rio 76.83%

(b) UnitÃ¡rio com Curvas de ConfianÃ§a 75.779%

(c) GeoestatÃ­stica 76.16%

(d) CRF 77.32%

(e) Ground Truth

Figura 30 â€“ Mapa temÃ¡tico dos Mosaicos para o dataset Redsea. As figuras mostram a
porcentagem de acerto relativa ao GroundTruth. As classes sÃ£o representadas pelas se-
guintes cores: Verde Brain Coral; Amarelo Branchin Coral; Azul Faviid Coral; Magenta
Urchin e sem cor Ã© o background. Os seguintes resultados sÃ£o mostrados.(30a) classifica-
Ã§Ã£o UnÃ¡ria. (30b) mostra a classificaÃ§Ã£o UnÃ¡ria baseada nas curvas de confianÃ§a. (30c)
classificaÃ§Ã£o com adiÃ§Ã£o de contexto baseada em GeoestatÃ­stica. (30d) classificaÃ§Ã£o com
adiÃ§Ã£o de contexto utilizando CRF.

6.6. ConclusÃµes

93

(a) UnitÃ¡rio 78.04%

(b) UnitÃ¡rio com Curvas de ConfianÃ§a 78.02%

(c) GeoestatÃ­stica 79.2%

(d) CRF 83.26%

(e) Ground Truth

Figura 31 â€“ Mapa temÃ¡tico dos Mosaicos para o dataset Marker. As figuras mostram a
porcentagem de acerto relativa ao GroundTruth. As classes sÃ£o representadas pelas se-
guintes cores: Verde Sand; Amarelo Sea Gorgon; Azul Corals e sem cor Ã© o background.
Os seguintes resultados sÃ£o mostrados.(31a) classificaÃ§Ã£o UnÃ¡ria. (31b) mostra a classifi-
caÃ§Ã£o UnÃ¡ria baseada nas curvas de confianÃ§a. (31c) classificaÃ§Ã£o com adiÃ§Ã£o de contexto
baseada em GeoestatÃ­stica. (31d) classificaÃ§Ã£o com adiÃ§Ã£o de contexto utilizando CRF.

94

CapÃ­tulo 6. Testes e Resultados 2: Contexto em ClassificaÃ§Ã£o SubaquÃ¡tica

(a) 0.6985

(b) 0.6988

(c) 0.745

(d) Ground Truth

(e) 0.859

(f) 0.8691

(g) 0.858

(h) Ground Truth

(i) 0.756

(j) 0.77

(k) 0.763

(l) Ground Truth

Figura 32 â€“ Resultados de classificaÃ§Ã£o para os datasets Marker e os datasets Redsea. A
primeira coluna apresenta a classificaÃ§Ã£o unitÃ¡ria. A segunda coluna apresenta os resulta-
dos de GeoestatÃ­stica. A terceira coluna apresenta os resultados para o CRF. Por fim, a
ultima coluna apresenta o GroundTruth. Foi utilizada como peso para o potencial local ğ‘¤ğ‘™
como sendo 0.4 para ambas as abordagens. Na primeira coluna foi possÃ­vel perceber um
resultado melhor para o CRF devido a uma maior suavizaÃ§Ã£o local. Na segunda linha, o
mÃ©todo de GeoestatÃ­stica obteve melhores resultados devido a suas medidas estatÃ­sticas
de longa distÃ¢ncia. Na Ãºltima linha Ã© mostrado os resultados para o dataset Marker, onde
ambas as abordagens tiveram melhores resultados para esse caso.

7 ConclusÃµes Finais

95

Considerando o problema de estender a utilizaÃ§Ã£o de mÃ©todos de visÃ£o computaci-
onal para o cenÃ¡rio subaquÃ¡tico, esta dissertaÃ§Ã£o apresentou o estudo e tratamento para
alguns dos principais problemas existentes no meio.

Foi feito um estudo sobre duas Ã¡reas distintas relevantes para o problema: A detec-
Ã§Ã£o de pontos de interesse e o uso da informaÃ§Ã£o de contexto na classificaÃ§Ã£o de imagens.
Nas seÃ§Ãµes que seguem serÃ£o apresentadas as contribuiÃ§Ãµes sobre as duas Ã¡reas

distintas analisadas, como tambÃ©m as limitaÃ§Ãµes das propostas.

7.1 Detectores de Pontos de Interesse em Imagens SubaquÃ¡ticas

Turvas
No contexto subaquÃ¡tico, foi feito um estudo sobre como se comportam os mÃºlti-
plos detectores de pontos de interesse sobre a presenÃ§a da turbidez, fenÃ´meno o qual se
faz presente no meio subaquÃ¡tico.

7.1.1 ContribuiÃ§Ãµes Obtidas

As principais contribuiÃ§Ãµes obtidas foram:

âˆ™ A proposta de um dataset novo contendo imagens reais do assoalho oceÃ¢nico porÃ©m

com a turbidez controlada.

âˆ™ Uma anÃ¡lise geral da repetibilidade dos detectores em meios tÃºrbidos, dividindo a

anÃ¡lise em intervalos de turbidez distintos.

âˆ™ Dentre os detectores estudados, foi apontado o DoG como o mais robusto detector
para ambientes com presenÃ§a de turbidez. Tal detector contÃ©m tambÃ©m invariÃ¢ncia
a escala.

âˆ™ Foi concluÃ­da a possibilidade do uso de espaÃ§os nÃ£o gaussianos para geraÃ§Ã£o de

espaÃ§o de escala em meios subaquÃ¡ticos tÃºrbidos.

7.1.2 LimitaÃ§Ãµes e Trabalhos Futuros

O estudo nÃ£o foi capaz de propor um mÃ©todo para medir de fato a degradaÃ§Ã£o
causada pela turbidez. A medida utilizada Ã© capaz de verificar a degradaÃ§Ã£o estrutural o
que nÃ£o necessariamente estÃ¡ associada a turbidez.

96

CapÃ­tulo 7. ConclusÃµes Finais

Um outro ponto a ser tratado diz respeito a uma anÃ¡lise mais criteriosa com
respeito a invariÃ¢ncia a outras transformaÃ§Ãµes como rotaÃ§Ã£o ou escala, juntamente com a
robustez Ã  degradaÃ§Ã£o causada pela turbidez.

7.2 AdiÃ§Ã£o de Contexto Baseado em GeoestatÃ­stica

Foi proposto um novo mÃ©todo para adicionar informaÃ§Ã£o espacial na classificaÃ§Ã£o
de imagens. O mÃ©todo se baseou nos estudos da Ã¡rea de GeoestatÃ­stica. Tal mÃ©todo foi
aplicado em imagens de mosaicos de recifes de corais. O mesmo foi comparado com as
versÃµes sem a utilizaÃ§Ã£o de contexto e com o modelo dos Conditional Random Fields
(CRF).

Apresentou-se que a adiÃ§Ã£o de contexto pode, em alguns casos, ser benÃ©fica para
a classificaÃ§Ã£o de imagens subaquÃ¡ticas. Obtendo-se um ganho de atÃ© 5% a mais em taxa
de acerto.

7.2.1 ContribuiÃ§Ãµes Obtidas

O trabalho apresentou um novo mÃ©todo para adiÃ§Ã£o de contexto em imagens su-
baquÃ¡ticas. O uso de medidas estatÃ­sticas mais ricas, como as baseadas em GeoestatÃ­stica,
mostrou-se Ãºtil em algumas situaÃ§Ãµes para adiÃ§Ã£o de informaÃ§Ã£o de contexto.

TambÃ©m essa dissertaÃ§Ã£o serve como uma conexÃ£o entre duas Ã¡reas distintas: a
GeoestatÃ­stica e os Modelos ProbabilÃ­stico GrÃ¡ficos (MPGs). Acredita-se que atravÃ©s desta
intersecÃ§Ã£o, as aplicaÃ§Ãµes que fazem uso de GeoestatÃ­stica podem tambÃ©m se beneficiar
dos MPGs.

7.2.2 LimitaÃ§Ãµes e Trabalhos Futuros

Coloca-se que a abordagem apresentada foi aplicada para um cenÃ¡rio subaquÃ¡tico
especÃ­fico. Uma direÃ§Ã£o seria a aplicaÃ§Ã£o em dataset com classes mais genÃ©ricas, os da
Pascal Visual Object Classes Challenge (VOC) (EVERINGHAM et al., 2010).

Os resultados para o mÃ©todo de GeoestatÃ­stica foram satisfatÃ³rios porÃ©m ficaram
abaixo em taxa de acerto quando comparados ao CRF. Apesar da tendÃªncia de se usar os
mÃ©todos de GeoestatÃ­stica para casos onde hÃ¡ pouca quantidade de informaÃ§Ãµes (CARLE;
FOGG, 1996).

Existe ainda uma necessidade maior de alteraÃ§Ã£o no modelo original de Geoesta-

tÃ­stica visando uma melhor adaptaÃ§Ã£o para o caso de imagens subaquÃ¡ticas.

ReferÃªncias

97

ABFALG, J. et al. Multi-represented classification based on confidence estimation. In:
Advances in Knowledge Discovery and Data Mining. [S.l.]: Springer, 2007. p. 23â€“34.
Citado 2 vezes nas pÃ¡ginas 51 e 52.
AGRAWAL, M.; KONOLIGE, K.; BLAS, M. R. Censure: Center surround extremas
for realtime feature detection and matching. In: Computer Visionâ€“ECCV 2008. [S.l.]:
Springer, 2008. p. 102â€“115. Citado 4 vezes nas pÃ¡ginas 14, 35, 77 e 81.
AGTERBERG, F. Mathematical geologymathematical geology. In: General Geology.
Springer US, 1988, (Encyclopedia of Earth Science). p. 573â€“582. ISBN 978-0-442-22499-8.
DisponÃ­vel em: <http://dx.doi.org/10.1007/0-387-30844-X 76>. Citado na pÃ¡gina 55.
ALCANTARILLA, P. F.; BARTOLI, A.; DAVISON, A. J. Kaze features. In: Computer
Visionâ€“ECCV 2012. [S.l.]: Springer, 2012. p. 214â€“227. Citado 5 vezes nas pÃ¡ginas 35,
36, 77, 80 e 82.
AULINAS, J. et al. Feature extraction for underwater visual slam. In: IEEE. OCEANS,
2011 IEEE-Spain. [S.l.], 2011. p. 1â€“7. Citado na pÃ¡gina 24.
BAR, M. Visual objects in context. Nature Reviews Neuroscience, Nature Publishing
Group, v. 5, n. 8, p. 617â€“629, 2004. Citado 2 vezes nas pÃ¡ginas 25 e 41.
BAY, H. et al. Speeded-up robust features (surf). Computer vision and image
understanding, Elsevier, v. 110, n. 3, p. 346â€“359, 2008. Citado 2 vezes nas pÃ¡ginas 33
e 77.
BAZEILLE, S. et al. Automatic underwater image pre-processing. In: CMMâ€™06. [S.l.:
s.n.], 2006. p. xx. Citado na pÃ¡gina 66.
BEALL, C. et al. 3d reconstruction of underwater structures. In: IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS). [S.l.: s.n.], 2010. p. 4418â€“4423.
Citado 2 vezes nas pÃ¡ginas 24 e 27.
BEATTIE, C.; MILLS, B.; MAYO, V. Development drilling of the tawila field, yemen,
based on three-dimensional reservoir modeling and simulation. In: SPE annual technical
conference. [S.l.: s.n.], 1998. p. 715â€“725. Citado na pÃ¡gina 49.
BEAUDET, P. R. Rotationally invariant image operators. In: Proceedings of the 4th
International Joint Conference on Pattern Recognition. Kyoto, Japan: [s.n.], 1978. p.
579â€“583. Citado 4 vezes nas pÃ¡ginas 30, 71, 77 e 81.
BEIJBOM, O. et al. Automated annotation of coral reef survey images. In: IEEE.
Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. [S.l.],
2012. p. 1170â€“1177. Citado na pÃ¡gina 67.
BIEDERMAN, I.; MEZZANOTTE, R. J.; RABINOWITZ, J. C. Scene perception:
Detecting and judging objects undergoing relational violations. Cognitive psychology,
Elsevier, v. 14, n. 2, p. 143â€“177, 1982. Citado na pÃ¡gina 39.

98

ReferÃªncias

BOIX, X. et al. Harmony potentials. International journal of computer vision, Springer,
v. 96, n. 1, p. 83â€“102, 2012. Citado 2 vezes nas pÃ¡ginas 46 e 47.
BOYKOV, Y. Y.; JOLLY, M.-P. Interactive graph cuts for optimal boundary & region
segmentation of objects in nd images. In: IEEE. Computer Vision, 2001. ICCV 2001.
Proceedings. Eighth IEEE International Conference on. [S.l.], 2001. v. 1, p. 105â€“112.
Citado na pÃ¡gina 45.
CARBONETTO, P.; FREITAS, N. de; BARNARD, K. A statistical model for general
contextual object recognition. In: Computer Vision-ECCV 2004. [S.l.]: Springer, 2004. p.
350â€“362. Citado 2 vezes nas pÃ¡ginas 43 e 45.
CARLE, S. F.; FOGG, G. E. Transition probability-based indicator geostatistics.
Mathematical Geology, Springer, v. 28, n. 4, p. 453â€“476, 1996. Citado 4 vezes nas
pÃ¡ginas 49, 56, 57 e 96.
CARLE, S. F. et al. Conditional simulation of hydrofacies architecture: a transition
probability/markov approach. Hydrogeologic models of sedimentary aquifers, concepts
in hydrogeology and environmental geology, v. 1, p. 147â€“170, 1998. Citado 2 vezes nas
pÃ¡ginas 53 e 55.
CORKE, P. et al. Experiments with underwater robot localization and tracking. In:
Robotics and Automation, 2007 IEEE International Conference on. [S.l.: s.n.], 2007. p.
4556â€“4561. ISSN 1050-4729. Citado na pÃ¡gina 27.
CRISTIANINI, N.; SHAWE-TAYLOR, J. An introduction to support vector machines
and other kernel-based learning methods. [S.l.]: Cambridge university press, 2000. Citado
na pÃ¡gina 51.
CRISTINACCE, D.; COOTES, T. F. Feature detection and tracking with constrained
local models. In: CITESEER. BMVC. [S.l.], 2006. v. 2, n. 5, p. 6. Citado na pÃ¡gina 78.
DEMPSTER, A. P.; LAIRD, N. M.; RUBIN, D. B. Maximum likelihood from
incomplete data via the em algorithm. Journal of the royal statistical society. Series B
(methodological), JSTOR, p. 1â€“38, 1977. Citado na pÃ¡gina 45.
DERPANIS, K. G.; LEUNG, E. T.; SIZINTSEV, M. Fast scale-space feature
representations by generalized integral images. In: IEEE. Image Processing, 2007. ICIP
2007. IEEE International Conference on. [S.l.], 2007. v. 4, p. IVâ€“521. Citado na pÃ¡gina
33.
DUNTLEY, S. Q. Light in the sea. JOSA, Optical Society of America, v. 53, n. 2, p.
214â€“233, 1963. Citado 2 vezes nas pÃ¡ginas 65 e 73.
EMERY, X. Properties and limitations of sequential indicator simulation. Stochastic
Environmental Research and Risk Assessment, Springer, v. 18, n. 6, p. 414â€“424, 2004.
Citado na pÃ¡gina 54.
EVERINGHAM, M. et al. The pascal visual object classes (voc) challenge. International
journal of computer vision, Springer, v. 88, n. 2, p. 303â€“338, 2010. Citado na pÃ¡gina 96.
FINK, M.; PERONA, P. Mutual boosting for contextual inference. In: Advances in
neural information processing systems. [S.l.: s.n.], 2003. p. None. Citado na pÃ¡gina 42.

ReferÃªncias

99

FISCHLER, M. A.; ELSCHLAGER, R. A. The representation and matching of pictorial
structures. IEEE Transactions on Computers, Citeseer, v. 22, n. 1, p. 67â€“92, 1973.
Citado na pÃ¡gina 40.
FULKERSON, B.; VEDALDI, A.; SOATTO, S. Class segmentation and object
localization with superpixel neighborhoods. In: IEEE. Computer Vision, 2009 IEEE 12th
International Conference on. [S.l.], 2009. p. 670â€“677. Citado 4 vezes nas pÃ¡ginas 41, 44,
45 e 68.
GALLEGUILLOS, C.; BELONGIE, S. Context based object categorization: A critical
survey. Computer Vision and Image Understanding, Elsevier, v. 114, n. 6, p. 712â€“722,
2010. Citado 3 vezes nas pÃ¡ginas 39, 40 e 41.
GARCIA, R.; GRACIAS, N. Detection of interest points in turbid underwater images.
In: IEEE. OCEANS, 2011 IEEE-Spain. [S.l.], 2011. p. 1â€“9. Citado 6 vezes nas pÃ¡ginas
24, 71, 74, 76, 77 e 78.
GIL, A. et al. A comparative evaluation of interest point detectors and local descriptors
for visual slam. Machine Vision and Applications, Springer, v. 21, n. 6, p. 905â€“920, 2010.
Citado 2 vezes nas pÃ¡ginas 27 e 78.
GONZALEZ, R. C.; WOODS, R. E. Digital Image Processing (3rd Edition). Upper
Saddle River, NJ, USA: Prentice-Hall, Inc., 2006. ISBN 013168728X. Citado na pÃ¡gina
67.
GUO, Z.; ZHANG, D. A completed modeling of local binary pattern operator for texture
classification. Image Processing, IEEE Transactions on, IEEE, v. 19, n. 6, p. 1657â€“1663,
2010. Citado na pÃ¡gina 69.
HANSON, A. R.; RISEMAN, E. M. VISIONS: A computer system for interpreting
scenes. In: HANSON, A. R.; RISEMAN, E. M. (Ed.). Computer Vision Systems. New
York: Academic Press, 1978. Citado na pÃ¡gina 40.
HARALICK, R. M.; SHANMUGAM, K.; DINSTEIN, I. H. Textural features for image
classification. Systems, Man and Cybernetics, IEEE Transactions on, IEEE, n. 6, p.
610â€“621, 1973. Citado na pÃ¡gina 69.
HARRIS, C.; STEPHENS, M. A combined corner and edge detector. In: MANCHESTER,
UK. Alvey vision conference. [S.l.], 1988. v. 15, p. 50. Citado 5 vezes nas pÃ¡ginas 29, 30,
71, 77 e 81.
JOHNSON-ROBERSON, M.; KUMAR, S.; WILLAMS, S. Segmentation and
classification of coral for oceanographic surveys: a semi-supervised machine learning
approach. In: IEEE. OCEANS 2006-Asia Pacific. [S.l.], 2007. p. 1â€“6. Citado na pÃ¡gina
66.
KOLTUN; VLADLEN. Efficient inference in fully connected crfs with gaussian edge
potentials. In: . [S.l.: s.n.], 2011. Citado 2 vezes nas pÃ¡ginas 46 e 60.
KOSCHMIEDER, H. Theorie der horizontalen Sichtweite. [S.l.]: Keim Nemnich, 1924.
Citado na pÃ¡gina 64.
KRUPPA, H.; SCHIELE, B. Using Local Context to Improve Face Detection. 2003.
Citado na pÃ¡gina 42.

100

ReferÃªncias

KUMAR, S.; HEBERT, M. A hierarchical field framework for unified context-based
classification. In: IEEE. Computer Vision, 2005. ICCV 2005. Tenth IEEE International
Conference on. [S.l.], 2005. v. 2, p. 1284â€“1291. Citado na pÃ¡gina 40.

LEVINSHTEIN, A. et al. Turbopixels: Fast superpixels using geometric flows. Pattern
Analysis and Machine Intelligence, IEEE Transactions on, IEEE, v. 31, n. 12, p.
2290â€“2297, 2009. Citado 3 vezes nas pÃ¡ginas 49, 69 e 84.

LINDEBERG, T. Scale-space theory: A basic tool for analyzing structures at different
scales. Journal of applied statistics, Taylor & Francis, v. 21, p. 225â€“270, 1994. Citado 2
vezes nas pÃ¡ginas 31 e 32.

LINDEBERG, T. On the axiomatic foundations of linear scale-space. [S.l.]: Springer,
1997. Citado na pÃ¡gina 32.

LINDEBERG, T. Feature detection with automatic scale selection. International journal
of computer vision, Springer, v. 30, n. 2, p. 79â€“116, 1998. Citado na pÃ¡gina 32.

LINDEBERG, T.; EKLUNDH, J.-O. On the computation of a scale-space primal sketch.
Journal of Visual Communication and Image Representation, v. 2, n. 1, p. 55 â€“ 78, 1991.
ISSN 1047-3203. Citado na pÃ¡gina 29.

LOWE, D. G. Distinctive image features from scale-invariant keypoints. International
journal of computer vision, Springer, v. 60, n. 2, p. 91â€“110, 2004. Citado 8 vezes nas
pÃ¡ginas 11, 33, 34, 45, 72, 77, 80 e 82.

LUCCHI, A. et al. Are spatial and global constraints really necessary for segmentation?
In: IEEE. Computer Vision (ICCV), 2011 IEEE International Conference on. [S.l.],
2011. p. 9â€“16. Citado 3 vezes nas pÃ¡ginas 47, 49 e 91.

MARCOS, M. S. A.; SORIANO, M.; SALOMA, C. Classification of coral reef images
from underwater video using neural networks. Optics express, Optical Society of America,
v. 13, n. 22, p. 8766â€“8771, 2005. Citado na pÃ¡gina 67.

MARQUARDT, D. W. An algorithm for least-squares estimation of nonlinear
parameters. Journal of the Society for Industrial & Applied Mathematics, SIAM, v. 11,
n. 2, p. 431â€“441, 1963. Citado na pÃ¡gina 52.

MIKOLAJCZYK, K.; SCHMID, C. Scale & affine invariant interest point detectors.
International journal of computer vision, Springer, v. 60, n. 1, p. 63â€“86, 2004. Citado 2
vezes nas pÃ¡ginas 32 e 71.

NARASIMHAN, S. G. et al. Acquiring scattering properties of participating media by
dilution. ACM Transactions on Graphics (TOG), ACM, v. 25, n. 3, p. 1003â€“1012, 2006.
Citado na pÃ¡gina 73.

NEMETH, R. S. et al. Characterization of deep water reef communities within the
marine conservation district, st. thomas, us virgin islands. 2008. Citado na pÃ¡gina 23.

NICOSEVICI, T. et al. Efficient three-dimensional scene modeling and mosaicing.
Journal of Field Robotics, v. 26, 2009. Citado 2 vezes nas pÃ¡ginas 24 e 27.

ReferÃªncias

101

OMAR, A. F. B.; MATJAFRI, M. Z. B. Turbidimeter design and analysis: a review on
optical fiber sensors for the measurement of water turbidity. Sensors, Molecular Diversity
Preservation International, v. 9, n. 10, p. 8311â€“8335, 2009. Citado na pÃ¡gina 65.

PADMAVATHI, G.; MUTHUKUMAR, M.; THAKUR, S. K. Kernel principal component
analysis feature detection and classification for underwater images. In: IEEE. Image
and Signal Processing (CISP), 2010 3rd International Congress on. [S.l.], 2010. v. 2, p.
983â€“988. Citado 2 vezes nas pÃ¡ginas 24 e 27.

PERONA, P.; MALIK, J. Scale-space and edge detection using anisotropic diffusion.
Pattern Analysis and Machine Intelligence, IEEE Transactions on, IEEE, v. 12, n. 7, p.
629â€“639, 1990. Citado na pÃ¡gina 36.

PETROU, M.; GARCÃA-SEVILLA, P. Image processing - dealing with texture. [S.l.]:
Wiley, 2006. ISBN 978-0-470-02628-1. Citado na pÃ¡gina 69.

PIZARRO, O.; EUSTICE, R.; SINGH, H. Large area 3d reconstructions from underwater
surveys. In: OCEANS â€™04. MTTS/IEEE TECHNO-OCEAN â€™04. [S.l.: s.n.], 2004. v. 2,
p. 678â€“687 Vol.2. Citado 2 vezes nas pÃ¡ginas 66 e 67.

PIZARRO, O. et al. Towards image-based marine habitat classification. In: IEEE.
OCEANS 2008. [S.l.], 2008. p. 1â€“7. Citado na pÃ¡gina 66.

PLATT, J. C. Probabilistic outputs for support vector machines and comparisons to
regularized likelihood methods. In: CITESEER. Advances in large margin classifiers.
[S.l.], 1999. Citado na pÃ¡gina 52.

PORTER, R.; CANAGARAJAH, N. Robust rotation-invariant texture classification:
wavelet, gabor filter and gmrf based schemes. In: IET. Vision, Image and Signal
Processing, IEE Proceedings-. [S.l.], 1997. v. 144, n. 3, p. 180â€“188. Citado na pÃ¡gina 69.

PURKIS, S.; VLASWINKEL, B.; GRACIAS, N. Vertical-to-lateral transitions among
cretaceous carbonate facies: A means to 3-d framework construction via markov analysis.
Journal of Sedimentary Research, SEPM, v. 82, n. 4, p. 232â€“243, 2012. Citado na
pÃ¡gina 49.

RABINOVICH, A. et al. Objects in context. In: Proceedings of the International
Conference on Computer Vision (ICCV). [S.l.: s.n.], 2007. Citado na pÃ¡gina 40.

SCHETTINI, R.; CORCHS, S. Underwater image processing: state of the art of
restoration and image enhancement methods. EURASIP Journal on Advances in Signal
Processing, Hindawi Publishing Corp., v. 2010, p. 14, 2010. Citado na pÃ¡gina 67.

SCHMID, C.; MOHR, R.; BAUCKHAGE, C. Evaluation of interest point detectors.
International Journal of computer vision, Springer, v. 37, 2000. Citado 2 vezes nas
pÃ¡ginas 30 e 77.

SCHMIDT, M. W. et al. Optimizing costly functions with simple constraints: A
limited-memory projected quasi-newton algorithm. In: International Conference on
Artificial Intelligence and Statistics. [S.l.: s.n.], 2009. p. None. Citado na pÃ¡gina 90.

102

ReferÃªncias

SHIHAVUDDIN, A. et al. Image-based coral reef classification and thematic mapping.
Remote Sensing, v. 5, n. 4, p. 1809â€“1841, 2013. ISSN 2072-4292. DisponÃ­vel em:
<http://www.mdpi.com/2072-4292/5/4/1809>. Citado 7 vezes nas pÃ¡ginas 25, 66, 67,
68, 69, 70 e 89.

SHOTTON, J. et al. Textonboost for image understanding: Multi-class object recognition
and segmentation by jointly modeling texture, layout, and context. International Journal
of Computer Vision, Springer, v. 81, n. 1, p. 2â€“23, 2009. Citado 2 vezes nas pÃ¡ginas 44
e 45.
SIVIC, J.; ZISSERMAN, A. Video google: Efficient visual search of videos. In: Toward
Category-Level Object Recognition. [S.l.]: Springer, 2006. p. 127â€“144. Citado na pÃ¡gina
45.
SOJKA, E. A new approach to detecting the corners in digital images. In: IEEE. Image
Processing, 2003. ICIP 2003. Proceedings. 2003 International Conference on. [S.l.], 2003.
v. 3, p. IIIâ€“445. Citado 2 vezes nas pÃ¡ginas 11 e 31.

STOKES, M. D.; DEANE, G. B. Automated processing of coral reef benthic images.
Limnol. Oceanogr.: Methods, v. 7, n. 157, p. 157â€“168, 2009. Citado 2 vezes nas pÃ¡ginas
67 e 70.

SUTTON, C.; MCCALLUM, A. An introduction to conditional random fields for
relational learning. Introduction to statistical relational learning, MIT press, p. 93â€“128,
2006. Citado 2 vezes nas pÃ¡ginas 42 e 43.
TORRALBA, A. Contextual priming for object detection. International journal of
computer vision, Springer, v. 53, n. 2, p. 169â€“191, 2003. Citado na pÃ¡gina 40.

TORRALBA, A.; MURPHY, K. P.; FREEMAN, W. T. Contextual models for object
detection using boosted random fields. In: Advances in neural information processing
systems. [S.l.: s.n.], 2004. p. 1401â€“1408. Citado 2 vezes nas pÃ¡ginas 40 e 41.
TREIBITZ, T.; SCHECHNER, Y. Y. Instant 3descatter. In: Proceedings of the 2006
IEEE Computer Society Conference on Computer Vision and Pattern Recognition -
Volume 2. [S.l.: s.n.], 2006. (CVPR â€™06), p. 1861â€“1868. Citado 2 vezes nas pÃ¡ginas 64
e 73.

TUYTELAARS, T.; MIKOLAJCZYK, K. Local invariant feature detectors: a survey.
Foundations and Trends Râ—‹ in Computer Graphics and Vision, Now Publishers Inc., v. 3,
n. 3, p. 177â€“280, 2008. Citado 4 vezes nas pÃ¡ginas 27, 28, 30 e 77.

WANG, Z. et al. Image quality assessment: From error visibility to structural similarity.
Image Processing, IEEE Transactions on, IEEE, v. 13, n. 4, p. 600â€“612, 2004. Citado 2
vezes nas pÃ¡ginas 76 e 78.

WEICKERT, J.; ROMENY, B. T. H.; VIERGEVER, M. A. Efficient and reliable
schemes for nonlinear diffusion filtering. Image Processing, IEEE Transactions on, IEEE,
v. 7, n. 3, p. 398â€“410, 1998. Citado na pÃ¡gina 36.
WEIJER, J. V. D.; SCHMID, C. Coloring local feature extraction. In: Computer
Visionâ€“ECCV 2006. [S.l.]: Springer, 2006. p. 334â€“348. Citado na pÃ¡gina 69.

ReferÃªncias

103

WEISS, Y. Correctness of local probability propagation in graphical models with loops.
Neural computation, MIT Press, v. 12, n. 1, p. 1â€“41, 2000. Citado 2 vezes nas pÃ¡ginas
45 e 84.
ZUIDERVELD, K. Contrast limited adaptive histogram equalization. In: ACADEMIC
PRESS PROFESSIONAL, INC. Graphics gems IV. [S.l.], 1994. p. 474â€“485. Citado 2
vezes nas pÃ¡ginas 67 e 84.
ZVULONI, A. et al. Spatio-temporal transmission patterns of black-band disease in a
coral community. PLoS One, Public Library of Science, v. 4, n. 4, p. e4993, 2009. Citado
2 vezes nas pÃ¡ginas 73 e 83.

